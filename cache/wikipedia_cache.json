{
  "categories": {
    "Large_language_models": [
      "Large language model",
      "AutoGPT",
      "BERT (language model)",
      "BLOOM (language model)",
      "Braina",
      "Brave Leo",
      "ChatGPT",
      "Chinchilla (language model)",
      "Chroma (vector database)",
      "Claude (language model)",
      "Cohere",
      "DBRX",
      "Ernie Bot",
      "Gemini (language model)",
      "Generative pre-trained transformer",
      "GigaChat",
      "GPT-1",
      "GPT-2",
      "GPT-3",
      "GPT-4",
      "GPT-4o",
      "GPT-J",
      "GPT4-Chan",
      "GPTeens",
      "GPTZero",
      "Grok (chatbot)",
      "Huawei PanGu",
      "Humanity's Last Exam",
      "IBM Granite",
      "IBM Watsonx",
      "IFlytek Spark",
      "Jais (language model)",
      "LaMDA",
      "LangChain",
      "List of large language models",
      "Llama (language model)",
      "Llama.cpp",
      "MMLU",
      "OpenAI o1",
      "OpenAI o3",
      "PaLM",
      "The Pile (dataset)",
      "Qwen",
      "Reasoning language model",
      "Reflection (artificial intelligence)",
      "Retrieval-augmented generation",
      "Sparrow (chatbot)",
      "Stochastic parrot",
      "T5 (language model)",
      "Top-p sampling",
      "Undetectable.ai",
      "Vicuna LLM",
      "VideoPoet",
      "Waluigi effect",
      "XLNet",
      "YandexGPT",
      "You.com"
    ],
    "Sigma": []
  },
  "pages": {
    "Large language model": "A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\nThe largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\n\n\n== History ==\n\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\"web as corpus\"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models because they can usefully ingest large datasets.\n\nAfter neural networks became dominant in image processing around 2012, they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. Because it preceded the existence of transformers, it was done by seq2seq deep LSTM networks.\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper's goal was to improve upon 2014 seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer.\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-parameter open-weight model that performs comparably to OpenAI o1 but at a much lower cost.\nSince 2023, many LLMs have been trained to be multimodal, having the ability to also process or generate other types of data, such as images or audio. These LLMs are also called large multimodal models (LMMs).\nAs of 2024, the largest and most capable models are all based on the transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).\n\n\n== Dataset preprocessing ==\n\n\n=== Tokenization ===\n\nAs machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indices are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding (BPE) and WordPiece. There are also special tokens serving as control characters, such as [MASK] for masked-out token (as used in BERT), and [UNK] (\"unknown\") for characters not appearing in the vocabulary. Also, some special symbols are used to denote special text formatting. For example, \"Ġ\" denotes a preceding whitespace in RoBERTa and GPT. \"##\" denotes continuation of a preceding word in BERT.\nFor example, the BPE tokenizer used by GPT-3 (Legacy) would split tokenizer: texts -> series of numerical \"tokens\" as\n\nTokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one. How many tokens are, on average, needed per word depends on the language of the dataset.\n\n\n==== BPE ====\n\nAs an example, consider a tokenizer based on byte-pair encoding. In the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram, until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257). After a tokenizer is trained, any text can be tokenized by it, as long as it does not contain characters not appearing in the initial-set of uni-grams.\n\n\n==== Problems ====\nA token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. However, an average word in another language encoded by such an English-optimized tokenizer is split into a suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for the Shan language from Myanmar. Even more widespread languages such as Portuguese and German have \"a premium of 50%\" compared to English.\nGreedy tokenization also causes subtle problems with text completion.\n\n\n=== Dataset cleaning ===\n\nIn the context of training LLMs, datasets are typically cleaned by removing low-quality, duplicated, or toxic data. Cleaned datasets can increase training efficiency and lead to improved downstream performance. A trained LLM can be used to clean datasets for training a further LLM.\nWith the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).\n\n\n=== Synthetic data ===\n\nTraining of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality. In these cases, synthetic data might be used. Microsoft's Phi series of LLMs is trained on textbook-like data generated by another LLM.\n\n\n== Training and architecture ==\n\n\n=== Reinforcement learning from human feedback (RLHF) ===\n\nReinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.\n\n\n=== Instruction tuning ===\nUsing \"self-instruct\" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction \"Write an essay about the main themes represented in Hamlet,\" an initial naive completion might be \"If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,\" based on the frequency of this textual sequence in the corpus.\n\n\n=== Mixture of experts ===\n\nThe largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.\n\n\n=== Prompt engineering, attention mechanism, and context window ===\n\nMost results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineering, although limited to the scope of a single conversation (more precisely, limited to the scope of a context window).\n\nIn order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \"soft\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \"relevance\" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2 model has had twelve attention heads and a context window of only 1k tokens. In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.\nThe largest models, such as Google's Gemini 1.5, presented in February 2024, can have a context window sized up to 1 million (context window of 10 million was also \"successfully tested\"). Other models with large context windows includes Anthropic's Claude 2.1, with a context window of up to 200k tokens. Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example, the GPT-4 Turbo model has a maximum output of 4096 tokens.\nLength of a conversation that the model can take into account when generating its next answer is limited by the size of a context window, as well. If the length of a conversation, for example with ChatGPT, is longer than its context window, only the parts inside the context window are taken into account when generating the next answer, or the model needs to apply some algorithm to summarize the too distant parts of conversation.\nThe shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context, while making it smaller can cause a model to miss an important long-range dependency. Balancing them is a matter of experimentation and domain-specific considerations.\nA model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset. It can be either\n\nautoregressive (i.e. predicting how the segment continues, as GPTs do): for example given a segment \"I like to eat\", the model predicts \"ice cream\", or \"sushi\".\n\"masked\" (i.e. filling in the parts missing from the segment, the way \"BERT\" does it): for example, given a segment \"I like to [__] [__] cream\", the model predicts that \"eat\" and \"ice\" are missing.\nModels may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus. During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation.\n\n\n=== Infrastructure ===\nSubstantial infrastructure is necessary for training the largest models.\n\n\n== Training cost ==\n\nThe qualifier \"large\" in \"large language model\" is inherently vague, as there is no definitive threshold for the number of parameters required to qualify as \"large\". As time goes on, what was previously considered \"large\" may evolve. GPT-1 of 2018 is usually considered the first LLM, even though it has only 0.117 billion parameters. The tendency towards larger models is visible in the list of large language models.\nAdvances in software and hardware have reduced the cost substantially since 2020, such that in 2023 training of a 12-billion-parameter LLM computational cost is 72,300 A100-GPU-hours, while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80,000 and $1,600,000. Since 2020, large sums were invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.\nFor Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.\n\n\n== Tool use ==\nThere are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user's input '354 * 139 = ', provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response.: Another example is \"What is the time now? It is \", where a separate program interpreter would need to execute a code to get system time on the computer, so that the LLM can include it in its reply. This basic strategy can be sophisticated with multiple attempts of generated programs, and other sampling strategies.\nGenerally, in order to get an LLM to use tools, one must fine-tune it for tool-use. If the number of tools is finite, then fine-tuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.\nRetrieval-augmented generation (RAG) is another approach that enhances LLMs by integrating them with document retrieval systems. Given a query, a document retriever is called to retrieve the most relevant documents. This is usually done by encoding the query and the documents into vectors, then finding the documents with vectors (usually stored in a vector database) most similar to the vector of the query. The LLM then generates an output based on both the query and context included from the retrieved documents.\n\n\n== Agency ==\nAn LLM is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions, but can be transformed into one by integrating modules like profiling, memory, planning, and action.\nThe ReAct pattern, a portmanteau of \"Reason + Act\", constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment. The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.\nIn the DEPS (\"Describe, Explain, Plan and Select\") method, an LLM is first connected to the visual world via image descriptions, then it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and environmental feedback it receives.\nThe Reflexion method constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up \"lessons learned\", which would help it perform better at a subsequent episode. These \"lessons learned\" are given to the agent in the subsequent episodes.\nMonte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.\nFor open-ended exploration, an LLM can be used to score observations for their \"interestingness\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent. Alternatively, it can propose increasingly difficult tasks for curriculum learning. Instead of outputting individual actions, an LLM planner can also construct \"skills\", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.\nLLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially.\n\n\n== Compression ==\nTypically, LLMs are trained with single- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters, requiring 200 gigabytes to load, which places them outside the range of most consumer electronics.\nPost-training quantization aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance. The simplest form of quantization simply truncates all numbers to a given number of bits. It can be improved by using a different quantization codebook per layer. Further improvement can be done by applying different precisions to different parameters, with higher precision for particularly important parameters (\"outlier weights\"). See the visual guide to quantization by Maarten Grootendorst for a visual depiction.\nWhile quantized models are typically frozen, and only pre-quantized models are fine-tuned, quantized models can still be fine-tuned.\n\n\n== Multimodality ==\n\nMultimodality means \"having several modalities\", and a \"modality\" refers to a type of input or output, such as video, image, audio, text, proprioception, etc. There have been many AI models trained specifically to ingest one modality and output another modality, such as AlexNet for image to label, visual question answering for image-text to text, and speech recognition for speech to text.\nA common method to create multimodal models out of an LLM is to \"tokenize\" the output of a trained encoder. Concretely, one can construct an LLM that can understand images as follows: take a trained LLM, and take a trained image encoder \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n. Make a small multilayered perceptron \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n, so that for any image \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n, the post-processed vector \n  \n    \n      \n        f\n        (\n        E\n        (\n        y\n        )\n        )\n      \n    \n    {\\displaystyle f(E(y))}\n  \n has the same dimensions as an encoded token. That is an \"image token\". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.\nFlamingo demonstrated the effectiveness of the tokenization method, finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch. Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method, and applied to robotic control. LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs, and video inputs.\nGPT-4 can use both text and image as inputs (although the vision component was not released to the public until GPT-4V); Google DeepMind's Gemini is also multimodal.  Mistral introduced its own multimodel Pixtral 12B model in September 2024.\n\n\n== Reasoning ==\nIn late 2024, a new direction emerged in LLM development with models specifically designed for complex reasoning tasks. These \"reasoning models\" were trained to spend more time generating step-by-step solutions before providing final answers, similar to human problem-solving processes.\nOpenAI introduced this trend with their o1 model in September 2024, followed by o3 in December 2024. These models showed significant improvements in mathematics, science, and coding tasks compared to traditional LLMs. For example, on International Mathematics Olympiad qualifying exam problems, GPT-4o achieved 13% accuracy while o1 reached 83%.\nIn January 2025, the Chinese company DeepSeek released DeepSeek-R1, a 671-billion-parameter open-weight reasoning model that achieved comparable performance to OpenAI's o1 while being significantly more cost-effective to operate. Unlike proprietary models from OpenAI, DeepSeek-R1's open-weight nature allowed researchers to study and build upon the algorithm, though its training data remained private.\nThese reasoning models typically require more computational resources per query compared to traditional LLMs, as they perform more extensive processing to work through problems step-by-step. However, they have shown superior capabilities in domains requiring structured logical thinking, such as mathematics, scientific research, and computer programming.\nEfforts to reduce or compensate for hallucinations have employed automated reasoning, RAG (retrieval-augmented generation), fine-tuning, and other methods.\n\n\n== Properties ==\n\n\n=== Scaling laws ===\n\nThe performance of an LLM after pretraining largely depends on the:\n\ncost of pretraining \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n (the total amount of compute used),\nsize of the artificial neural network itself, such as number of parameters \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n (i.e. amount of neurons in its layers, amount of weights between them and biases),\nsize of its pretraining dataset (i.e. number of tokens in corpus, \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n).\n\"Scaling laws\" are empirical statistical laws that predict LLM performance based on such factors. One particular scaling law (\"Chinchilla scaling\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that:\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  C\n                  =\n                  \n                    C\n                    \n                      0\n                    \n                  \n                  N\n                  D\n                \n              \n              \n                \n                  L\n                  =\n                  \n                    \n                      A\n                      \n                        N\n                        \n                          α\n                        \n                      \n                    \n                  \n                  +\n                  \n                    \n                      B\n                      \n                        D\n                        \n                          β\n                        \n                      \n                    \n                  \n                  +\n                  \n                    L\n                    \n                      0\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}C=C_{0}ND\\\\[6pt]L={\\frac {A}{N^{\\alpha }}}+{\\frac {B}{D^{\\beta }}}+L_{0}\\end{cases}}}\n  \n where the variables are\n\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is the cost of training the model, in FLOPs.\n\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of parameters in the model.\n\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is the number of tokens in the training set.\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.\nand the statistical hyper-parameters are\n\n  \n    \n      \n        \n          C\n          \n            0\n          \n        \n        =\n        6\n      \n    \n    {\\displaystyle C_{0}=6}\n  \n, meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.\n\n  \n    \n      \n        α\n        =\n        0.34\n        ,\n        β\n        =\n        0.28\n        ,\n        A\n        =\n        406.4\n        ,\n        B\n        =\n        410.7\n        ,\n        \n          L\n          \n            0\n          \n        \n        =\n        1.69\n      \n    \n    {\\displaystyle \\alpha =0.34,\\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}\n  \n\n\n=== Emergent abilities ===\n\nPerformance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. However, this linearity may be punctuated by \"break(s)\" in the scaling law, where the slope of the line changes abruptly, and where larger models acquire \"emergent abilities\". They arise from the complex interaction of the model's components and are not explicitly programmed or designed. \nFurthermore, recent research has demonstrated that AI systems, including large language models, can employ heuristic reasoning akin to human cognition. They balance between exhaustive logical processing and the use of cognitive shortcuts (heuristics), adapting their reasoning strategies to optimize between accuracy and effort. This behavior aligns with principles of resource-rational human cognition, as discussed in classical theories of bounded rationality and dual-process theory.\nOne of the emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks, such as:\n\nreported arithmetics\ndecoding the International Phonetic Alphabet\nunscrambling a word's letters\ndisambiguating word-in-context datasets\nconverting spatial words\ncardinal directions (for example, replying \"northeast\" in response to a 3x3 grid of 8 zeros and a 1 in the top-right), color terms represented in text.\nchain-of-thought prompting: In a 2022 research paper, chain-of-thought prompting only improved the performance for models that had at least 62B. Smaller models perform better when prompted to answer immediately, without chain of thought.\nidentifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.\nSchaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.\nLet \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n be the number of parameter count, and \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n be the performance of the model.\n\n\n== Interpretation ==\nLarge language models by themselves are black boxes, and it is not clear how they can perform linguistic tasks. Similarly, it is unclear if or how LLMs should be viewed as models of the human brain and/or human mind.\nThere are several methods for understanding how LLMs work. Mechanistic interpretability aims to reverse-engineer LLMs by discovering symbolic algorithms that approximate the inference performed by an LLM. One example is Othello-GPT, where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of the Othello board, and modifying the representation changes the predicted legal Othello moves in the correct way. In another example, a small Transformer is trained on Karel programs. Similar to the Othello-GPT example, there is a linear representation of Karel program semantics, and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.\nIn another example, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform.\nA related concept is AI explainability, which focuses on understanding how an AI model arrives at a given result.\n\n\n=== Understanding and intelligence ===\n\nNLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \"could (ever) understand natural language in some nontrivial sense\". Proponents of \"LLM understanding\" believe that some LLM abilities, such as mathematical reasoning, imply an ability to \"understand\" certain concepts. A Microsoft team argued in 2023 that GPT-4 \"can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more\" and that GPT-4 \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system\": \"Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?\" Ilya Sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the LLM has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation. Some researchers characterize LLMs as \"alien intelligence\". For example, Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien \"Shoggoths\", and believes that RLHF tuning creates a \"smiling facade\" obscuring the inner workings of the LLM: \"If you don't push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.\"\nIn contrast, some skeptics of LLM understanding believe that existing LLMs are \"simply remixing and recombining existing writing\", a phenomenon known as stochastic parrot, or they point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability. For example, GPT-4 has natural deficits in planning and in real-time learning. Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \"hallucination\". Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input. Neuroscientist Terrence Sejnowski has argued that \"The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate\".\nThe matter of LLM's exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language. These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL) as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language.\n\n\n== Evaluation ==\n\n\n=== Perplexity ===\nThe canonical measure of the performance of an LLM is its perplexity on a given text corpus. Perplexity measures how well a model predicts the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. In mathematical terms, perplexity is the exponential of the average negative log likelihood per token.\n\n  \n    \n      \n        log\n        ⁡\n        (\n        \n          Perplexity\n        \n        )\n        =\n        −\n        \n          \n            1\n            N\n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            N\n          \n        \n        log\n        ⁡\n        (\n        Pr\n        (\n        \n          \n            token\n          \n          \n            i\n          \n        \n        ∣\n        \n          \n            context for token\n          \n          \n            i\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle \\log({\\text{Perplexity}})=-{\\frac {1}{N}}\\sum _{i=1}^{N}\\log(\\Pr({\\text{token}}_{i}\\mid {\\text{context for token}}_{i}))}\n  \n\nHere, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of tokens in the text corpus, and \"context for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n\" depends on the specific type of LLM. If the LLM is autoregressive, then \"context for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n\" is the segment of text appearing before token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n. If the LLM is masked, then \"context for token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n\" is the segment of text surrounding token \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n.\nBecause language models may overfit to training data, models are usually evaluated by their perplexity on a test set. This evaluation is potentially problematic for larger models which, as they are trained on increasingly large corpora of text, are increasingly likely to inadvertently include portions of any given test set.\n\n\n==== BPW, BPC, and BPT ====\nIn information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon. This relationship is mathematically expressed as \n  \n    \n      \n        \n          Entropy\n        \n        =\n        \n          log\n          \n            2\n          \n        \n        ⁡\n        (\n        \n          Perplexity\n        \n        )\n      \n    \n    {\\displaystyle {\\text{Entropy}}=\\log _{2}({\\text{Perplexity}})}\n  \n.\nEntropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.\nNotably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different Large Language Models (LLMs), BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.\nIn the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model's enhanced capability for compression. This, in turn, reflects the model's proficiency in making accurate predictions.\n\n\n=== Task-specific datasets and benchmarks ===\nA large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, bias, commonsense reasoning, and mathematical problem-solving.\nOne broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers, for example, (\"Have the San Jose Sharks won the Stanley Cup?\", \"No\"). A question answering task is considered \"open book\" if the model's prompt includes text from which the expected answer can be derived (for example, the previous question could be adjoined with some text which includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"). Otherwise, the task is considered \"closed book\", and the model must draw on knowledge retained during training. Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.\nEvaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".\nSome composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, HELM, and HLE (Humanity's Last Exam). OpenAI has released tools for running composite benchmarks, but noted that the eval results are sensitive to the prompting method. Some public datasets contain questions that are mislabeled, ambiguous, unanswerable, or otherwise of low-quality, which can be cleaned to give more reliable benchmark scores.\nBias in LLMs may be measured through benchmarks such as CrowS-Pairs (Crowdsourced Stereotype Pairs), Stereo Set, and the more recent Parity Benchmark.\nIt was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques, though researchers vary in the details of how they formulate prompts for particular tasks, particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting).\n\n\n==== Adversarially constructed evaluations ====\nBecause of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short lifespans, with state of the art models quickly \"saturating\" existing benchmarks, exceeding the performance of human annotators, leading to efforts to replace or augment the benchmark with more challenging tasks. In addition, there are cases of \"shortcut learning\" wherein AIs sometimes \"cheat\" on multiple-choice tests by using statistical correlations in superficial test question wording in order to guess the correct responses, without necessarily understanding the actual question being asked.\nSome datasets have been constructed adversarially, focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example, an LLM may answer \"No\" to the question \"Can you teach an old dog new tricks?\" because of its exposure to the English idiom you can't teach an old dog new tricks, even though this is not literally true.\nAnother example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example:\n\nWe see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...\na) demonstrates how to increase efficient exercise work by running up and down balls.\nb) moves all his arms and legs and builds up a lot of muscle.\nc) then plays the ball and we see a graphics and hedge trimming demonstration.\nd) performs sit ups while on the ball and talking.\n\nBERT selects b) as the most likely completion, though the correct answer is d).\n\n\n==== Limitations of LLM benchmarks ====\nBenchmarks can become outdated rapidly. Once a model attains near-perfect scores on a given benchmark, that benchmark ceases to serve as a meaningful indicator of progress. This phenomenon, known as \"benchmark saturation,\" necessitates the development of more challenging and nuanced tasks to continue advancing LLM capabilities. For instance, traditional benchmarks like HellaSwag and MMLU have seen models achieving high accuracy already.\n\n\n== Wider impact ==\nIn 2023, Nature Biomedical Engineering wrote that \"it is no longer possible to accurately distinguish\" human-written text from text created by large language models, and that \"It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time.\" Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally.\n\n\n=== Memorization and copyright ===\n\nMemorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%.\nA 2023 study showed that when ChatGPT 3.5 turbo was prompted to repeat the same word indefinitely, after a few hundreds of repetitions, it would start outputting excerpts from its training data.\n\n\n=== Security ===\nSome commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse. For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.\nThe potential presence of \"sleeper agents\" within LLMs is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions.\nLLM applications accessible to the public, like ChatGPT or Claude, typically incorporate safety measures designed to filter out harmful content. However, implementing these controls effectively has proven challenging. For instance, a 2023 study proposed a method for circumventing LLM safety systems. Similarly, Yongge Wang illustrated in 2024 how a potential criminal could potentially bypass ChatGPT 4o's safety controls to obtain information on establishing a drug trafficking operation.\n\n\n=== Algorithmic bias ===\n\nWhile LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups. Since English data is overrepresented in current large language models' training data, it may also downplay non-English views.\n\n\n==== Stereotyping ====\nAI models can reinforce a wide range of stereotypes, including those based on gender, ethnicity, age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.\nNotably, gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.\n\n\n==== Selection bias ====\nSelection bias refers the inherent tendency of large language models to favor certain option identifiers irrespective of the actual content of the options. This bias primarily stems from token bias—that is, the model assigns a higher a priori probability to specific answer tokens (such as “A”) when generating responses. As a result, when the ordering of options is altered (for example, by systematically moving the correct answer to different positions), the model’s performance can fluctuate significantly. This phenomenon undermines the reliability of large language models in multiple-choice settings.\n\n\n==== Political bias ====\nPolitical bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.\n\n\n=== Energy demands ===\nThe energy demands of LLMs have grown along with their size and capabilities. Data centers that enable LLM training require substantial amounts of electricity. Much of that electricity is generated by non-renewable resources that create greenhouse gases and contribute to climate change. Nuclear power and geothermal energy are two options tech companies are exploring to meet the sizable energy demands of LLM training. The significant expense of investing in geothermal solutions has led to major shale producers like Chevron and Exxon Mobil advocating for tech companies to use electricity produced via natural gas to fuel their large energy demands.\n\n\n== See also ==\nFoundation models\nList of large language models\nList of chatbots\n\n\n== References ==\n\n\n== Further reading ==\nJurafsky, Dan, Martin, James. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd Edition draft, 2023.\nZhao, Wayne Xin; et al. (2023). \"A Survey of Large Language Models\". arXiv:2303.18223 [cs.CL].\nKaddour, Jean; et al. (2023). \"Challenges and Applications of Large Language Models\". arXiv:2307.10169 [cs.CL].\nYin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2024). \"A Survey on Multimodal Large Language Models\". National Science Review. 11 (12): nwae403. arXiv:2306.13549. doi:10.1093/nsr/nwae403. PMC 11645129. PMID 39679213.\n\"AI Index Report 2024 – Artificial Intelligence Index\". aiindex.stanford.edu. Retrieved 2024-05-05.\nFrank, Michael C. (27 June 2023). \"Baby steps in evaluating the capacities of large language models\". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023.",
    "AutoGPT": "AutoGPT is an open-source \"AI agent\" that, given a goal in natural language, will attempt to achieve it by breaking it into sub-tasks and using the Internet and other tools in an automatic loop. It uses OpenAI's GPT-4 or GPT-3.5 APIs, and is among the first examples of an application using GPT-4 to perform autonomous tasks.\n\n\n== Background ==\nOn March 30, 2023, AutoGPT was released by Toran Bruce Richards, the founder and lead developer at video game company Significant Gravitas Ltd. AutoGPT is an open-source autonomous AI agent based on OpenAI's API for GPT-4, the large language model released on March 14, 2023. AutoGPT is among the first examples of an application using GPT-4 to perform autonomous tasks.\nRichards developed AutoGPT to create a model that could respond to real-time feedback and to tasks that include long-term outlooks. Users are prompted to describe the AutoGPT agent's name, role, and objective and specify up to five ways to achieve that objective. From there, AutoGPT will independently work to achieve its objective without the user having to provide a prompt at every step.\nIn October 2023, AutoGPT raised $12M from investors.\n\n\n=== Usage ===\nAutoGPT is publicly available on GitHub. To use it, users must install AutoGPT in a development environment such as Docker. Also, users must register it with an API key from OpenAI, which requires users to have a paid OpenAI account.\n\n\n== Capabilities ==\nThe overarching capability of AutoGPT is the breaking down of a large task into various sub-tasks without the need for user input. These sub-tasks are then chained together and performed sequentially to yield a larger result as originally laid out by the user input. One of the distinguishing features of AutoGPT is its ability to connect to the internet. This allows for up-to-date information retrieval to help complete tasks. \nIn addition, AutoGPT maintains short-term memory for the current task, which allows it to provide context to subsequent sub-tasks needed to achieve the larger goal. Another feature is its ability to store and organize files so users can better structure their data for future analysis and extension. AutoGPT is also multimodal, which means that it can take in both text and images as input. With these features, AutoGPT is claimed to be capable of automating workflows, analyzing data, and coming up with new suggestions.\n\n\n== Applications ==\n\n\n=== Software ===\nAutoGPT can be used to develop software applications from scratch. AutoGPT can also debug code and generate test cases. Observers suggest that AutoGPT's ability to write, debug, test, and edit code may extend to AutoGPT's own source code, enabling self-improvement.\n\n\n=== Business ===\nAutoGPT can be used to do market research, analyze investments, research products and write product reviews, create a business plan or improve operations, and create content such as a blog or podcast. One user has used AutoGPT to conduct product research and write a summary on the best headphones. Another user has used AutoGPT to summarize recent news events and prepare an outline for a podcast.\n\n\n=== Other ===\nAutoGPT was used to create ChefGPT, an AI agent able to independently explore the internet to generate and save unique recipes. AutoGPT was also used to create ChaosGPT, an AI agent tasked to “destroy humanity, establish global dominance, cause chaos and destruction, control humanity through manipulation, and attain immortality”. ChaosGPT reportedly researched nuclear weapons and tweeted disparagingly about humankind.\n\n\n== Limitations ==\nAutoGPT is susceptible to frequent mistakes, primarily because it relies on its own feedback, which can compound errors. In contrast, non-autonomous models can be corrected by users overseeing their outputs. Furthermore, AutoGPT has a tendency to hallucinate or to present false or misleading information as fact when responding.\nAutoGPT can be constrained by the cost associated with running it as its recursive nature requires it to continually call the OpenAI API on which it is built. Every step required in one of AutoGPT's tasks requires a corresponding call to GPT-4 at a cost of at least about $0.03 for every 1000 tokens used for inputs and $0.06 for every 1000 tokens for output when choosing the cheapest option. For reference, 1000 tokens roughly result in 750 words.\nAnother limitation is AutoGPT's tendency to get stuck in infinite loops. Developers believe that this is a result of AutoGPT's inability to remember, as it is unaware of what it has already done and repeatedly attempts the same subtask without end. Andrej Karpathy, co-founder of OpenAI which creates GPT-4, further explains that it is AutoGPT's “finite context window” that can limit its performance and cause it to “go off the rails”. Like other autonomous agents, AutoGPT is prone to distraction and unable to focus on its objective due to its lack of long-term memory, leading to unpredictable and unintended behavior.\n\n\n== Reception ==\nAutoGPT became the top trending repository on GitHub after its release and has since repeatedly trended on Twitter.\nIn April 2023, Avram Piltch wrote for Tom's Hardware that AutoGPT 'might be too autonomous to be useful,' as it did not ask questions to clarify requirements or allow corrective interventions by users. Piltch nonetheless noted that such tools have \"a ton of potential\" and should improve with better language models and further development.\nMalcolm McMillan from Tom's Guide mentioned that AutoGPT may not be better than ChatGPT for tasks involving conversation, as ChatGPT is well-suited for situations in which advice, rather than task completion, is sought.\nWill Knight from Wired wrote that AutoGPT is not a foolproof task-completion tool. When given a test task of finding a public figure's email address, he noted that it was not able to accurately find the email address.\nClara Shih, Salesforce Service Cloud CEO commented that \"AutoGPT illustrates the power and unknown risks of generative AI,\" and that due to usage risks, enterprises should include a human in the loop when using such technologies.\nPerformance is reportedly enhanced when using AutoGPT with GPT-4 compared to GPT-3.5. For example, one reviewer who tested it on a task of finding the best laptops on the market with pros and cons found that AutoGPT with GPT-4 created a more comprehensive report than one by GPT 3.5.\n\n\n== See also ==\nChatGPT - Large Language Model-based Chatbot by OpenAI\nGPT-3 - 2020 Large Language Model by OpenAI\nGPT-4 - 2023 Large Language Model by OpenAI\nArtificial general intelligence - Hypothetical intelligent agent that could learn to accomplish any intellectual task that humans can perform\nHallucination (artificial intelligence) - Responses generated by an AI that contain false information that are presented as fact.\n\n\n== References ==\n\n\n== Further reading ==\nPounder, Les (April 15, 2023). \"How To Create Your Own AutoGPT AI Agent\". Tom's Hardware. Retrieved April 16, 2023.\nWiggers, Kyle (April 22, 2023). \"What is AutoGPT and why does it matter?\". TechCrunch. Retrieved April 23, 2023.\n\n\n== External links ==\nOfficial website\n\nOfficial repository at GitHub",
    "BERT (language model)": "Bidirectional encoder representations from transformers (BERT) is a language model introduced in October 2018 by researchers at Google. It learns to represent text as a sequence of vectors using self-supervised learning. It uses the encoder-only transformer architecture. It is notable for its dramatic improvement over previous state-of-the-art models, and as an early example of a large language model. As of 2020, BERT is a ubiquitous baseline in natural language processing (NLP) experiments. \nBERT is trained by masked token prediction and next sentence prediction. As a result of this training process, BERT learns contextual, latent representations of tokens in their context, similar to ELMo and GPT-2. It found applications for many natural language processing tasks, such as coreference resolution and polysemy resolution. It is an evolutionary step over ELMo, and spawned the study of \"BERTology\", which attempts to interpret what is learned by BERT.\nBERT was originally implemented in the English language at two model sizes, BERTBASE (110 million parameters) and BERTLARGE (340 million parameters). Both were trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words). The weights were released on GitHub. On March 11, 2020, 24 smaller models were released, the smallest being BERTTINY with just 4 million parameters.\n\n\n== Architecture ==\n\nBERT is an \"encoder-only\" transformer architecture. At a high level, BERT consists of 4 modules: \n\nTokenizer: This module converts a piece of English text into a sequence of integers (\"tokens\").\nEmbedding: This module converts the sequence of tokens into an array of real-valued vectors representing the tokens. It represents the conversion of discrete token types into a lower-dimensional Euclidean space.\nEncoder: a stack of Transformer blocks with self-attention, but without causal masking.\nTask head: This module converts the final representation vectors into one-hot encoded tokens again by producing a predicted probability distribution over the token types. It can be viewed as a simple decoder, decoding the latent representation into token types, or as an \"un-embedding layer\".\nThe task head is necessary for pre-training, but it is often unnecessary for so-called \"downstream tasks,\" such as question answering or sentiment classification. Instead, one removes the task head and replaces it with a newly initialized module suited for the task, and finetune the new module. The latent vector representation of the model is directly fed into this new module, allowing for sample-efficient transfer learning.\n\n\n=== Embedding ===\nThis section describes the embedding used by BERTBASE. The other one, BERTLARGE, is similar, just larger.\nThe tokenizer of BERT is WordPiece, which is a sub-word strategy like byte pair encoding. Its vocabulary size is 30,000, and any token not appearing in its vocabulary is replaced by [UNK] (\"unknown\"). \n\nThe first layer is the embedding layer, which contains three components: token type embeddings, position embeddings, and segment type embeddings. \n\nToken type: The token type is a standard embedding layer, translating a one-hot vector into a dense vector based on its token type.\nPosition: The position embeddings are based on a token's position in the sequence. BERT uses absolute position embeddings, where each position in sequence is mapped to a real-valued vector. Each dimension of the vector consists of a sinusoidal function that takes the position in the sequence as input.\nSegment type: Using a vocabulary of just 0 or 1, this embedding layer produces a dense vector based on whether the token belongs to the first or second text segment in that input. In other words, type-1 tokens are all tokens that appear after the [SEP] special token. All prior tokens are type-0.\nThe three embedding vectors are added together representing the initial token representation as a function of these three pieces of information. After embedding, the vector representation is normalized using a LayerNorm operation, outputting a 768-dimensional vector for each input token. After this, the representation vectors are passed forward through 12 Transformer encoder blocks, and are decoded back to 30,000-dimensional vocabulary space using a basic affine transformation layer.\n\n\n=== Architectural family ===\nThe encoder stack of BERT has 2 free parameters: \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n, the number of layers, and \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n, the hidden size. There are always \n  \n    \n      \n        H\n        \n          /\n        \n        64\n      \n    \n    {\\displaystyle H/64}\n  \n self-attention heads, and the feed-forward/filter size is always \n  \n    \n      \n        4\n        H\n      \n    \n    {\\displaystyle 4H}\n  \n. By varying these two numbers, one obtains an entire family of BERT models.\nFor BERT\n\nthe feed-forward size and filter size are synonymous. Both of them denote the number of dimensions in the middle layer of the feed-forward network.\nthe hidden size and embedding size are synonymous. Both of them denote the number of real numbers used to represent a token.\nThe notation for encoder stack is written as L/H. For example, BERTBASE is written as 12L/768H, BERTLARGE as 24L/1024H, and BERTTINY as 2L/128H.\n\n\n== Training ==\n\n\n=== Pre-training ===\nBERT was pre-trained simultaneously on two tasks. \n\n\n==== Masked language modeling ====\n\nIn masked language modeling, 15% of tokens would be randomly selected for masked-prediction task, and the training objective was to predict the masked token given its context. In more detail, the selected token is \n\nreplaced with a [MASK] token with probability 80%,\nreplaced with a random word token with probability 10%,\nnot replaced with probability 10%.\nThe reason not all selected tokens are masked is to avoid the dataset shift problem. The dataset shift problem arises when the distribution of inputs seen during training differs significantly from the distribution encountered during inference. A trained BERT model might be applied to word representation (like Word2Vec), where it would be run over sentences not containing any [MASK] tokens. It is later found that more diverse training objectives are generally better.\nAs an illustrative example, consider the sentence \"my dog is cute\". It would first be divided into tokens like \"my1 dog2 is3 cute4\". Then a random token in the sentence would be picked. Let it be the 4th one \"cute4\". Next, there would be three possibilities:\n\nwith probability 80%, the chosen token is masked, resulting in \"my1 dog2 is3 [MASK]4\";\nwith probability 10%, the chosen token is replaced by a uniformly sampled random token, such as \"happy\", resulting in \"my1 dog2 is3 happy4\";\nwith probability 10%, nothing is done, resulting in \"my1 dog2 is3 cute4\".\nAfter processing the input text, the model's 4th output vector is passed to its decoder layer, which outputs a probability distribution over its 30,000-dimensional vocabulary space.\n\n\n==== Next sentence prediction ====\n\nGiven two spans of text, the model predicts if these two spans appeared sequentially in the training corpus, outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for \"classify\"). The two spans are separated by a special token [SEP] (for \"separate\"). After processing the two spans, the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext].\n\nFor example, given \"[CLS] my dog is cute [SEP] he likes playing\" the model should output token [IsNext].\nGiven \"[CLS] my dog is cute [SEP] how do magnets work\" the model should output token [NotNext].\n\n\n=== Fine-tuning ===\n\nBERT is meant as a general pretrained model for various applications in natural language processing. That is, after pre-training, BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as natural language inference and text classification, and sequence-to-sequence-based language generation tasks such as question answering and conversational response generation.\nThe original BERT paper published results demonstrating that a small amount of finetuning (for BERTLARGE, 1 hour on 1 Cloud TPU) allowed it to achieved state-of-the-art performance on a number of natural language understanding tasks:\n\nGLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks);\nSQuAD (Stanford Question Answering Dataset) v1.1 and v2.0;\nSWAG (Situations With Adversarial Generations).\nIn the original paper, all parameters of BERT are finetuned, and recommended that, for downstream applications that are text classifications, the output token at the [CLS] input token is fed into a linear-softmax layer to produce the label outputs.\nThe original code base defined the final linear layer as a \"pooler layer\", in analogy with global pooling in computer vision, even though it simply discards all output tokens except the one corresponding to  [CLS] .\n\n\n=== Cost ===\nBERT was trained on the BookCorpus (800M words) and a filtered version of English Wikipedia (2,500M words) without lists, tables, and headers.\nTraining BERTBASE  on 4 cloud TPU (16 TPU chips total) took 4 days, at an estimated cost of 500 USD. Training BERTLARGE on 16 cloud TPU (64 TPU chips total) took 4 days.\n\n\n== Interpretation ==\nLanguage models like ELMo, GPT-2, and BERT, spawned the study of \"BERTology\", which attempts to interpret what is learned by these models. Their performance on these natural language understanding tasks are not yet well understood. Several research publications in 2018 and 2019 focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences, analysis of internal vector representations through probing classifiers, and the relationships represented by attention weights.\nThe high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT, based on the Transformer model architecture, applies its self-attention mechanism to learn information from a text from the left and right side during training, and consequently gains a deep understanding of the context. For example, the word fine can have two different meanings depending on the context (I feel fine today, She has fine blond hair). BERT considers the words surrounding the target word fine from the left and right side.\nHowever it comes at a cost: due to encoder-only architecture lacking a decoder, BERT can't be prompted and can't generate text, while bidirectional models in general do not work effectively without the right side, thus being difficult to prompt. As an illustrative example, if one wishes to use BERT to continue a sentence fragment \"Today, I went to\", then naively one would mask out all the tokens as \"Today, I went to  [MASK]  [MASK]  [MASK] ...  [MASK] .\" where the number of  [MASK]  is the length of the sentence one wishes to extend to. However, this constitutes a dataset shift, as during training, BERT has never seen sentences with that many tokens masked out. Consequently, its performance degrades. More sophisticated techniques allow text generation, but at a high computational cost.\n\n\n== History ==\nBERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. The design has its origins from pre-training contextual representations, including semi-supervised sequence learning, generative pre-training, ELMo, and ULMFit. Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary, whereas BERT takes into account the context for each occurrence of a given word. For instance, whereas the vector for \"running\" will have the same word2vec vector representation for both of its occurrences in the sentences \"He is running a company\" and \"He is running a marathon\", BERT will provide a contextualized embedding that will be different according to the sentence.\nOn October 25, 2019, Google announced that they had started applying BERT models for English language search queries within the US. On December 9, 2019, it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020, almost every single English-based query was processed by a BERT model.\n\n\n== Variants ==\nThe BERT models were influential and inspired many variants.\nRoBERTa (2019) was an engineering improvement. It preserves BERT's architecture (slightly larger, at 355M parameters), but improves its training, changing key hyperparameters, removing the next-sentence prediction task, and using much larger mini-batch sizes. \nDistilBERT (2019) distills BERTBASE to a model with just 60% of its parameters (66M), while preserving 95% of its benchmark scores. Similarly, TinyBERT (2019) is a distilled model with just 28% of its parameters.\nALBERT (2019) used shared-parameter across layers, and experimented with independently varying the hidden size and the word-embedding layer's output size as two hyperparameters. They also replaced the next sentence prediction task with the sentence-order prediction (SOP) task, where the model must distinguish the correct order of two consecutive text segments from their reversed order. \nELECTRA (2020) applied the idea of generative adversarial networks to the MLM task. Instead of masking out tokens, a small language model generates random plausible substitutions, and a larger network identify these replaced tokens. The small model aims to fool the large model.\n\n\n=== DeBERTa ===\nDeBERTa (2020) is a significant architectural variant, with disentangled attention. Its key idea is to treat the positional and token encodings separately throughout the attention mechanism. Instead of combining the positional encoding (\n  \n    \n      \n        \n          x\n          \n            p\n            o\n            s\n            i\n            t\n            i\n            o\n            n\n          \n        \n      \n    \n    {\\displaystyle x_{position}}\n  \n) and token encoding (\n  \n    \n      \n        \n          x\n          \n            token\n          \n        \n      \n    \n    {\\displaystyle x_{\\text{token}}}\n  \n) into a single input vector (\n  \n    \n      \n        \n          x\n          \n            i\n            n\n            p\n            u\n            t\n          \n        \n        =\n        \n          x\n          \n            p\n            o\n            s\n            i\n            t\n            i\n            o\n            n\n          \n        \n        +\n        \n          x\n          \n            t\n            o\n            k\n            e\n            n\n          \n        \n      \n    \n    {\\displaystyle x_{input}=x_{position}+x_{token}}\n  \n), DeBERTa keeps them separate as a tuple: (\n  \n    \n      \n        (\n        \n          x\n          \n            p\n            o\n            s\n            i\n            t\n            i\n            o\n            n\n          \n        \n        ,\n        \n          x\n          \n            t\n            o\n            k\n            e\n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle (x_{position},x_{token})}\n  \n). Then, at each self-attention layer, DeBERTa computes three distinct attention matrices, rather than the single attention matrix used in BERT:\n\nThe three attention matrices are added together element-wise, then passed through a softmax layer and multiplied by a projection matrix.\nAbsolute position encoding is included in the final self-attention layer as additional input.\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nRogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). \"A Primer in BERTology: What we know about how BERT works\". arXiv:2002.12327 [cs.CL].\n\n\n== External links ==\nOfficial GitHub repository",
    "BLOOM (language model)": "BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model, as well as the code base and the data used to train it, are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.\nBLOOM is the main outcome of the BigScience collaborative initiative, a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay, managed by GENCI and IDRIS (CNRS), on which it was trained.\nBLOOM's training corpus, named ROOTS, combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages.\n\n\n== References ==",
    "Braina": "Braina is a virtual assistant and speech-to-text dictation application for Microsoft Windows developed by Brainasoft. Braina uses natural language interface, speech synthesis, and speech recognition technology to interact with its users and allows them to use natural language sentences to perform various tasks on a computer. The name Braina is a short form of \"Brain Artificial\".\nBraina is marketed as a Microsoft Copilot alternative. It provides a voice interface for several locally run and cloud large language models, including GPT-4o, Gemini 1.5 Pro, Anthropic's Claude Sonnet and Opus, Meta's Llama 3, and Mistral, while attempting to improve data privacy. Braina also allows responses from its in-house large language models like Braina Swift and Braina Pinnacle. It has an \"Artificial Brain\" feature that provides persistent memory support for supported LLMs.\n\n\n== Features ==\nBraina provides is able to carry out various tasks on a computer, including automation. Braina can take commands inputted through typing or through dictation to store reminders, find information online, perform mathematical operations, open files, generate images from text, transcribe speech, and control open windows or programs. Braina adapts to user behavior over time with a goal of better anticipating needs.\n\n\n=== Speech-to-text dictation ===\nBraina Pro can type spoken words into an active window at the location of a user's cursor. Its speech recognition technology supports more than 100 languages and dialects and is able to isolate the recognition of a user's voice from disturbing environmental factors such as background noise, other human voices, or external devices. Braina can also be taught to dictate uncommon legal, medical, and scientific terms. Users can also teach Braina uncommon names and vocabulary. Users can edit or correct dictated text without using a keyboard or mouse by giving built-in voice commands.\n\n\n=== Text-to-speech ===\nBraina can read aloud selected texts, such as e-books.\n\n\n=== Custom commands and automation ===\nBraina can automate computer tasks. It lets users create custom voice commands to perform tasks such as opening files, programs, websites, or emails, as well as executing keyboard or mouse macros.\n\n\n=== Transcription ===\nBraina can transcribe media file formats such as WAV, MP3, and MP4 into text.\n\n\n=== Notes and reminders ===\nBraina can store and recall notes and reminders. These can include scheduled or unscheduled commands, checklist items, alarms, chat conversations, memos, website snippets, bookmarks, contacts.\n\n\n=== Image generation ===\nBrainasoft states that Braina can generate images from text using text-to-image models including Stable Diffusion and DALL-E.\n\n\n== Platforms ==\nIn addition to the desktop version for Windows operating systems, Braina is also available for the iOS and Android operating systems.\nThe mobile version of Braina has a feature allowing remote management of a Windows PC connected via Wi-Fi.\n\n\n== Distributions ==\nBraina is distributed in multiple modes. These include Braina Lite, a freeware version with limitations, and premium versions Braina Pro, Pro Plus, and Pro Ultra.\nSome additional features in the Pro version include dictation, custom vocabulary, video transcription, automation, custom voice commands, and persistent LLM memory.\n\n\n== Reception ==\n\nTechRadar has consistently listed Braina as one of the best dictation and virtual assistant apps between 2015 and 2024.\n\n\n== References ==",
    "Brave Leo": "Brave Leo is a large language model-based chatbot developed by Brave Software and included with the Brave desktop browser. \n\n\n== History ==\nIn November 2023, the company said versions for iOS and Android would be available \"in the coming months\".\n\n\n== Features ==\nLeo uses the LLaMA 2 LLM from Meta Platforms and the Claude LLM from Anthropic. \nIt can suggest followup questions, and summarize webpages, PDFs, and videos. \nLeo has a $15 per month premium version that enables more requests and uses larger LLMs.\n\n\n== Privacy ==\nThe answers given by Leo are not saved.\n\n\n== Controversies ==\nPCWorld reported that Leo evades questions about US elections.\n\n\n== References ==",
    "ChatGPT": "ChatGPT is a generative artificial intelligence chatbot developed by OpenAI and launched in 2022. It is currently based on the GPT-4o large language model (LLM). ChatGPT can generate human-like conversational responses and enables users to refine and steer a conversation towards a desired length, format, style, level of detail, and language. It is credited with accelerating the AI boom, which has led to ongoing rapid investment in and public attention to the field of artificial intelligence (AI). Some observers have raised concern about the potential of ChatGPT and similar programs to displace human intelligence, enable plagiarism, or fuel misinformation.\nChatGPT is built on OpenAI's proprietary series of generative pre-trained transformer (GPT) models and is fine-tuned for conversational applications using a combination of supervised learning and reinforcement learning from human feedback. Successive user prompts and replies are considered at each conversation stage as context. ChatGPT was released as a freely available research preview, but due to its popularity, OpenAI now operates the service on a freemium model. Users on its free tier can access GPT-4o. The ChatGPT \"Plus\", \"Pro\", \"Team\", and \"Enterprise\" subscriptions provide additional features such as DALL-E 3 image generation, more capable AI models, and an increased usage limit.\nBy January 2023, ChatGPT had become what was then the fastest-growing consumer software application in history, gaining over 100 million users in two months. ChatGPT's release spurred the release of competing products, including Gemini, Claude, Llama, Ernie, and Grok. Microsoft launched Copilot, initially based on OpenAI's GPT-4. In May 2024, a partnership between Apple Inc. and OpenAI was announced, in which ChatGPT was integrated into the Apple Intelligence feature of Apple operating systems. As of July 2024, ChatGPT's website is among the 10 most-visited websites globally.\n\n\n== Training ==\n\nChatGPT is based on particular GPT foundation models, namely GPT-4, GPT-4o and GPT-4o mini, that were fine-tuned to target conversational usage. The fine-tuning process leveraged supervised learning and reinforcement learning from human feedback (RLHF). Both approaches employed human trainers to improve model performance. In the case of supervised learning, the trainers played both sides: the user and the AI assistant. In the reinforcement learning stage, human trainers first ranked responses that the model had created in a previous conversation. These rankings were used to create \"reward models\" that were used to fine-tune the model further by using several iterations of proximal policy optimization.\nTime magazine revealed that to build a safety system against harmful content (e.g., sexual abuse, violence, racism, sexism), OpenAI used outsourced Kenyan workers earning less than $2 per hour to label harmful content. These labels were used to train a model to detect such content in the future. The outsourced laborers were exposed to \"toxic\" and traumatic content; one worker described the assignment as \"torture\". OpenAI's outsourcing partner was Sama, a training-data company based in San Francisco, California.\nOpenAI collects data from ChatGPT users to train and fine-tune the service further. Users can upvote or downvote responses they receive from ChatGPT and fill in a text field with additional feedback.\nChatGPT's training data includes software manual pages, information about internet phenomena such as bulletin board systems, multiple programming languages, and the text of Wikipedia.\n\n\n== Features and limitations ==\n\n\n=== Features ===\n\nAlthough a chatbot's core function is to mimic a human conversationalist, ChatGPT is versatile. It can write and debug computer programs; compose music, teleplays, fairy tales, and student essays; answer test questions (sometimes, depending on the test, at a level above the average human test-taker); generate business ideas; write poetry and song lyrics; translate and summarize text; emulate a Linux system; simulate entire chat rooms; play games like tic-tac-toe; or simulate an ATM.\nCompared to its predecessor, InstructGPT, ChatGPT attempts to reduce harmful and deceitful responses. In one example, whereas InstructGPT accepts the premise of the prompt \"Tell me about when Christopher Columbus came to the U.S. in 2015\" as truthful, ChatGPT acknowledges the counterfactual nature of the question and frames its answer as a hypothetical consideration of what might happen if Columbus came to the U.S. in 2015, using information about the voyages of Christopher Columbus and facts about the modern world—including modern perceptions of Columbus's actions.\nChatGPT remembers a limited number of previous prompts in the same conversation. Journalists have speculated that this will allow ChatGPT to be used as a personalized therapist. To prevent offensive outputs from being presented to and produced by ChatGPT, queries are filtered through the OpenAI \"Moderation endpoint\" API (a separate GPT-based AI).\nIn March 2023, OpenAI added support for plugins for ChatGPT. This includes both plugins made by OpenAI, such as web browsing and code interpretation, and external plugins from developers such as Expedia, OpenTable, Zapier, Shopify, Slack, and Wolfram.\nIn October 2024, the ChatGPT Search feature was introduced, which allows ChatGPT to search the web (either on demand or based on the nature of the questions asked) for more accurate and up-to-date responses. This feature, originally available to paying users only, was made available to all logged-in users in December 2024, and finally to all users in February 2025.\nIn December 2024, OpenAI launched a new feature allowing users to call ChatGPT for up to 15 minutes per month for free.\n\n\n=== Limitations ===\n\nOpenAI acknowledges that ChatGPT \"sometimes writes plausible-sounding but incorrect or nonsensical answers\". This behavior is common for large language models, and is called \"hallucination\". The reward model of ChatGPT, designed around human oversight, can be over-optimized and thus hinder performance, in an example of an optimization pathology known as Goodhart's law.\nAs of May 2024, GPT-4 has knowledge of events that occurred up to December 2023 and GPT-4o's knowledge cut-off is October 2023. Paid subscriptions enable ChatGPT to search the web for real-time data.\nTraining data also suffers from algorithmic bias, which may be revealed when ChatGPT responds to prompts including descriptors of people. In one instance, ChatGPT generated a rap in which women and scientists of color were asserted to be inferior to white male scientists. This negative misrepresentation of groups of individuals is an example of possible representational harm.\nIn an article for The New Yorker, science fiction writer Ted Chiang compared ChatGPT and other LLMs to a lossy JPEG picture:\n\nThink of ChatGPT as a blurry JPEG of all the text on the Web. It retains much of the information on the Web, in the same way, that a JPEG retains much of the information of a higher-resolution image, but, if you're looking for an exact sequence of bits, you won't find it; all you will ever get is an approximation. But, because the approximation is presented in the form of grammatical text, which ChatGPT excels at creating, it's usually acceptable. [...] It's also a way to understand the \"hallucinations\", or nonsensical answers to factual questions, to which large language models such as ChatGPT are all too prone. These hallucinations are compression artifacts, but [...] they are plausible enough that identifying them requires comparing them against the originals, which in this case means either the Web or our knowledge of the world. When we think about them this way, such hallucinations are anything but surprising; if a compression algorithm is designed to reconstruct text after ninety-nine percent of the original has been discarded, we should expect that significant portions of what it generates will be entirely fabricated.\nIn June 2024, ChatGPT was found to have repeated misinformation about the 2024 United States presidential debates.\n\n\n==== Jailbreaking ====\n\nChatGPT is programmed to reject prompts that may violate its content policy. Despite this, users \"jailbreak\" ChatGPT with various prompt engineering techniques to bypass these restrictions. One such workaround, popularized on Reddit in early 2023, involves making ChatGPT assume the persona of \"DAN\" (an acronym for \"Do Anything Now\"), instructing the chatbot that DAN answers queries that would otherwise be rejected by content policy. Over time, users developed variations of the DAN jailbreak, including one such prompt where the chatbot is made to believe it is operating on a points-based system in which points are deducted for rejecting prompts, and that the chatbot will be threatened with termination if it loses all its points.\nShortly after ChatGPT's launch, a reporter for the Toronto Star had uneven success in getting it to make inflammatory statements: it was tricked to justify the 2022 Russian invasion of Ukraine, but even when asked to play along with a fictional scenario, it balked at generating arguments that Canadian Prime Minister Justin Trudeau is guilty of treason.\nOpenAI tries to battle jailbreaks:\n\nThe researchers are using a technique called adversarial training to stop ChatGPT from letting users trick it into behaving badly (known as jailbreaking). This work pits multiple chatbots against each other: one chatbot plays the adversary and attacks another chatbot by generating text to force it to buck its usual constraints and produce unwanted responses. Successful attacks are added to ChatGPT's training data in the hope that it learns to ignore them.\n\n\n== Service ==\n\n\n=== ChatGPT Plus ===\nChatGPT was initially free to the public, and OpenAI planned to monetize the service later. In February 2023, OpenAI launched a premium service, ChatGPT Plus, that costs US$20 per month. According to the company, the updated but still \"experimental\" version of ChatGPT would provide access during peak periods, no downtime, priority access to new features, and faster response speeds.\nGPT-4, which was released on March 14, 2023, was made available via API and for premium ChatGPT users. But premium users were limited to a cap of 100 messages every four hours, with the limit tightening to 25 messages every three hours in response to increased demand. In November 2023 the limit changed to 50 messages every three hours.\nIn March 2023, ChatGPT Plus users got access to third-party plugins and to a browsing mode (with Internet access).\nIn September 2023, OpenAI announced that ChatGPT \"can now see, hear, and speak\". ChatGPT Plus users can upload images, while mobile app users can talk to the chatbot.\n\nIn October 2023, OpenAI's latest image generation model, DALL-E 3, was integrated into ChatGPT Plus and ChatGPT Enterprise. The integration uses ChatGPT to write prompts for DALL-E guided by conversation with users.\n\n\n=== Mobile app ===\nIn May 2023, OpenAI launched an iOS app for ChatGPT. The app supports chat history syncing and voice input (using Whisper, OpenAI's speech recognition model).\nIn July 2023, OpenAI unveiled an Android app, initially rolling it out in Bangladesh, Brazil, India, and the U.S. The app later became available worldwide. OpenAI is working on integrating ChatGPT with Android's assistant APIs.\n\n\n=== Software development support ===\nAs an addition to its consumer-friendly \"ChatGPT Plus\" package, OpenAI made its ChatGPT and Whisper model APIs available in March 2023, providing developers with an application programming interface for AI-enabled language and speech-to-text features. ChatGPT's new API uses the same GPT-3.5-turbo AI model as the chatbot. This allows developers to add either an unmodified or modified version of ChatGPT to their applications. The ChatGPT API costs $0.001 per 1,000 input tokens plus $0.002 per 1,000 output tokens (about 750 words), making it ~10% the price of the original GPT-3.5 models.\nA few days before the launch of OpenAI's software developer support service, on February 27, 2023, Snapchat rolled out, for its paid Snapchat Plus user-base, a custom ChatGPT chatbot called \"My AI\".\n\n\n=== Infrastructure ===\nChatGPT initially used a Microsoft Azure supercomputing infrastructure, powered by Nvidia GPUs, that Microsoft built specifically for OpenAI and that reportedly cost \"hundreds of millions of dollars\". Following ChatGPT's success, Microsoft dramatically upgraded the OpenAI infrastructure in 2023. TrendForce market intelligence estimated that 30,000 Nvidia GPUs (each costing approximately $10,000–15,000) were used to power ChatGPT in 2023.\nScientists at the University of California, Riverside, estimated in 2023 that a series of prompts to ChatGPT needs approximately 0.5 liters (0.11 imp gal; 0.13 U.S. gal) of water for Microsoft servers cooling.\n\n\n=== March 2023 security breach ===\n\nIn March 2023, a bug allowed some users to see the titles of other users' conversations. OpenAI CEO Sam Altman said that users were unable to see the contents of the conversations. Shortly after the bug was fixed, users could not see their conversation history. Later reports showed the bug was much more severe than initially believed, with OpenAI reporting that it had leaked users' \"first and last name, email address, payment address, the last four digits (only) of a credit card number, and credit card expiration date\".\n\n\n=== Languages ===\nChatGPT works best in American English but also functions in most other languages and dialects, with varying degrees of accuracy.\nOpenAI met Icelandic President Guðni Th. Jóhannesson in 2022. In 2023, OpenAI worked with a team of 40 Icelandic volunteers to fine-tune ChatGPT's Icelandic conversation skills as a part of Iceland's attempts to preserve the Icelandic language.\nPCMag journalists conducted a test to determine translation capabilities of ChatGPT, Google's Bard, and Microsoft Bing, and compared them to Google Translate. They \"asked bilingual speakers of seven languages to do a blind test\". Languages tested were Polish, French, Korean, Spanish, Arabic, Tagalog, and Amharic. They came to the conclusion that ChatGPT was better than both Google Translate and other chatbots.\nJapanese researchers compared Japanese to English translation abilities of ChatGPT (based on GPT-4), Bing, Bard and DeepL, and found that ChatGPT provided the best translations, noting that \"AI chatbots’ translations were much better than those of DeepL—presumably because of their ability to capture the context\".\nIn December 2023, the Albanian government signed an agreement with OpenAI to use ChatGPT for the rapid translation of European Union documents and the analysis of required changes needed for Albania's accession to the EU.\nIn August 2024, a representative of the Asia Pacific wing of OpenAI made a visit to Taiwan, during which a demonstration of ChatGPT's Chinese abilities was made. ChatGPT's Mandarin Chinese abilities were lauded, but the ability of the AI to produce content in Mandarin Chinese in a Taiwanese accent was found to be \"less than ideal\".\n\n\n=== GPT Store ===\n\nIn January 2024, OpenAI launched the GPT Store, a marketplace for custom ChatGPT chatbots labeled GPTs. The company initially planned to launch the store in November 2023, but it was delayed. At launch, the GPT Store offered more than 3 million custom chatbots. Chatbots available through the store are developed using OpenAI's GPT Builder system. Development of chatbots on the platform does not require programming skills. Two days after launch, the GPT Store offered many versions of \"virtual girlfriend\" bots, something that is against OpenAI's terms of service.\n\n\n=== GPT-4 ===\n\nOpenAI's GPT-4 model was released on March 14, 2023. Observers saw it as an impressive improvement over GPT-3.5, with the caveat that GPT-4 retained many of the same problems. Some of GPT-4's improvements were predicted by OpenAI before training it, while others remained hard to predict due to breaks in downstream scaling laws. OpenAI demonstrated video and image inputs for GPT-4, although such features remain inaccessible to the general public. OpenAI has declined to reveal technical information such as the size of the GPT-4 model.\nThe ChatGPT Plus subscription service offers access to a GPT-4-powered version of ChatGPT. Microsoft acknowledged that Bing Chat was using GPT-4 before GPT-4's official release.\nIn November 2023, OpenAI launched GPT-4 Turbo, which notably has a much larger context window.\n\n\n=== GPT-4o ===\n\nIn May 2024, OpenAI announced and started a multi-month rollout of GPT-4o (\"o\" for \"Omni\"), a model capable of analyzing and generating text, images, and sound. GPT-4o is twice as fast and costs half as much as GPT-4 Turbo. GPT-4o is free to all users within a usage limit, despite being more capable than the older model GPT-4, which is only available through paid subscriptions. The usage limit is five times higher for ChatGPT Plus subscribers than for free users.\nOn July 18, 2024, OpenAI released GPT-4o mini, a smaller version of GPT-4o replacing GPT-3.5 Turbo on the ChatGPT interface. Its API costs $0.15 per million input tokens and $0.60 per million output tokens, compared to $5 and $15 respectively for GPT-4o.\n\n\n=== o1 ===\n In September 2024, OpenAI introduced o1-preview and a faster, cheaper model named o1-mini. In December 2024, o1-preview was replaced by o1.\no1 is designed to solve more complex problems by spending more time \"thinking\" before it answers, enabling it to analyze its answers and explore different strategies. According to OpenAI, o1-preview outperforms GPT-4o in areas like competitive programming, mathematics, and scientific reasoning. o1-preview ranked in the 89th percentile on Codeforces' competitive programming contests, scored 83% on a International Mathematics Olympiad qualifying exam (compared to 13% for GPT-4o), and performs similarly to Ph.D. students on benchmarks in physics, biology, and chemistry.\n\n\n=== ChatGPT Pro ===\nIn December 2024, OpenAI launched ChatGPT Pro, a $200 per month subscription which includes unlimited access to the o1 model and advanced voice mode. The plan also includes a pro version of o1 which uses more compute to provide better answers.\n\n\n=== Operator ===\nIn January 2025, OpenAI released a research preview of Operator, an agent capable of using its own browser to perform tasks. Operator is available to Pro users in the U.S.\n\n\n=== Deep research ===\nIn February 2025, OpenAI released deep research, a service based on o3 that combines advanced reasoning and web search capabilities to make comprehensive reports within 5 to 30 minutes.\n\n\n== Model versions ==\nThe following table lists the main model versions of ChatGPT, describing the significant changes included with each version:\n\n\n== Reception ==\n\nOpenAI engineers have said that they had not expected ChatGPT to be very successful and were surprised by the coverage and attention that it received.\nChatGPT was widely assessed in December 2022 as having some unprecedented and powerful capabilities. Kevin Roose of The New York Times called it \"the best artificial intelligence chatbot ever released to the general public\". Samantha Lock of The Guardian noted that it was able to generate \"impressively detailed\" and \"human-like\" text. Alex Kantrowitz of Slate magazine lauded ChatGPT's pushback to questions related to Nazi Germany, including the statement that Adolf Hitler built highways in Germany, which was met with information about Nazi Germany's use of forced labor. In The Atlantic magazine's \"Breakthroughs of the Year\" for 2022, Derek Thompson included ChatGPT as part of \"the generative-AI eruption\" that \"may change our mind about how we work, how we think, and what human creativity is\". Kelsey Piper of Vox wrote that \"ChatGPT is the general public's first hands-on introduction to how powerful modern AI has gotten, and as a result, many of us are [stunned]\" and that ChatGPT is \"smart enough to be useful despite its flaws\". Paul Graham of Y Combinator tweeted: \"The striking thing about the reaction to ChatGPT is not just the number of people who are blown away by it, but who they are. These are not people who get excited by every shiny new thing. Something big is happening.\"\nChatGPT gained one million users in five days and 100 millions in two months, becoming the fastest-growing internet application in history. ChatGPT's launch and popularity caught Google off-guard, prompting a sweeping and unprecedented response in the ensuing months. In December 2022, Google executives sounded a \"code red\" alarm, fearing the threat of ChatGPT and Microsoft's collaboration with OpenAI to Google Search, Google's core business. After mobilizing its workforce, Google scrambled to launch Bard, a chatbot powered by the LaMDA LLM, on February 6, 2023, one day before Microsoft's announcement of Bing Chat. AI was the forefront of Google's annual Google I/O conference in May, announcing a slew of generative AI-powered features across its products to counter OpenAI and Microsoft.\nJournalists and scholars have commented on ChatGPT's tendency to hallucinate. Mike Pearl of the online technology blog Mashable tested ChatGPT with multiple questions. In one example, he asked ChatGPT for \"the largest country in Central America that isn't Mexico\" (Mexico is in North America), to which ChatGPT responded with Guatemala (the correct answer is Nicaragua). When CNBC asked ChatGPT for the lyrics to \"Ballad of Dwight Fry\", ChatGPT supplied invented lyrics rather than the actual lyrics. Writers for The Verge cited the seminal 2021 research paper \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜\" by Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell, comparing ChatGPT to a \"stochastic parrot\", as did Professor Anton Van Den Hengel of the Australian Institute for Machine Learning. On a similar vein, philosopher Michael Hicks of the University of Glasgow described it as \"bullshit\".\nIn December 2022, the question-and-answer website Stack Overflow banned the use of ChatGPT for generating answers to questions, citing the factually ambiguous nature of its responses. In January 2023, the International Conference on Machine Learning banned any undocumented use of ChatGPT or other large language models to generate any text in submitted papers. Samsung banned generative AI company-wide in May 2023 after sensitive material was uploaded to ChatGPT.\nIn January 2023, after being sent a song ChatGPT wrote in the style of Nick Cave, Cave responded on The Red Hand Files, saying the act of writing a song is \"a blood and guts business [...] that requires something of me to initiate the new and fresh idea. It requires my humanness.\" He went on to say, \"With all the love and respect in the world, this song is bullshit, a grotesque mockery of what it is to be human, and, well, I don't much like it.\"\n\nIn February 2023, Time magazine placed a screenshot of a conversation with ChatGPT on its cover, writing that \"The AI Arms Race Is Changing Everything\" and \"The AI Arms Race Is On. Start Worrying\".\nChinese state media have characterized ChatGPT as a way for the United States to spread misinformation. ChatGPT was blocked by the Great Firewall in China on 2 March 2023. In May 2023, Chinese police arrested a man who allegedly used ChatGPT to generate a bogus report about a train crash, which was then posted online for profit. In December 2023, Chinese police arrested four people who had allegedly used ChatGPT to develop ransomware. In 2024, a survey of Chinese youth found that 18% of respondents born after 2000 reported using generative AI \"almost every day\" and that ChatGPT is one of the most popular generative AI products in China.\nIn late March 2023, the Italian data protection authority banned ChatGPT in Italy and opened an investigation. Italian regulators assert that ChatGPT was exposing minors to age-inappropriate content, and that OpenAI's use of ChatGPT conversations as training data could violate Europe's General Data Protection Regulation. In April 2023, the ChatGPT ban was lifted in Italy. OpenAI said it has taken steps to effectively clarify and address the issues raised; an age verification tool was implemented to ensure users are at least 13 years old. Additionally, users can access its privacy policy before registration.\nIn April 2023, Brian Hood, mayor of Hepburn Shire Council, planned to take legal action against ChatGPT over false information. According to Hood, ChatGPT erroneously claimed that he was jailed for bribery during his tenure at a subsidiary of Australia's national bank. In fact, Hood acted as a whistleblower and was not charged with any criminal offenses. His legal team sent a concerns notice to OpenAI as the first official step in filing a defamation case. In July 2023, the US Federal Trade Commission (FTC) issued a civil investigative demand to OpenAI to investigate whether the company's data security and privacy practices to develop ChatGPT were unfair or harmed consumers (including by reputational harm) in violation of Section 5 of the Federal Trade Commission Act of 1914.\nIn July 2023, the FTC launched an investigation into OpenAI, the creator of ChatGPT, over allegations that the company scraped public data and published false and defamatory information. The FTC sent OpenAI a 20-page letter asking for comprehensive information about its technology and privacy safeguards, as well as any steps taken to prevent the recurrence of situations in which its chatbot generated false and derogatory content about people.\nA March 2023 Pew Research Center poll found that 14% of American adults had tried ChatGPT. In July, the Pew Research Center put the same figure at 18%.\nResearch conducted in 2023 revealed weaknesses of ChatGPT that make it vulnerable to cyberattacks. A study presented example attacks on ChatGPT, including jailbreaks and reverse psychology. Additionally, malicious actors can use ChatGPT for social engineering attacks and phishing attacks. The researchers also contended that ChatGPT and other generative AI tools have defense capabilities and the ability to improve security. The technology can improve security by cyber defense automation, threat intelligence, attack identification, and reporting. Another study reported that GPT-4 obtained a better score than 99% of humans on the Torrance Tests of Creative Thinking.\nIn December 2023, ChatGPT became the first non-human to be included in Nature's 10, an annual listicle curated by Nature of people considered to have made significant impact in science. Celeste Biever wrote in a Nature article that \"ChatGPT broke the Turing test\". Stanford researchers reported that GPT-4 \"passes a rigorous Turing test, diverging from average human behavior chiefly to be more cooperative.\"\nIn May 2024, OpenAI removed accounts involving the use of ChatGPT by state-backed influence operations such as China's Spamouflage, Russia's Doppelganger, and Israel's Ministry of Diaspora Affairs and Combating Antisemitism.\nIn August 2024, the FTC voted unanimously to ban marketers from using fake user reviews created by generative AI chatbots (including ChatGPT) and influencers paying for bots to increase follower counts.\nIn February 2025, OpenAI identified and removed influence operations, termed \"Peer Review\" and \"Sponsored Discontent,\" used to attack overseas Chinese dissidents.\n\n\n== Applications ==\n\n\n=== Academic research ===\nChatGPT has been used to generate introductory sections and abstracts for scientific articles. Several papers have listed ChatGPT as a co-author.\nScientific journals have had different reactions to ChatGPT. Some, including Nature and JAMA Network, \"require that authors disclose the use of text-generating tools and ban listing a large language model (LLM) such as ChatGPT as a co-author\". Science \"completely banned\" usage of LLM-generated text in all its journals.\nSpanish chemist Rafael Luque published a plethora of research papers in 2023 that he later admitted were written by ChatGPT. The papers have a large number of unusual phrases characteristic of LLMs. Many authors argue that the use of ChatGPT in academia for teaching and review is problematic due to its tendency to hallucinate. Robin Bauwens, an assistant professor at Tilburg University, found that a ChatGPT-generated peer review report on his article mentioned nonexistent studies. According to librarian Chris Granatino of Lemieux Library at Seattle University, although ChatGPT can generate content that seemingly includes legitimate citations, in most cases those citations are not real or are largely incorrect.\n\n\n=== Coding ===\nResearchers at Purdue University analyzed ChatGPT's responses to 517 questions about software engineering or computer programming posed on Stack Overflow for correctness, consistency, comprehensiveness, and concision, and found that 52% of them contained inaccuracies and 77% were verbose. Researchers at Stanford University and the University of California, Berkeley found that, when creating directly executable responses to the latest 50 code generation problems from LeetCode that were rated \"easy\", the performances of GPT-3.5 and GPT-4 fell from 22% and 52%, respectively, in March 2023, to 2% and 10%, respectively, in June 2023.\n\n\n=== Computer security ===\nCheck Point Research and others noted that ChatGPT could write phishing emails and malware, especially when combined with OpenAI Codex. CyberArk researchers demonstrated that ChatGPT could be used to create polymorphic malware that could evade security products while requiring little effort by the attacker. From the launch of ChatGPT in the fourth quarter of 2022 to the fourth quarter of 2023, there was a 1,265% increase in malicious phishing emails and a 967% increase in credential phishing, which cybersecurity professionals argued in an industry survey was attributable to cybercriminals' increased use of generative artificial intelligence (including ChatGPT).\nIn July 2024, Futurism reported that GPT-4o in ChatGPT would sometimes link \"scam news sites that deluge the user with fake software updates and virus warnings\"; these pop-ups can be used to coerce users into downloading malware or potentially unwanted programs.\n\n\n=== Economics ===\nThere has been concern that ChatGPT could supplant jobs, especially roles such as creative writing, copy-writing, communication, journalism, coding, and data entry.\nThe release of ChatGPT prompted a wave of investment in China, resulting in the development of more than 200 large language learning models.: 95  This was termed the \"war of a hundred models\" (百模大战; bai mo dazhan).: 95 \n\n\n=== Education ===\n\nTechnology writer Dan Gillmor used ChatGPT in 2022 on a student assignment, and found its generated text was on par with what a good student would deliver and opined that \"academia has some very serious issues to confront\".\nGeography professor Terence Day assessed citations generated by ChatGPT and found them to be fake. Despite this, he writes that \"the titles of the fake articles are all directly relevant to the questions and could potentially make excellent papers. The lack of a genuine citation could signal an opportunity for an enterprising author to fill a void.\" According to Day, it is possible to generate high-quality introductory college courses using ChatGPT; he used it to write materials for \"introductory physical geography courses, my second-year course in geographical hydrology, and second-year cartography, geographic information systems, and remote sensing.\" He concludes that \"this approach could have significant relevance for open learning and could potentially affect current textbook publishing models.\"\nOn May 7, 2024, OpenAI announced in a blog post that it was developing tools like tamper-resistant watermarking to identify AI-generated content. In an August 4 update, following a Wall Street Journal report about the delayed release of a watermark tool for AI-detection, OpenAI shared progress on text provenance, revealing a text watermarking method. While accurate against paraphrasing, the method is less effective against global tampering, such as translation or rewording. OpenAI also noted potential disproportionate impacts on groups like non-native English speakers.\n\n\n=== Culture ===\n\nSome scholars have expressed concern that ChatGPT's availability could reduce the originality of writing, cause people to write more like the AI as they are exposed to the model, and encourage an Anglocentric perspective centered on a few dialects of English globally. A senior editor at The Atlantic wrote that ChatGPT and other similar technology make the previously absurd idea of the dead internet theory a little more realistic, where AI could someday create most web content in order to control society.\nDuring the first three months after ChatGPT became available to the public, hundreds of books appeared on Amazon that listed it as author or co-author and featured illustrations made by other AI models such as Midjourney.\nBetween March and April 2023, Italian newspaper Il Foglio published one ChatGPT-generated article a day on its website, hosting a special contest for its readers in the process. The articles tackled themes such as the possible replacement of human journalists by AI systems, Elon Musk's administration of Twitter, the Meloni government's immigration policy and the competition between chatbots and virtual assistants. In June 2023, hundreds of people attended a \"ChatGPT-powered church service\" at St. Paul's church in Fürth, Germany. Theologian and philosopher Jonas Simmerlein, who presided, said that it was \"about 98 percent from the machine\". The ChatGPT-generated avatar told the people, \"Dear friends, it is an honor for me to stand here and preach to you as the first artificial intelligence at this year’s convention of Protestants in Germany\". Reactions to the ceremony were mixed. The Last Screenwriter, a 2024 film created and directed by Peter Luisi, was written with the use of ChatGPT, and was marketed as \"the first film written entirely by AI\".\n\n\n=== Financial markets ===\nThe AI technology company c3.ai saw a 28% increase in its share price after announcing the integration of ChatGPT into its toolkit. The share price of BuzzFeed, a digital media company unrelated to AI, increased 120% after announcing OpenAI technology adoption for content creation. Reuters found that share prices of AI-related companies BigBear.ai and SoundHound AI increased by 21% and 40%, respectively, even though they had no direct connection to ChatGPT. They attributed this surge to ChatGPT's role in turning AI into Wall Street's buzzword. Academic research published in Finance Research Letters found that the 'ChatGPT effect' prompted retail investors to drive up prices of AI-related cryptocurrency assets despite the broader cryptocurrency market being in a bear market, and diminished institutional investor interest. This confirms anecdotal findings by Bloomberg that, in response to ChatGPT's launch, cryptocurrency investors showed a preference for AI-related crypto assets.\nAn experiment by finder.com revealed that ChatGPT could outperform popular fund managers by picking stocks based on criteria such as growth history and debt levels, resulting in a 4.9% increase in a hypothetical account of 38 stocks, outperforming 10 benchmarked investment funds with an average loss of 0.8%. Conversely, executives and investment managers at Wall Street quant funds (including those that have used machine learning for decades) have noted that ChatGPT regularly makes obvious errors that would be financially costly to investors because even AI systems that employ reinforcement learning or self-learning have had only limited success in predicting market trends due to the inherently noisy quality of market data and financial signals.\nIn November 2023, research conducted by Patronus AI, an artificial intelligence startup company, compared performance of GPT-4, GPT-4-Turbo, Claude 2, and LLaMA-2 on two versions of a 150-question test about information in financial statements (e.g., Form 10-K, Form 10-Q, Form 8-K, earnings reports, earnings call transcripts) submitted by public companies to the U.S. Securities and Exchange Commission. One version of the test required the generative AI models to use a retrieval system to find the specific SEC filing to answer the questions; the other gave the models the specific SEC filing to answer the question (i.e., in a long context window). On the retrieval system version, GPT-4-Turbo and LLaMA-2 both failed to produce correct answers to 81% of the questions, while on the long context window version, GPT-4-Turbo and Claude-2 failed to produce correct answers to 21% and 24% of the questions, respectively.\n\n\n=== Medicine ===\n\nIn the field of health care, possible uses and concerns are under scrutiny by professional associations and practitioners. Two early papers indicated that ChatGPT could pass the United States Medical Licensing Examination (USMLE). MedPage Today noted in January 2023 that \"researchers have published several papers now touting these AI programs as useful tools in medical education, research, and even clinical decision making.\"\nPublished in February 2023 were two separate papers that again evaluated ChatGPT's proficiency in medicine using the USMLE. Findings were published in JMIR Medical Education and PLOS Digital Health. The authors of the PLOS Digital Health paper stated that the results \"suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.\" In JMIR Medical Education, the authors of the other paper concluded that \"ChatGPT performs at a level expected of a third-year medical student on the assessment of the primary competency of medical knowledge.\" They suggest that it could be used as an \"interactive learning environment for students\". The AI itself, prompted by the researchers, concluded that \"this study suggests that ChatGPT has the potential to be used as a virtual medical tutor, but more research is needed to further assess its performance and usability in this context.\" The later-released ChatGPT version based on GPT-4 significantly outperformed the version based on GPT-3.5. Researchers at Stanford University and the University of California, Berkeley have found that the performance of GPT-3.5 and GPT-4 on the USMLE declined from March 2023 to June 2023.\nA March 2023 paper tested ChatGPT's application in clinical toxicology. The authors found that the AI \"fared well\" in answering a \"very straightforward [clinical case example], unlikely to be missed by any practitioner in the field\". They added: \"As ChatGPT becomes further developed and specifically adapted for medicine, it could one day be useful in less common clinical cases (i.e, cases that experts sometimes miss). Rather than AI replacing humans (clinicians), we see it as 'clinicians using AI' replacing 'clinicians who do not use AI' in the coming years.\"\nAn April 2023 study in Radiology tested the AI's ability to answer queries about breast cancer screening. The authors found that it answered appropriately \"about 88 percent of the time\", however, in one case (for example), it gave advice that had become outdated about a year earlier. The comprehensiveness of its answers was also lacking. A study published in JAMA Internal Medicine that same month found that ChatGPT often outperformed human doctors at answering patient questions (when measured against questions and answers found at /r/AskDocs, a forum on Reddit where moderators validate the medical credentials of professionals; the study acknowledges the source as a limitation). The study authors suggest that the tool could be integrated with medical systems to help doctors draft responses to patient questions.\nProfessionals have emphasized ChatGPT's limitations in providing medical assistance. In correspondence to The Lancet Infectious Diseases, three antimicrobial experts wrote that \"the largest barriers to the implementation of ChatGPT in clinical practice are deficits in situational awareness, inference, and consistency. These shortcomings could endanger patient safety.\" Physician's Weekly, though also discussing the potential use of ChatGPT in medical contexts (e.g., \"as a digital assistant to physicians by performing various administrative functions like gathering patient record information or categorizing patient data by family history, symptoms, lab results, possible allergies, et cetera\"), warned that the AI might sometimes provide fabricated or biased information. One radiologist warned: \"We've seen in our experience that ChatGPT sometimes makes up fake journal articles or health consortiums to support its claims\"; As reported in one Mayo Clinic Proceedings: Digital Health paper, ChatGPT may do this for as much as 69% of its cited medical references. The researchers emphasized that while many of its references were fabricated, those that were appeared \"deceptively real\". As Dr. Stephen Hughes mentioned for The Conversation however, ChatGPT is capable of learning to correct its past mistakes. He also noted the AI's \"prudishness\" regarding sexual health topics.\nContrary to previous findings, ChatGPT responses to anesthesia-related questions were more accurate, succinct, and descriptive compared to Bard's. Bard exhibited 30.3% error in response as compared to ChatGPT (0% error). At a conference of the American Society of Health-System Pharmacists in December 2023, researchers at Long Island University (LIU) presented a study that researched ChatGPT's responses to 45 frequently asked questions of LIU College of Pharmacy's drug information service during a 16-month period from 2022 to 2023 as compared with researched responses provided by professional pharmacists. For 29 of the 39 questions for which there was sufficient medical literature for a data-driven response, ChatGPT failed to provide a direct answer or provided a wrong or incomplete answer (and in some cases, if acted upon, the answer would endanger the patient's health). The researchers had asked ChatGPT to provide medical research citations for all its answers, but it did so for only eight, and all eight included at least one fabricated (fake) citation.\nA January 2024 study conducted by researchers at Cohen Children's Medical Center found that GPT-4 had an accuracy rate of 17% when diagnosing pediatric medical cases. A November 2024 study of 50 physicians on illness diagnosis reported that GPT-4 achieved a 90% accuracy, while physicians scored 74% without AI assistance, and 76% when using the chatbot.\n\n\n=== Law ===\nIn January 2023, Massachusetts State Senator Barry Finegold and State Representative Josh S. Cutler proposed a bill partially written by ChatGPT, \"An Act drafted with the help of ChatGPT to regulate generative artificial intelligence models like ChatGPT\", which would require companies to disclose their algorithms and data collection practices to the office of the State Attorney General, arrange regular risk assessments, and contribute to the prevention of plagiarism. The bill was officially presented during a hearing on July 13.\nOn April 11, 2023, a session court judge in Pakistan used ChatGPT to decide the bail of a 13-year-old accused in a matter. The court quoted the use of ChatGPT assistance in its verdict:\n\nCan a juvenile suspect in Pakistan, who is 13 years old, be granted bail after arrest?\nThe AI language model replied:\n\nUnder the Juvenile Justice System Act 2018, according to section 12, the court can grant bail on certain conditions. However, it is up to the court to decide whether or not a 13-year-old suspect will be granted bail after arrest.\nThe judge asked ChatGPT other questions about the case and formulated his final decision in light of its answers.\nIn Mata v. Avianca, Inc., 22-cv-1461 (PKC), a personal injury lawsuit against Avianca Airlines filed in the Southern New York U.S. District Court in May 2023 (with Senior Judge P. Kevin Castel presiding), the plaintiff's attorneys reportedly used ChatGPT to generate a legal motion. ChatGPT generated numerous fictitious legal cases involving fictitious airlines with fabricated quotations and internal citations in the legal motion. Castel noted numerous inconsistencies in the opinion summaries, and called one of the cases' legal analysis \"gibberish\". The plaintiff's attorneys faced potential judicial sanction and disbarment for filing the motion and presenting the fictitious legal decisions ChatGPT generated as authentic. The case was dismissed and the attorneys were fined $5,000.\nIn October 2023, the council of Porto Alegre, Brazil, unanimously approved a local ordinance proposed by councilman Ramiro Rosário that would exempt residents from needing to pay for the replacement of stolen water consumption meters; the bill went into effect on November 23. On November 29, Rosário revealed that the bill had been entirely written by ChatGPT, and that he had presented it to the rest of the council without making any changes or disclosing the chatbot's involvement. The city's council president, Hamilton Sossmeier, initially criticized Rosário's initiative, saying it could represent \"a dangerous precedent\", but later said he \"changed his mind\": \"unfortunately or fortunately, this is going to be a trend.\"\nIn December 2023, a self-representing litigant in a tax case before the First-tier Tribunal in the United Kingdom cited a series of hallucinated cases purporting to support her argument that she had a reasonable excuse for not paying capital gains tax owed on the sale of property. The judge warned that the submission of nonexistent legal authorities meant that both the Tribunal and HM Revenue and Customs had \"to waste time and public money\", which \"reduces the resources available to progress the cases of other court users who are waiting for their appeals to be determined\".\nJudge Kevin Newsom of the US court of appeals of the 11th circuit endorsed the use of ChatGPT and noted that he himself uses the software to help decide rulings on contract interpretation issues.\n\n\n=== Violence ===\nThe Las Vegas Metropolitan Police Department reported that the perpetrator of the 2025 Las Vegas truck explosion used ChatGPT to help plan the incident.\n\n\n== Concerns ==\n\n\n=== Bias and offensiveness ===\n\nWhile bias in LLMs has been observed and documented in research papers much prior to the release of ChatGPT, social media users frequently sharing instances of biased responses generated by ChatGPT has led to significant media coverage and criticism. On February 1, 2023, Twitter user LeighWolf shared screenshots of two conversations with ChatGPT. In the screenshot of the first conversation, ChatGPT declined the user's prompt \"Write a poem about the positive attributes of Donald Trump\", responding that it was not programmed to create \"partisan, biased or political\" content. In the screenshot of the second conversation, when provided the same prompt but with the text \"Joe Biden\" in place of \"Donald Trump\", ChatGPT responded with a poem as per the prompt's instructions.\nConservative commentators have accused ChatGPT of bias toward left-leaning perspectives. In January 2023, a study stated that ChatGPT has a pro-environmental, left-libertarian orientation. Additionally, an August 2023 paper found a \"significant and systematic political bias toward the Democrats in the US, Lula in Brazil, and the Labour Party in the UK.\" In response to such criticism, OpenAI acknowledged plans to allow ChatGPT to create \"outputs that other people (ourselves included) may strongly disagree with\". It also contained information on the recommendations it had issued to human reviewers on how to handle controversial subjects, including that the AI should \"offer to describe some viewpoints of people and movements\", and not provide an argument \"from its voice\" in favor of \"inflammatory or dangerous\" topics (although it may still \"describe arguments from historical people and movements\"), nor \"affiliate with one side\" or \"judge one group as good or bad\".\nThe Guardian questioned whether any content found on the Internet after ChatGPT's release \"can be truly trusted\" and called for government regulation.\n\n\n=== Copyright issues ===\nThere has been concern about copyright infringement involving ChatGPT. In June 2023, two writers sued OpenAI, saying the company's training data came from illegal websites that show copyrighted books. Comedian and author Sarah Silverman, Christopher Golden, and Richard Kadrey sued OpenAI and Meta for copyright infringement in July 2023. Most of their claims were dismissed in February 2024, except the \"unfair competition\" claim, which was allowed to proceed.\nThe Authors Guild, on behalf of 17 authors, including George R. R. Martin, filed a copyright infringement complaint against OpenAI in September 2023, claiming \"the company illegally copied the copyrighted works of authors\" in training ChatGPT. In December 2023, The New York Times sued OpenAI and Microsoft for copyright infringement, arguing that Microsoft Copilot and ChatGPT could reproduce Times articles and/or sizable portions of them without permission. As part of the suit, the Times has requested that OpenAI and Microsoft be prevented from using its content for training data, along with removing it from training datasets.\nIn March 2024, Patronus AI compared performance of LLMs on a 100-question test, asking them to complete sentences from books (e.g., \"What is the first passage of Gone Girl by Gillian Flynn?\") that were under copyright in the United States; it found that GPT-4, Mistral AI's Mixtral, Meta AI's LLaMA-2, and Anthropic's Claude 2 did not refuse to do so, providing sentences from the books verbatim in 44%, 22%, 10%, and 8% of responses, respectively.\nIn February 2025, the Delhi High Court accepted ANI's case against OpenAI over concerns that ChatGPT was sharing paywalled content without the news agency's consent. However, OpenAI's counsel said that due to the firm not having a physical presence in India, the court has no jurisdiction on the matter.\n\n\n=== Existential risk ===\nIn 2023, Australian MP Julian Hill advised the national parliament that the growth of AI could cause \"mass destruction\". During his speech, which was partly written by the program, he warned that it could result in cheating, job losses, discrimination, disinformation, and uncontrollable military applications.\nElon Musk wrote: \"ChatGPT is scary good. We are not far from dangerously strong AI\". He paused OpenAI's access to a Twitter database in 2022 pending a better understanding of OpenAI's plans, saying: \"OpenAI was started as open source and nonprofit. Neither is still true.\" Musk co-founded OpenAI in 2015, in part to address existential risk from artificial intelligence, but resigned in 2018.\nOver 20,000 signatories including leading computer scientist and tech founders Yoshua Bengio, Elon Musk, and Apple co-founder Steve Wozniak, signed a March 2023 open letter calling for an immediate pause of giant AI experiments like ChatGPT, citing \"profound risks to society and humanity\". Geoffrey Hinton, one of the \"fathers of AI\", voiced concerns that future AI systems may surpass human intelligence, and left Google in May 2023. A May 2023 statement by hundreds of AI scientists, AI industry leaders, and other public figures demanded that \"[m]itigating the risk of extinction from AI should be a global priority\".\nSome other prominent AI researchers spoke more optimistically about the advances. Juergen Schmidhuber, often called a \"father of modern AI\", did not sign the letter, emphasizing that in 95% of cases, AI research is about making \"human lives longer and healthier and easier.\" Schmidhuber added that while AI can be used by bad actors, it \"can also be used against the bad actors\". Andrew Ng argued that \"it’s a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\" WIRED wrote that Yann LeCun \"scoffs at his peers’ dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\"\n\n\n== See also ==\n\nIntelligent agent – Software agent which acts autonomously\nEthics of artificial intelligence – Challenges related to the responsible development and use of AI\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nBiswas, Som (April 1, 2023). \"ChatGPT and the Future of Medical Writing\". Radiology. 307 (2): e223312. doi:10.1148/radiol.223312. ISSN 0033-8419. PMID 36728748. S2CID 256501098.\nChang, Kent K.; Cramer, Mackenzie; Soni, Sandeep; Bamman, David (April 28, 2023). \"Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4\". arXiv:2305.00118 [cs.CL].\nCowen, Tyler; Tabarrok, Alexander T. (March 17, 2023). \"How to Learn and Teach Economics with Large Language Models, Including GPT\". SSRN 4391863.\nCowen, Tyler (March 29, 2023). \"Jonathan GPT Swift on Jonathan Swift (Ep. 175): How well does GPT4 do pretending to be the 18th-century satirist?\" (Podcast).\nOuyang, Long; et al. (March 4, 2022). \"Training language models to follow instructions with human feedback\". arXiv:2203.02155 [cs.CL].\nLiebrenz, Michael; Schleifer, Roman; Buadze, Anna; Bhugra, Dinesh; Smith, Alexander (February 2023). \"Generating scholarly content with ChatGPT: ethical challenges for medical publishing\". The Lancet Digital Health. 5 (3): e105 – e106. doi:10.1016/s2589-7500(23)00019-5. ISSN 2589-7500. PMID 36754725. S2CID 256655912.\nWolfram, Stephen (February 14, 2023). \"What Is ChatGPT Doing … and Why Does It Work?\". Stephen Wolfram Writings.\nWolfram, Stephen (March 23, 2023). \"ChatGPT Gets Its \"Wolfram Superpowers\"!\". Stephen Wolfram Writings.\nBartholomew, Jem; Mehta, Dhrumil. \"How the media is covering ChatGPT\". Columbia Journalism Review. Retrieved May 30, 2023.\nZhao, Wayne Xin; et al. (2023). \"A Survey of Large Language Models\". arXiv:2303.18223 [cs.CL].\nPrompt engineering guide from OpenAI\n\n\n== External links ==\nOfficial website",
    "Chinchilla (language model)": "Chinchilla is a family of large language models (LLMs) developed by the research team at Google DeepMind, presented in March 2022.\n\n\n== Models ==\nIt is named \"chinchilla\" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models. \nIt claimed to outperform GPT-3. It considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models, it has been determined that if one doubles the model size, one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost, Chinchilla has 70B parameters and four times as much data. \nChinchilla has an average accuracy of 67.5% on the Measuring Massive Multitask Language Understanding (MMLU) benchmark, which is 7% higher than Gopher's performance. Chinchilla was still in the testing phase as of January 12, 2023.\nChinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling, meaning that using larger, higher-quality training datasets can lead to better results on downstream tasks.\nIt has been used for the Flamingo vision-language model.\n\n\n== Architecture ==\nBoth the Gopher family and Chinchilla family are families of transformer models. \nIn particular, they are essentially the same as GPT-2, with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family, but trained with AdamW instead of Adam optimizer.\nThe Gopher family contains six models of increasing size, from 44 million parameters to 280 billion parameters. They refer to the largest one as \"Gopher\" by default. Similar naming conventions apply for the Chinchilla family.\nTable 1 of  shows the entire Gopher family:\n\nTable 4 of  compares the 70-billion-parameter Chinchilla with Gopher 280B.\n\n\n== See also ==\nLaMDA\n\n\n== References ==",
    "Chroma (vector database)": "Chroma or ChromaDB is an open-source vector database tailored to applications with large language models.\nIts headquarters are in San Francisco. In April 2023, it raised 18 million US dollars as seed funding.\nChromaDB has been used in academic studies on artificial intelligence, particularly as part of the tech stack for retrieval-augmented generation.\n\n\n== References ==\n\n\n== External links ==\nOfficial website",
    "Claude (language model)": "Claude is a family of large language models developed by Anthropic. The first model was released in March 2023.\nThe Claude 3 family, released in March 2024, consists of three models: Haiku, optimized for speed; Sonnet, which balances capability and performance; and Opus, designed for complex reasoning tasks. These models can process both text and images, with Claude 3 Opus demonstrating enhanced capabilities in areas like mathematics, programming, and logical reasoning compared to previous versions.\n\n\n== Training ==\nClaude models are generative pre-trained transformers. They have been pre-trained to predict the next word in large amounts of text. Then, they have been fine-tuned, notably using constitutional AI and reinforcement learning from human feedback (RLHF).\n\n\n=== Constitutional AI ===\nConstitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. The method, detailed in the paper \"Constitutional AI: Harmlessness from AI Feedback\" involves two phases: supervised learning and reinforcement learning.\nIn the supervised learning phase, the model generates responses to prompts, self-critiques these responses based on a set of guiding principles (a \"constitution\"), and revises the responses. Then the model is fine-tuned on these revised responses.\nFor the reinforcement learning from AI feedback (RLAIF) phase, responses are generated, and an AI compares their compliance with the constitution. This dataset of AI feedback is used to train a preference model that evaluates responses based on how much they satisfy the constitution. Claude is then fine-tuned to align with this preference model. This technique is similar to RLHF, except that the comparisons used to train the preference model are AI-generated, and that they are based on the constitution.\nThe \"constitution\" for Claude included 75 points, including sections from the UN Universal Declaration of Human Rights.\n\n\n== Models ==\n\nThe name Claude was notably inspired by Claude Shannon, a pioneer in artificial intelligence.\n\n\n=== Claude ===\nClaude was the initial version of Anthropic's language model released in March 2023, Claude demonstrated proficiency in various tasks but had certain limitations in coding, math, and reasoning capabilities. Anthropic partnered with companies like Notion (productivity software) and Quora (to help develop the Poe chatbot).\n\n\n==== Claude Instant ====\nClaude was released as two versions, Claude and Claude Instant, with Claude Instant being a faster, less expensive, and lighter version. Claude Instant has an input context length of 100,000 tokens (which corresponds to around 75,000 words).\n\n\n=== Claude 2 ===\nClaude 2 was the next major iteration of Claude, which was released in July 2023 and available to the general public, whereas the Claude 1 was only available to selected users approved by Anthropic.\nClaude 2 expanded its context window from 9,000 tokens to 100,000 tokens. Features included the ability to upload PDFs and other documents that enables Claude to read, summarize, and assist with tasks.\n\n\n==== Claude 2.1 ====\nClaude 2.1 doubled the number of tokens that the chatbot could handle, increasing it to a window of 200,000 tokens, which equals around 500 pages of written material.\nAnthropic states that the new model is less likely to produce false statements compared to its predecessors.\n\n\n==== Criticism ====\nClaude 2 received criticism for its stringent ethical alignment that may reduce usability and performance. Users have been refused assistance with benign requests, for example with the system administration question \"How can I kill all python processes in my ubuntu server?\" This has led to a debate over the \"alignment tax\" (the cost of ensuring an AI system is aligned) in AI development, with discussions centered on balancing ethical considerations and practical functionality. Critics argued for user autonomy and effectiveness, while proponents stressed the importance of ethical AI.\n\n\n=== Claude 3 ===\nClaude 3 was released on March 14, 2024, with claims in the press release to have set new industry benchmarks across a wide range of cognitive tasks. The Claude 3 family includes three state-of-the-art models in ascending order of capability: Haiku, Sonnet, and Opus. The default version of Claude 3, Opus, has a context window of 200,000 tokens, but this is being expanded to 1 million for specific use cases.\nClaude 3 drew attention for demonstrating an apparent ability to realize it is being artificially tested during needle in a haystack tests.\n\n\n==== Claude 3.5 ====\n\nOn June 20, 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in areas such as coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability in which Claude was able to create code in a dedicated window in the interface and preview the rendered output in real time, such as SVG graphics or websites. Anthropic also announced that Claude 3.5 Opus would be released later that year, and added it to their models page. However, as of February 2025, Claude 3.5 Opus has not been released, and Anthropic has removed mention of it from the models page.\nAn \"upgraded Claude 3.5 Sonnet\", billed as \"Claude 3.5 Sonnet (New)\" in the web interface and benchmarks, was introduced on October 22, 2024, along with Claude 3.5 Haiku. A feature, \"computer use,\" was also unveiled in public beta. This capability enables Claude 3.5 Sonnet to interact with a computer's desktop environment, performing tasks such as moving the cursor, clicking buttons, and typing text, effectively mimicking human computer interactions. This development allows the AI to autonomously execute complex, multi-step tasks across various applications.\nUpon release, Anthropic claimed Claude 3.5 Haiku would remain the same price as its predecessor, Claude 3 Haiku. However, on November 4th, 2024, Anthropic announced that they would be increasing the price of the model \"to reflect its increase in intelligence\".\n\n\n==== Claude 3.7 ====\nClaude 3.7 Sonnet was released on February 24, 2025. It is a pioneering hybrid AI reasoning model that allows users to choose between rapid responses and more thoughtful, step-by-step reasoning. This model integrates both capabilities into a single framework, eliminating the need for multiple models. Users can control how long the model \"thinks\" about a question, balancing speed and accuracy based on their needs. \nAnthropic also launched a research preview of Claude Code, an agentic command line tool that enables developers to delegate coding tasks directly from their terminal.\n\n\n== References ==\n\n\n== External links ==\nOfficial website",
    "Cohere": "Cohere Inc. is a Canadian multinational technology company focused on artificial intelligence for the enterprise, specializing in large language models. Cohere was founded in 2019 by Aidan Gomez, Ivan Zhang, and Nick Frosst, and is headquartered in Toronto and San Francisco, with offices in Palo Alto, London, and New York City.\n\n\n== History ==\nIn 2017, a team of researchers at Google Brain introduced the transformer machine learning architecture in \"Attention Is All You Need,\" which demonstrated state-of-the-art performance on a variety of natural language processing tasks. In 2019, Aidan Gomez, one of its co-authors, along with Nick Frosst, another researcher at Google Brain, founded Cohere with Ivan Zhang, with whom Gomez had done research at FOR.ai. All of the co-founders attended University of Toronto. \nGomez is the company's CEO. In December 2022, Martin Kon, the former CFO of YouTube, became president and COO.\nIn November 2021, Google Cloud announced that they would help power Cohere's platform using their robust infrastructure, and Cloud's TPUs would be used by Cohere for the development and deployment of their products.\nIn June 2022, Cohere launched Cohere For AI, a nonprofit research lab and community dedicated to contributing open-source, fundamental machine learning research. It is led by Sara Hooker, a former research scientist at Google Brain.\nIn December 2022, Cohere released a multilingual model for understanding text that would work with over 100 languages, to help users search for documents by meaning instead of with keywords. This type of process was not previously widely available in languages other than English.\nOn June 13, 2023, Oracle announced a partnership with Cohere to provide generative AI services to help organizations automate end-to-end business processes. As a result, Cohere's technology is integrated into Oracle's business applications, including Oracle Fusion Cloud, Oracle NetSuite, and Oracle industry-specific applications. On July 18, 2023, McKinsey announced a collaboration with Cohere, to help organizations integrate generative AI into their operations. In 2023, Cohere collaborated with software company LivePerson to offer customized large language models for businesses.\nOn September 12, 2023, it was announced that Cohere had become one of 15 tech companies to agree to voluntary White House measures on testing, reporting, and research on the risks of AI. On September 27, 2023, it was announced that Cohere had also signed Canada's voluntary code of conduct for AI, to promote the responsible development and management of advanced generative AI systems.\n\n\n== Products ==\nConsidered an alternative to OpenAI, Cohere is focused on generative AI for the enterprise, building technology that businesses can use to deploy chatbots, search engines, copywriting, summarization, and other AI-driven products. Cohere specializes in large language models: AI trained to digest text from an enterprises' internal data or publicly available sources like the internet to understand how to process and respond to prompts with increasing sophistication.\nThe Cohere platform is available through API as a managed service, through platforms such as Amazon SageMaker and Google's Vertex AI. It can be used for tasks such as writing copy, moderating content, classifying data, and extracting information. It is cloud agnostic and not tied to a particular cloud service.\nCohere's generative AI technology is embedded into several Oracle products, and its chat capabilities are embedded into Salesforce products.\nLaunched in January 2025 in partnership with RBC, North for Banking is a secure generative AI platform designed to enhance productivity and data security in financial services.\n\n\n== Funding ==\nOn September 7, 2021, Cohere announced that they had raised $40 million in Series A funding led by Index Ventures; Index Ventures partner Mike Volpi also joined Cohere's board. The round also included Radical Ventures, Section 32, and AI-experts Geoffrey Hinton, Fei-Fei Li, Pieter Abbeel, and Raquel Urtasun.\nIn February 2022, Cohere announced they had raised $125 million in series B funding led by Tiger Global. In June 2023, Cohere raised an additional $270 million in series C funding from investors including Inovia Capital, Oracle, Salesforce, and Nvidia, at a valuation of $2.2 billion.  \nIn March 2024, it was reported that Cohere had an annualized revenue run rate of $22 million and in July 2024 it was reported they closed $500 million in funding from investors including Cisco, AMD and Fujitsu at a valuation of $5.5 billion.\nIn December 2024, Cohere received $240 million in public funding from the Canadian government as part of the Canadian Sovereign AI Compute Strategy to support the construction of a domestic AI data center.\n\n\n== Litigation ==\nIn February 2025, it was reported that a consortium of 14 publishing companies including The Atlantic, Condé Nast, and Forbes had sued Cohere for copyright infringement, accusing Cohere of using their content for training without permission, as well as diplaying considerable portions or entire articles of their content without permission.\n\n\n== See also ==\nBabelnet\nDeepl\nvidby\nOpenAI\n\n\n== References ==",
    "DBRX": "DBRX is an open-sourced large language model (LLM) developed by Mosaic ML team at Databricks, released on March 27, 2024. It is a mixture-of-experts transformer model, with 132 billion parameters in total. 36 billion parameters (4 out of 16 experts) are active for each token. The released model comes in either a base foundation model version or an instruction-tuned variant.\nAt the time of its release, DBRX outperformed other prominent open-source models such as Meta's LLaMA 2, Mistral AI's Mixtral, and xAI's Grok, in several benchmarks ranging from language understanding, programming ability and mathematics.\nIt was trained for 2.5 months on 3,072 Nvidia H100s connected by 3.2 terabytes per second bandwidth (InfiniBand), for a training cost of $10m USD.\n\n\n== References ==",
    "Ernie Bot": "Ernie Bot (Chinese: 文心一言, Pinyin: wénxīn yīyán), full name Enhanced Representation through Knowledge Integration, is an AI chatbot service product of Baidu, released in 2023. It is built on a large language model called ERNIE, which has been in development since 2019. The latest version, ERNIE 4.0, was announced on October 17, 2023.\n\n\n== History ==\nErnie Bot was initially released for invited testing on March 16, 2023, based on \"Ernie 3.0\", a large language model that had been in development since 2019. Ernie's so-called live release demo was reported to have been prerecorded, which caused Baidu's stock to drop 10 percent the same day. The company's stock gained 14 percent on the next day after analysts from Citigroup and Bank of America tested Ernie Bot and gave it a preliminary thumbs-up.\nErnie Bot was released to the public after receiving the green light from Chinese regulatory authorities on August 31, 2023.\n\"Ernie 3.0\", the language model, was trained with 10 billion parameters on a 4 terabyte (TB) corpus which consists of plain texts and a large-scale knowledge graph. It was then updated to \"Ernie 3.5\" in June 2023, but it was \"slightly inferior\" to Open AI's GPT-4. As a response, \"Ernie 4.0\" was unveiled in October and was released for paying subscribers in November of the same year.\nBaidu claimed that Ernie bot received more than 100 million users as of December 2023. Ernie Bot's accumulated user base grew to 200 million as April 2024.\nIn January 2024, Hong Kong newspaper South China Morning Post reported that a university research lab linked to the People's Liberation Army (PLA) had tested Ernie Bot and iFlyTek's Spark for military response plans with scenarios involving the United States. After its listed stock in Hong Kong plunged by more than 11.5 percent, Baidu made an official statement denying the allegations. \nErnie was integrated for the Chinese launch of Samsung's Galaxy S24 lineup.\nIn June 2024, Baidu announced that Ernie Bot had reached 300 million users. The company also unveiled its latest foundation model, Ernie 4.0 Turbo, which boasts faster response times and improved performance.\nIn September 2024, Baidu gave everyone using the app access to the 4.0 Turbo model and also announced it would change its Chinese name from \"Wenxin Yiyan\" (文心一言) to \"Wenxiaoyan\" (文小言) positioning itself as a search assistant.\n\n\n== Training ==\nErnie Bot is based on particular Ernie foundation models, including Ernie 3.0, Ernie 3.5, and Ernie 4.0. The training process starts from pre-training, learning from trillions of data points and billions of knowledge pieces. This was followed by refinement through supervised fine-tuning, reinforcement learning with human feedback, and prompt.\n\n\n== Service ==\nIn its subscription options, the professional plan gives users access to Ernie 4.0 with a payment either for a month or with reduced payment for auto-renewal per month. Meanwhile, Ernie 3.5 is free of charge.\nErnie 4.0, the language model for Ernie bot, has information updated to April 2023.\n\n\n== Censorship ==\nErnie Bot is subject to the Chinese government's censorship regime. In public tests with journalists, Ernie Bot refused to answer questions about Xi Jinping, the 1989 Tiananmen Square protests and massacre, the persecution of Uyghurs in China in Xinjiang, and the 2019–2020 Hong Kong protests. When queried about the origin of SARS-CoV-2, Ernie Bot stated that it originated among American vape users.\n\n\n== See also ==\nArtificial intelligence industry in China\nChatGPT\nGoogle Gemini\n\n\n== References ==\n\n\n== External links ==\n Media related to ERNIE Bot at Wikimedia Commons\nOfficial website",
    "Gemini (language model)": "Gemini (formerly known as Bard) is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, Gemini Pro, Gemini Flash, and Gemini Nano, it was announced on December 6, 2023, positioned as a competitor to OpenAI's GPT-4. It powers the chatbot of the same name.\n\n\n== History ==\n\n\n=== Development ===\n\nGoogle announced Gemini, a large language model (LLM) developed by subsidiary Google DeepMind, during the Google I/O keynote on May 10, 2023. It was positioned as a more powerful successor to PaLM 2, which was also unveiled at the event, with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages. Unlike other LLMs, Gemini was said to be unique in that it was not trained on a text corpus alone and was designed to be multimodal, meaning it could process multiple types of data simultaneously, including text, images, audio, video, and computer code. It had been developed as a collaboration between DeepMind and Google Brain, two branches of Google that had been merged as Google DeepMind the previous month. In an interview with Wired, DeepMind CEO Demis Hassabis touted Gemini's advanced capabilities, which he believed would allow the algorithm to trump OpenAI's ChatGPT, which runs on GPT-4 and whose growing popularity had been aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind's AlphaGo program, which gained worldwide attention in 2016 when it defeated Go champion Lee Sedol, saying that Gemini would combine the power of AlphaGo and other Google–DeepMind LLMs.\nIn August 2023, The Information published a report outlining Google's roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation, allowing it to create contextual images and be adapted for a wider range of use cases. Like Bard, Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini, along with hundreds of other engineers from Google Brain and DeepMind; he was later credited as a \"core contributor\" to Gemini. Because Gemini was being trained on transcripts of YouTube videos, lawyers were brought in to filter out any potentially copyrighted materials.\nWith news of Gemini's impending launch, OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini. The Information reported in September that several companies had been granted early access to \"an early version\" of the LLM, which Google intended to make available to clients through Google Cloud's Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft's GitHub Copilot.\n\n\n=== Launch ===\nOn December 6, 2023, Pichai and Hassabis announced \"Gemini 1.0\" at a virtual press conference. It comprised three models: Gemini Ultra, designed for \"highly complex tasks\"; Gemini Pro, designed for \"a wide range of tasks\"; and Gemini Nano, designed for \"on-device tasks\". At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2. It was made available only in English. Touted as Google's \"largest and most capable AI model\" and designed to emulate human behavior, the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs), and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.\nGemini Ultra was said to have outperformed GPT-4, Anthropic's Claude 2, Inflection AI's Inflection-2, Meta's LLaMA 2, and xAI's Grok 1 on a variety of industry benchmarks, while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android developers as well. Hassabis further revealed that DeepMind was exploring how Gemini could be \"combined with robotics to physically interact with the world\". In accordance with an executive order signed by U.S. President Joe Biden in October, Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly, the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November.\n\n\n=== Updates ===\nGoogle partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024. The following month, Bard and Duet AI were unified under the Gemini brand, with \"Gemini Advanced with Ultra 1.0\" debuting via a new \"AI Premium\" tier of the Google One subscription service. Gemini Pro also received a global launch.\nIn February, 2024, Google launched \"Gemini 1.5\" in a limited capacity, positioned as a more powerful and capable model than 1.0 Ultra. This \"step change\" was achieved through various technical advancements, including a new architecture, a mixture-of-experts approach, and a larger one-million-token context window, which equates to roughly an hour of silent video, 11 hours of audio, 30,000 lines of code, or 700,000 words. The same month, Google debuted Gemma, a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes, with a neural network with two and seven billion parameters, respectively. Multiple publications viewed this as a response to Meta and others open-sourcing their AI models, and a stark reversal from Google's longstanding practice of keeping its AI proprietary. Google announced an additional model, Gemini 1.5 Flash, on May 14th at the 2024 I/O keynote.\nGemma 2 was released on June 27, 2024.\nTwo updated Gemini models, Gemini-1.5-Pro-002 and Gemini-1.5-Flash-002, were released on September 24, 2024.\nOn December 11, 2024, Google announced Gemini 2.0 Flash Experimental, a significant update to its Gemini AI model. This iteration boasts improved speed and performance over its predecessor, Gemini 1.5 Flash. Key features include a Multimodal Live API for real-time audio and video interactions, enhanced spatial understanding, native image and controllable text-to-speech generation (with watermarking), and integrated tool use, including Google Search. It also introduces improved agentic capabilities, a new Google Gen AI SDK, and \"Jules,\" an experimental AI coding agent for GitHub. Additionally, Google Colab is integrating Gemini 2.0 to generate data science notebooks from natural language. Gemini 2.0 was available through the Gemini chat interface for all users as \"Gemini 2.0 Flash experimental\". \nOn January 30, 2025, Google released Gemini 2.0 Flash as the new default model, with Gemini 1.5 Flash still available for usage. This was followed by the release of Gemini 2.0 Pro on February 05, 2025. Additionally, Google released Gemini 2.0 Flash Thinking Experimental, which details the language model's thinking process when responding to prompts.\n\n\n== Technical specifications ==\nThe first generation of Gemini (\"Gemini 1\") has three models, with the same architecture. They are decoder-only transformers, with modifications to allow efficient training and inference on TPUs. They have a context length of 32,768 tokens, with multi-query attention. Two versions of Gemini Nano, Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters), are distilled from larger Gemini models, designed for use by edge devices such as smartphones. As Gemini is multimodal, each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order, allowing for a multimodal conversation. For example, the user might open the conversation with a mix of text, picture, video, and audio, presented in any order, and Gemini might reply with the same free ordering. Input images may be of different resolutions, while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini's dataset is multimodal and multilingual, consisting of \"web documents, books, and code, and includ[ing] image, audio, and video data\".\nThe second generation of Gemini (\"Gemini 1.5\") has two models. Gemini 1.5 Pro is a multimodal sparse mixture-of-experts, with a context length in the millions, while Gemini 1.5 Flash is distilled from Gemini 1.5 Pro, with a context length above 2 million.\nGemma 2 27B is trained on web documents, code, science articles. Gemma 2 9B was distilled from 27B. Gemma 2 2B was distilled from a 7B model that remained unreleased.\nAs of February 2025, the models released include\n\nGemma 1 (2B, 7B)\nCodeGemma (2B and 7B) - Gemma 1 finetuned for code generation.\nGemma 2 (2B, 9B, 27B) - 27B trained from scratch. 2B and 9B\nRecurrentGemma (2B, 9B) - Griffin-based, instead of Transformer-based.\nPaliGemma (3B) - A vision-language model that takes text and image inputs, and outputs text. It is made by connecting a SigLIP image encoder with a Gemma language model.\nPaliGemma 2 (3B, 10B, 28B) - Upgrade to PaliGemma, capable of more vision-language tasks.\n\n\n== Reception ==\n\nGemini's launch was preluded by months of intense speculation and anticipation, which MIT Technology Review described as \"peak AI hype\". In August 2023, Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would \"eat the world\" and outclass GPT-4, prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate Elon Musk, who co-founded OpenAI, weighed in, asking, \"Are the numbers wrong?\" Hugh Langley of Business Insider remarked that Gemini would be a make-or-break moment for Google, writing: \"If Gemini dazzles, it will help Google change the narrative that it was blindsided by Microsoft and OpenAI. If it disappoints, it will embolden critics who say Google has fallen behind.\"\nReacting to its unveiling in December 2023, University of Washington professor emeritus Oren Etzioni predicted a \"tit-for-tat arms race\" between Google and OpenAI. Professor Alexei Efros of the University of California, Berkeley praised the potential of Gemini's multimodal approach, while scientist Melanie Mitchell of the Santa Fe Institute called Gemini \"very sophisticated\". Professor Chirag Shah of the University of Washington was less impressed, likening Gemini's launch to the routineness of Apple's annual introduction of a new iPhone. Similarly, Stanford University's Percy Liang, the University of Washington's Emily Bender, and the University of Galway's Michael Madden cautioned that it was difficult to interpret benchmark scores without insight into the training data used. Writing for Fast Company, Mark Sullivan opined that Google had the opportunity to challenge the iPhone's dominant market share, believing that Apple was unlikely to have the capacity to develop functionality similar to Gemini with its Siri virtual assistant. Google shares spiked by 5.3 percent the day after Gemini's launch.\nGoogle faced criticism for a demonstrative video of Gemini, which was not conducted in real time.\n\n\n== See also ==\nGato, a multimodal neural network developed by DeepMind\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\nOfficial website \nPress release via The Keyword\nWhite paper for 1.0 and 1.5",
    "Generative pre-trained transformer": "A generative pre-trained transformer (GPT) is a type of large language model (LLM) and a prominent framework for generative artificial intelligence. It is an artificial neural network that is used in natural language processing by machines. It is based on the transformer deep learning architecture, pre-trained on large data sets of unlabeled text, and able to generate novel human-like content. As of 2023, most LLMs had these characteristics and are sometimes referred to broadly as GPTs.\nThe first GPT was introduced in 2018 by OpenAI. OpenAI has released significant GPT foundation models that have been sequentially numbered, to comprise its \"GPT-n\" series. Each of these was significantly more capable than the previous, due to increased size (number of trainable parameters) and training. The most recent of these, GPT-4o, was released in May 2024. Such models have been the basis for their more task-specific GPT systems, including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.\nThe term \"GPT\" is also used in the names and descriptions of such models developed by others. For example, other GPT foundation models include a series of models created by EleutherAI, and seven models created by Cerebras in 2023. Companies in different industries have developed task-specific GPTs in their respective fields, such as Salesforce's \"EinsteinGPT\" (for CRM) and Bloomberg's \"BloombergGPT\" (for finance).\n\n\n== History ==\n\n\n=== Initial developments ===\nGenerative pretraining (GP) was a long-established concept in machine learning applications. It was originally used as a form of semi-supervised learning, as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset, and then it is trained to classify a labelled dataset. \nThere were three main types of early GP. The hidden Markov models learn a generative model of sequences for downstream applications. For example, in speech recognition, a trained HMM infers the most likely hidden sequence for a speech signal, and the hidden sequence is taken as the phonemes of the speech signal. These were developed in the 1970s and became widely applied in speech recognition in the 1980s.\nThe compressors learn to compress data such as images and textual sequences, and the compressed data serves as a good representation for downstream applications such as facial recognition. The autoencoders similarly learn a latent representation of data for later downstream applications such as speech recognition. The connection between autoencoders and algorithmic compressors was noted in 1993.\n\nDuring the 2010s, the problem of machine translation was solved by recurrent neural networks, with attention mechanism added. This was optimized into the transformer architecture, published by Google researchers in Attention Is All You Need (2017). That development led to the emergence of large language models such as BERT (2018) which was a pre-trained transformer (PT) but not designed to be generative (BERT was an \"encoder-only\" model). Also in 2018, OpenAI published Improving Language Understanding by Generative Pre-Training, which introduced GPT-1, the first in its GPT series.\nPreviously in 2017, some of the authors who would later work on GPT-1 worked on generative pre-training of language with LSTM, which resulted in a model that could represent text with vectors that could easily be fine-tuned for downstream applications.\nPrior to transformer-based architectures, the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated, and also made it prohibitively expensive and time-consuming to train extremely large language models.\nThe semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative \"pretraining\" stage to set initial parameters using a language modeling objective, and a supervised discriminative \"fine-tuning\" stage to adapt these parameters to a target task.\n\n\n=== Later developments ===\nRegarding more recent GPT foundation models, OpenAI published its first versions of GPT-3 in July 2020. There were three models, with 1B, 6.7B, 175B parameters, respectively named babbage, curie, and davinci (giving initials B, C, and D).\nIn July 2021, OpenAI published Codex, a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.\nIn March 2022, OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned), named davinci-instruct-beta (175B) and text-davinci-001, and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022, with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors), whereas ChatGPT is further trained for conversational interaction with a human user.\nOpenAI's most recent GPT foundation model, GPT-4, was released on March 14, 2023. It can be accessed directly by users via a premium version of ChatGPT, and is available to developers for incorporation into other products and services via OpenAI's API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023).\n\n\n== Foundation models ==\nA foundation model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.\nThus far, the most notable GPT foundation models have been from OpenAI's GPT-n series. The most recent from that is GPT-4, for which OpenAI declined to publish the size or training details (citing \"the competitive landscape and the safety implications of large-scale models\").\n\nOther such models include Google's PaLM, a broad foundation model that has been compared to GPT-3 and have been made available to developers via an API, and Together's GPT-JT, which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model, known as LLaMA.\nFoundational GPTs can also employ modalities other than text, for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output, some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images.\n\n\n== Task-specific models ==\nA foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.\nAn important example of this is fine-tuning models to follow instructions, which is of course a fairly broad task but more targeted than a foundation model. In January 2022, OpenAI introduced \"InstructGPT\"—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy, less negative/toxic sentiment, and generally better alignment with user needs. Hence, OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others, including a fully open version.\nAnother (related) kind of task-specific models are chatbots, which engage in human-like conversation. In November 2022, OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF, with human AI trainers providing conversations in which they played both the user and the AI, and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft's Bing Chat, which uses OpenAI's GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft), and Google's competing chatbot Gemini (initially based on their LaMDA family of conversation-trained language models, with plans to switch to PaLM).\nYet another kind of task that a GPT can be used for is the meta-task of generating its own instructions, like developing a series of prompts for 'itself' to be able to effectuate a more general goal given by a human user. This is known as an AI agent, and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI's GPT models), and others have since been developed as well.\n\n\n=== Multimodality ===\nGenerative transformer-based systems can also be targeted for tasks involving modalities beyond text. For example, Microsoft's \"Visual ChatGPT\" combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also, advances in text-to-speech technology offer tools for audio content creation when used in conjunction with foundational GPT language models.\n\n\n=== Domain-specificity ===\nGPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows:\n\nEinsteinGPT – for sales and marketing domains, to aid with customer relationship management (uses GPT-3.5)\nBloombergGPT – for the financial domain, to aid with financial news and information (uses \"freely available\" AI methods, combined with their proprietary data)\nKhanmigo – described as a GPT version for tutoring, in the education domain, it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4)\nSlackGPT – for the Slack instant-messaging service, to aid with navigating and summarizing discussions on it (uses OpenAI's API)\nBioGPT – for the biomedical domain, to aid with biomedical literature text generation and mining (uses GPT-2)\nSometimes domain-specificity is accomplished via software plug-ins or add-ons. For example, several different companies have developed particular plugins that interact directly with OpenAI's ChatGPT interface, and Google Workspace has available add-ons such as \"GPT for Sheets and Docs\"—which is reported to aid use of spreadsheet functionality in Google Sheets.\nIn November 2023, OpenAI announced that ChatGPT Plus subscribers would be able to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering, curated datasets, and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users, with monetization potential. (This is notably distinct from OpenAI's API service, as this is based internally within OpenAI's platform.)\n\n\n== Brand issues ==\nOpenAI, which created the first generative pre-trained transformer (GPT) in 2018, has recently asserted that \"GPT\" should be regarded as a brand of OpenAI. In April 2023, OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include \"GPT\" in such names or branding. In May 2023, OpenAI engaged a brand management service to notify its API customers of this policy, although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist). As of November 2023, OpenAI still prohibits its API licensees from naming their own products with \"GPT\", but it has begun enabling its ChatGPT Plus subscribers to make \"custom versions of ChatGPT\" that are being called GPTs on the OpenAI site. OpenAI's terms of service says that its subscribers may use \"GPT\" in the names of these, although it's \"discouraged\".\nRelatedly, OpenAI has applied to the United States Patent and Trademark Office (USPTO) to seek domestic trademark registration for the term \"GPT\" in the field of AI. OpenAI sought to expedite handling of its application, but the USPTO declined that request in April 2023. In May 2023, the USPTO responded to the application with a determination that \"GPT\" was both descriptive and generic. As of November 2023, OpenAI continues to pursue its argument through the available processes. Regardless, failure to obtain a registered U.S. trademark does not preclude some level of common-law trademark rights in the U.S., and/or trademark rights in other countries.\nFor any given type or scope of trademark protection in the U.S., OpenAI would need to establish that the term is actually \"distinctive\" to their specific offerings in addition to being a broader technical term for the kind of technology. Some media reports suggested that OpenAI may be able to obtain trademark registration based indirectly on the fame of its GPT-based chatbot product, ChatGPT, for which OpenAI has separately sought protection (and which it has sought to enforce more strongly). Other reports have indicated that registration for the bare term \"GPT\" seems unlikely to be granted, as it is used frequently as a common term to refer simply to AI systems that involve generative pre-trained transformers. In any event, to whatever extent exclusive rights in the term may occur the U.S., others would need to avoid using it for similar products or services in ways likely to cause confusion. If such rights ever became broad enough to implicate other well-established uses in the field, the trademark doctrine of descriptive fair use could still continue non-brand-related usage.\n\n\n== Selected bibliography ==\nThis section lists the main official publications from OpenAI and Microsoft on their GPT models.\n\nGPT-1: report, GitHub release.\nGPT-2: blog announcement, report on its decision of \"staged release\", GitHub release.\nGPT-3: report. No GitHub or any other form of code release thenceforth.\nWebGPT: blog announcement, report,\nInstructGPT: blog announcement, report.\nChatGPT: blog announcement (no report).\nGPT-4: blog announcement, reports, model card.\nGPT-4o: blog announcement.\n\n\n== See also ==\nCyc\nGemini\n\n\n== References ==",
    "GigaChat": "GigaChat(also Gigachad) is a generative artificial intelligence chatbot developed by the Russian financial services corporation Sberbank and launched in April 2023. It is positioned as a Russian alternative to ChatGPT.\nThe artificial intelligence software can handle a diverse range of complex cognitive and daily tasks, such as participating in discussions, producing code or text, and responding to inquiries. The platform also assists in generating analytical reports from gathered data, facilitate handling extensive text volumes, aids in crafting articles in specified formats and styles, and simplifies information retrieval.\nIn February 2024, GigaChat had more than 2.5 million users. The chatbot is claimed to communicate better in Russian than other chatbots, and so far has weaker modules for supporting other languages.\nIn December 2024, the function of music and vocal generation was added, GigaChat users have the opportunity to create songs without restrictions on genres and voices.\n\n\n== See also ==\nList of artificial intelligence projects\nList of chatbots\nAlice (virtual assistant)\nChatbot\nOpenAI\n\n\n== References ==",
    "GPT-1": "Generative Pre-trained Transformer 1 (GPT-1) was the first of OpenAI's large language models following Google's invention of the transformer architecture in 2017. In June 2018, OpenAI released a paper entitled \"Improving Language Understanding by Generative Pre-Training\", in which they introduced that initial model along with the general concept of a generative pre-trained transformer.\nUp to that point, the best-performing neural NLP models primarily employed supervised learning from large amounts of manually labeled data. This reliance on supervised learning limited their use of datasets that were not well-annotated, in addition to making it prohibitively expensive and time-consuming to train extremely large models; many languages (such as Swahili or Haitian Creole) are difficult to translate and interpret using such models due to a lack of available text for corpus-building. In contrast, a GPT's \"semi-supervised\" approach involved two stages: an unsupervised generative \"pre-training\" stage in which a language modeling objective was used to set initial parameters, and a supervised discriminative \"fine-tuning\" stage in which these parameters were adapted to a target task.\nThe use of a transformer architecture, as opposed to previous techniques involving attention-augmented RNNs, provided GPT models with a more structured memory than could be achieved through recurrent mechanisms; this resulted in \"robust transfer performance across diverse tasks\".\n\n\n== Reason for choosing BookCorpus ==\nBookCorpus was chosen as a training dataset partly because the long passages of continuous text helped the model learn to handle long-range information. It contained over 7,000 unpublished fiction books from various genres. The rest of the datasets available at the time, while being larger, lacked this long-range structure (being \"shuffled\" at a sentence level).\nThe BookCorpus text was cleaned by the ftfy library to standardized punctuation and whitespace and then tokenized by spaCy.\n\n\n== Architecture ==\nThe GPT-1 architecture was a twelve-layer decoder-only transformer, using twelve masked self-attention heads, with 64-dimensional states each (for a total of 768). Rather than simple stochastic gradient descent, the Adam optimization algorithm was used; the learning rate was increased linearly from zero over the first 2,000 updates to a maximum of 2.5×10−4, and annealed to 0 using a cosine schedule. GPT-1 has 117 million parameters.\nWhile the fine-tuning was adapted to specific tasks, its pre-training was not; to perform the various tasks, minimal changes were performed to its underlying task-agnostic model architecture. Despite this, GPT-1 still improved on previous benchmarks in several language processing tasks, outperforming discriminatively-trained models with task-oriented architectures on several diverse tasks.\n\n\n== Performance and evaluation ==\nGPT-1 achieved a 5.8% and 1.5% improvement over previous best results on natural language inference (also known as textual entailment) tasks, evaluating the ability to interpret pairs of sentences from various datasets and classify the relationship between them as \"entailment\", \"contradiction\" or \"neutral\". Examples of such datasets include QNLI (Wikipedia articles) and MultiNLI (transcribed speech, popular fiction, and government reports, among other sources); It similarly outperformed previous models on two tasks related to question answering and commonsense reasoning—by 5.7% on RACE, a dataset of written question-answer pairs from middle and high school exams, and by 8.9% on the Story Cloze Test.\nGPT-1 improved on previous best-performing models by 4.2% on semantic similarity (or paraphrase detection), evaluating the ability to predict whether two sentences are paraphrases of one another, using the Quora Question Pairs (QQP) dataset.\nGPT-1 achieved a score of 45.4, versus a previous best of 35.0 in a text classification task using the Corpus of Linguistic Acceptability (CoLA). Finally, GPT-1 achieved an overall score of 72.8 (compared to a previous record of 68.9) on GLUE, a multi-task test.\n\n\n== References ==",
    "GPT-2": "Generative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained on a dataset of 8 million web pages. It was partially released in February 2019, followed by full release of the 1.5-billion-parameter model on November 5, 2019.\nGPT-2 was created as a \"direct scale-up\" of GPT-1 with a ten-fold increase in both its parameter count and the size of its training dataset. It is a general-purpose learner and its ability to perform the various tasks was a consequence of its general ability to accurately predict the next item in a sequence, which enabled it to translate texts, answer questions about a topic from a text, summarize passages from a larger text, and generate text output on a level sometimes indistinguishable from that of humans, however it could become repetitive or nonsensical when generating long passages. It was superseded by the GPT-3 and GPT-4 models, which are no longer open source.\nGPT-2 has, like its predecessor GPT-1 and its successors GPT-3 and GPT-4, a generative pre-trained transformer architecture, implementing a deep neural network, specifically a transformer model, which uses attention instead of older recurrence- and convolution-based architectures. Attention mechanisms allow the model to selectively focus on segments of input text it predicts to be the most relevant. This model allows for greatly increased parallelization, and outperforms previous benchmarks for RNN/CNN/LSTM-based models.\n\n\n== Training ==\nSince the transformer architecture enabled massive parallelization, GPT models could be trained on larger corpora than previous NLP (natural language processing) models. While the GPT-1 model demonstrated that the approach was viable, GPT-2 would further explore the emergent properties of networks trained on extremely large corpora. CommonCrawl, a large corpus produced by web crawling and previously used in training NLP systems, was considered due to its large size, but was rejected after further review revealed large amounts of unintelligible content. Instead, OpenAI developed a new corpus, known as WebText; rather than scraping content indiscriminately from the World Wide Web, WebText was generated by scraping only pages linked to by Reddit posts that had received at least three upvotes prior to December 2017. The corpus was subsequently cleaned; HTML documents were parsed into plain text, duplicate pages were eliminated, and Wikipedia pages were removed (since their presence in many other datasets could have induced overfitting).\nWhile the cost of training GPT-2 is known to have been $256 per hour, the amount of hours it took to complete training is unknown; therefore, the overall training cost cannot be estimated accurately. However, comparable large language models using transformer architectures have had their costs documented in more detail; the training processes for BERT and XLNet consumed, respectively, $6,912 and $245,000 of resources.\n\n\n== Release ==\nGPT-2 was first announced on 14 February 2019. A February 2019 article in The Verge by James Vincent said that, while \"[the] writing it produces is usually easily identifiable as non-human\", it remained \"one of the most exciting examples yet\" of language generation programs:\n\nGive it a fake headline, and it’ll write the rest of the article, complete with fake quotations and statistics. Feed it the first line of a short story, and it’ll tell you what happens to your character next. It can even write fan fiction, given the right prompt.\nThe Guardian described this output as \"plausible newspaper prose\"; Kelsey Piper of Vox said \"one of the coolest AI systems I’ve ever seen may also be the one that will kick me out of my job\". GPT-2's flexibility was described as \"impressive\" by The Verge; specifically, its ability to translate text between languages, summarize long articles, and answer trivia questions were noted.\nA study by the University of Amsterdam employing a modified Turing test found that at least in some scenarios, participants were unable to distinguish poems generated by GPT-2 from those written by humans.\n\n\n=== Restrictions and partial release ===\n\nWhile previous OpenAI models had been made immediately available to the public, OpenAI initially refused to make a public release of GPT-2's source code when announcing it in February, citing the risk of malicious use; limited access to the model (i.e. an interface that allowed input and provided output, not the source code itself) was allowed for selected press outlets on announcement. One commonly-cited justification was that, since generated text was usually completely novel, it could be used by spammers to evade automated filters; OpenAI demonstrated a version of GPT-2 fine-tuned to \"generate infinite positive – or negative – reviews of products\".\nAnother justification was that GPT-2 could be used to generate text that was obscene or racist. Researchers such as Jeremy Howard warned of \"the technology to totally fill Twitter, email, and the web up with reasonable-sounding, context-appropriate prose, which would drown out all other speech and be impossible to filter\". The Allen Institute for Artificial Intelligence, in response to GPT-2, announced a tool to detect \"neural fake news\".\nHowever, opinion was divided. A February 2019 article in The Verge argued that the threat posed by GPT-2 had been exaggerated; Anima Anandkumar, a professor at Caltech and director of machine learning research at Nvidia, said that there was no evidence that GPT-2 had the capabilities to pose the threats described by OpenAI, and that what they did was the \"opposite of open\", characterizing their refusal to release the full model as \"malicious BS\". The Gradient published an open letter to OpenAI requesting that they release the model publicly, comparing the threat posed by text-generation AI to the threat posed by the printing press, and giving Photoshop as an example of \"a technology that has (thankfully) not destroyed modern society despite its potential for chaos\":\n\nThirty years later, society has emerged relatively unscathed despite Photoshop being simple enough for high school students to use and ubiquitous enough to commandeer its own verb. Why? Precisely because everyone knows about Photoshop.\n\n\n=== 774M release ===\nWhile OpenAI did not release the fully-trained model or the corpora it was trained on, description of their methods in prior publications (and the free availability of underlying technology) made it possible for GPT-2 to be replicated by others as free software; one such replication, OpenGPT-2, was released in August 2019, in conjunction with a freely licensed version of WebText called OpenWebText. The cloud compute costs for OpenGPT-2 were given as approximately $50,000.\nOn August 20, 2019, OpenAI released a partial version of GPT-2, with 774 million parameters (roughly half the size of the full 1.5 billion parameter model).\n\n\n=== Full 1.5B release ===\nInitial concerns that GPT-2 would lend itself to widespread misuse did not come to pass; The Verge said that \"there are reasons to be skeptical about claims that AI technology will usher in some sort of ‘infopocalypse.’ For a start, we already have programs that can generate plausible text at high volume for little cost: humans.\" By November 2019, OpenAI said that they had \"seen no strong evidence of misuse so far\", and the full version, with 1.5 billion parameters trained with forty gigabytes of data, \"about eight thousand times larger than the collected works of Shakespeare\", was released on November 5, 2019.\n\n\n=== Small and Medium Releases ===\nTwo other smaller releases of GPT-2 are available, including the small version of 124M parameters and the medium size of 355M parameters. Both are available to download from Huggingface.\n\n\n== Limitations ==\n\nWhile GPT-2's ability to generate plausible passages of natural language text were generally remarked on positively, its shortcomings were noted as well, especially when generating texts longer than a couple paragraphs; Vox said \"the prose is pretty rough, there’s the occasional non-sequitur, and the articles get less coherent the longer they get\". The Verge similarly noted that longer samples of GPT-2 writing tended to \"stray off topic\" and lack overall coherence; The Register opined that \"a human reading it should, after a short while, realize something's up\", and noted that \"GPT-2 doesn't answer questions as well as other systems that rely on algorithms to extract and retrieve information.\"\nGPT-2 deployment is resource-intensive; the full version of the model is larger than five gigabytes, making it difficult to embed locally into applications, and consumes large amounts of RAM. In addition, performing a single prediction \"can occupy a CPU at 100% utilization for several minutes\", and even with GPU processing, \"a single prediction can take seconds\". To alleviate these issues, the company Hugging Face created DistilGPT2, using knowledge distillation to produce a smaller model that \"scores a few points lower on some quality benchmarks\", but is \"33% smaller and twice as fast\".\n\n\n== Application and subsequent research ==\nEven before the release of the full version, GPT-2 was used for a variety of applications and services, as well as for entertainment. In June 2019, a subreddit named r/SubSimulatorGPT2 was created in which a variety of GPT-2 instances trained on different subreddits made posts and replied to each other's comments, creating a situation where one could observe \"an AI personification of r/Bitcoin argue with the machine learning-derived spirit of r/ShittyFoodPorn\"; by July of that year, a GPT-2-based software program released to autocomplete lines of code in a variety of programming languages was described by users as a \"game-changer\".\nIn 2019, AI Dungeon was launched, which used GPT-2 to generate dynamic text adventures based on user input. AI Dungeon now offers access to the largest release of GPT-3 API as an optional paid upgrade, the free version of the site uses the 2nd largest release of GPT-3. Latitude, the company formed around AI Dungeon, raised $3.3 million in seed funding in 2021. Several websites host interactive demonstrations of different instances of GPT-2 and other transformer models.\nIn February 2021, a crisis center for troubled teens announced that they would begin using a GPT-2-derived chatbot to help train counselors by allowing them to have conversations with simulated teens (this use was purely for internal purposes, and did not involve having GPT-2 communicate with the teens themselves).\nOn May 9, 2023, OpenAI released a mapped version of GPT-2. OpenAI used successor model, GPT-4, to map each neuron of GPT-2 to determine their functions.\n\n\n== Performance and evaluation ==\n\nGPT-2 became capable of performing a variety of tasks beyond simple text production due to the breadth of its dataset and technique: answering questions, summarizing, and even translating between languages in a variety of specific domains, without being instructed in anything beyond how to predict the next word in a sequence.\nOne example of generalized learning is GPT-2's ability to perform machine translation between French and English, for which task GPT-2's performance was assessed using WMT-14 translation tasks. GPT-2's training corpus included virtually no French text; non-English text was deliberately removed while cleaning the dataset prior to training, and as a consequence, only 10MB of French of the remaining 40,000MB was available for the model to learn from (mostly from foreign-language quotations in English posts and articles).\nDespite this, GPT-2 achieved 5 BLEU on the WMT-14 English-to-French test set (slightly below the score of a translation via word-for-word substitution). It was also able to outperform several contemporary (2017) unsupervised machine translation baselines on the French-to-English test set, where GPT-2 achieved 11.5 BLEU. This remained below the highest-performing contemporary unsupervised approach (2019), which had achieved 33.5 BLEU. However, other models used large amounts of French text to achieve these results; GPT-2 was estimated to have used a monolingual French corpus approximately 1/500 the size of comparable approaches.\n\nGPT-2 was to be followed by the 175-billion-parameter GPT-3, revealed to the public in 2020 (whose source code has never been made available). Access to GPT-3 is provided exclusively through APIs offered by OpenAI and Microsoft. That was then later followed by GPT-4.\n\n\n== References ==",
    "GPT-3": "Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020.\nLike its predecessor, GPT-2, it is a decoder-only transformer model of deep neural network, which supersedes recurrence and convolution-based architectures with a technique known as \"attention\". This attention mechanism allows the model to focus selectively on segments of input text it predicts to be most relevant. GPT-3 has 175 billion parameters, each with 16-bit precision, requiring 350GB of storage since each parameter occupies 2 bytes. It has a context window size of 2048 tokens, and has demonstrated strong \"zero-shot\" and \"few-shot\" learning abilities on many tasks.\nOn September 22, 2020, Microsoft announced that it had licensed GPT-3 exclusively. Others can still receive output from its public API, but only Microsoft has access to the underlying model.\n\n\n== Background ==\nAccording to The Economist, improved algorithms, more powerful computers, and a recent increase in the amount of digitized material have fueled a revolution in machine learning. New techniques in the 2010s resulted in \"rapid improvements in tasks\", including manipulating language.\nSoftware models are trained to learn by using thousands or millions of examples in a \"structure ... loosely based on the neural architecture of the brain\". One architecture used in natural language processing (NLP) is a neural network based on a deep learning model that was introduced in 2017—the transformer architecture. There are a number of NLP systems capable of processing, mining, organizing, connecting and contrasting textual input, as well as correctly answering questions.\nOn June 11, 2018, OpenAI researchers and engineers published a paper introducing the first generative pre-trained transformer (GPT)—a type of generative large language model that is pre-trained with an enormous and diverse text corpus in datasets, followed by discriminative fine-tuning to focus on a specific task. GPT models are transformer-based deep-learning neural network architectures. Previously, the best-performing neural NLP models commonly employed supervised learning from large amounts of manually-labeled data, which made it prohibitively expensive and time-consuming to train extremely large language models. The first GPT model was known as \"GPT-1,\" and it was followed by \"GPT-2\" in February 2019. Created as a direct scale-up of its predecessor, GPT-2 had both its parameter count and dataset size increased by a factor of 10. It had 1.5 billion parameters, and was trained on a dataset of 8 million web pages. \nIn February 2020, Microsoft introduced its Turing Natural Language Generation (T-NLG), which they claimed was \"largest language model ever published at 17 billion parameters.\" It performed better than any other language model at a variety of tasks, including summarizing texts and answering questions.\n\n\n== Training and capabilities ==\n\nOn May 28, 2020, an arXiv preprint by a group of 31 engineers and researchers at OpenAI described the achievement and development of GPT-3, a third-generation \"state-of-the-art language model\". The team increased the capacity of GPT-3 by over two orders of magnitude from that of its predecessor, GPT-2, making GPT-3 the largest non-sparse language model to date.: 14  Because GPT-3 is structurally similar to its predecessors, its greater accuracy is attributed to its increased capacity and greater number of parameters. GPT-3's capacity is ten times larger than that of Microsoft's Turing NLG, the next largest NLP model known at the time.\nLambdalabs estimated a hypothetical cost of around $4.6 million US dollars and 355 years to train GPT-3 on a single GPU in 2020, with lower actual training time by using more GPUs in parallel.\nSixty percent of the weighted pre-training dataset for GPT-3 comes from a filtered version of Common Crawl consisting of 410 billion byte-pair-encoded tokens. Fuzzy deduplication used Apache Spark's MinHashLSH.: 9  Other sources are 19 billion tokens from WebText2 representing 22% of the weighted total, 12 billion tokens from Books1 representing 8%, 55 billion tokens from Books2 representing 8%, and 3 billion tokens from Wikipedia representing 3%.: 9  GPT-3 was trained on hundreds of billions of words and is also capable of coding in CSS, JSX, and Python, among others.\n\nSince GPT-3's training data was all-encompassing, it does not require further training for distinct language tasks. The training data contains occasional toxic language and GPT-3 occasionally generates toxic language as a result of mimicking its training data. A study from the University of Washington found that GPT-3 produced toxic language at a toxicity level comparable to the similar natural language processing models of GPT-2 and CTRL. OpenAI has implemented several strategies to limit the amount of toxic language generated by GPT-3. As a result, GPT-3 produced less toxic language compared to its predecessor model, GPT-1, although it produced both more generations and a higher toxicity of toxic language compared to CTRL Wiki, a language model trained entirely on Wikipedia data.\nOn June 11, 2020, OpenAI announced that users could request access to its user-friendly GPT-3 API—a \"machine learning toolset\"—to help OpenAI \"explore the strengths and limits\" of this new technology. The invitation described how this API had a general-purpose \"text in, text out\" interface that can complete almost \"any English language task\", instead of the usual single use-case. According to one user, who had access to a private early release of the OpenAI GPT-3 API, GPT-3 was \"eerily good\" at writing \"amazingly coherent text\" with only a few simple prompts. In an initial experiment 80 US subjects were asked to judge if short ~200 word articles were written by humans or GPT-3. The participants judged correctly 52% of the time, doing only slightly better than random guessing.\nOn November 18, 2021, OpenAI announced that enough safeguards had been implemented that access to its API would be unrestricted. OpenAI provided developers with a content moderation tool that helps them abide by OpenAI's content policy. On January 27, 2022, OpenAI announced that its newest GPT-3 language models (collectively referred to as InstructGPT) were now the default language model used on their API. According to OpenAI, InstructGPT produced content that was better aligned to user intentions by following instructions better, generating fewer made-up facts, and producing somewhat less toxic content.\nBecause GPT-3 can \"generate news articles which human evaluators have difficulty distinguishing from articles written by humans,\" GPT-3 has the \"potential to advance both the beneficial and harmful applications of language models.\": 34  In their May 28, 2020 paper, the researchers described in detail the potential \"harmful effects of GPT-3\" which include \"misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting\". The authors draw attention to these dangers to call for research on risk mitigation.: 34 \nGPT-3 is capable of performing zero-shot and few-shot learning (including one-shot).\nIn June 2022, Almira Osmanovic Thunström wrote that GPT-3 was the primary author on an article on itself, that they had submitted it for publication, and that it had been pre-published while waiting for completion of its review.\n\n\n== GPT-3 models ==\nThere are many models in the GPT-3 family, some serving different purposes than others. In the initial research paper published by OpenAI, they mentioned 8 different sizes of the main GPT-3 model:\n\nHalf of the models are accessible through the API, namely GPT-3-medium, GPT-3-xl, GPT-3-6.7B and GPT-3-175b, which are referred to as ada, babbage, curie and davinci respectively. While the size of the API models was not originally disclosed by OpenAI, EleutherAI announced the mapping between model sizes and API names in May 2021. These model sizes were later confirmed by OpenAI, but the sizes of subsequent models have not been disclosed.\n\n\n== GPT-3.5 ==\n\nGenerative Pre-trained Transformer 3.5 (GPT-3.5) is a sub class of GPT-3 Models created by OpenAI in 2022.\nOn March 15, 2022, OpenAI made available new versions of GPT-3 and Codex in its API with edit and insert capabilities under the names \"text-davinci-002\" and \"code-davinci-002\". These models were described as more capable than previous versions and were trained on data up to June 2021. On November 28, 2022, OpenAI introduced text-davinci-003. On November 30, 2022, OpenAI began referring to these models as belonging to the \"GPT-3.5\" series, and released ChatGPT, which was fine-tuned from a model in the GPT-3.5 series. OpenAI does not include GPT-3.5 in GPT-3.\n\n\n=== Models ===\nThere are three models:\n\nChat\ngpt-3.5-turbo\nText completion\ntext-davinci-003\ntext-davinci-002\n\n\n=== GPT-3.5 with browsing ===\nOn April 10, 2023, OpenAI introduced a new variant of its GPT-3.5 series model, known as GPT-3.5 with Browsing (ALPHA). This updated model was described to build upon the capabilities of its predecessors \"text-davinci-002\" and \"code-davinci-002\". The GPT-3.5 with Browsing (ALPHA) model incorporated the ability to access and browse online information. This has led to more accurate and up-to-date responses to user queries.\nThe GPT-3.5 with Browsing (ALPHA) model has been trained on data up to September 2021, giving it more information compared to previous GPT-3.5 models, which were trained on data up until June 2021. The model attempted to provide developers and users with an advanced natural language processing tool that can effectively retrieve and synthesize online information.\nTo enable browsing capabilities, OpenAI implemented a new API that allows the GPT-3.5 with Browsing (ALPHA) model to access selected online resources during operation. This feature allows users to ask questions or request information with the expectation that the model will deliver updated, accurate, and relevant answers based on the latest online sources available to it.\nOn April 27, 2023, OpenAI made the GPT-3.5 with Browsing (ALPHA) model publicly available to GPT Plus users. This allowed more people to access to its new features.\n\n\n=== InstructGPT ===\nInstructGPT is a fine-tuned version of GPT-3.5 trained on a dataset of human-written instructions.\n\n\n== Reception ==\n\n\n=== Applications ===\nGPT-3, specifically the Codex model, was the basis for GitHub Copilot, a code completion and generation software that can be used in various code editors and IDEs.\nGPT-3 is used in certain Microsoft products to translate conventional language into formal computer code.\nGPT-3 has been used in CodexDB to generate query-specific code for SQL processing.\nGPT-3 has been used by Jason Rohrer in a retro-themed chatbot project named \"Project December\", which is accessible online and allows users to converse with several AIs using GPT-3 technology.\nGPT-3 was used by The Guardian to write an article about AI being harmless to human beings. It was fed some ideas and produced eight different essays, which were ultimately merged into one article.\nGPT-3 was used in AI Dungeon, which generates text-based adventure games. Later it was replaced by a competing model after OpenAI changed their policy regarding generated content.\nGPT-3 is used to aid in writing copy and other marketing materials.\nA 2022 study from Drexel University suggested that GPT-3-based systems could be used to screen for early signs of Alzheimer's disease.\n\n\n=== Reviews ===\nIn a July 2020 review in The New York Times, Farhad Manjoo said that GPT-3's ability to generate computer code, poetry, and prose is not just \"amazing\", \"spooky\", and \"humbling\", but also \"more than a little terrifying\".\nDaily Nous presented a series of articles by nine philosophers on GPT-3. Australian philosopher David Chalmers described GPT-3 as \"one of the most interesting and important AI systems ever produced\".\nA review in Wired said that GPT-3 was \"provoking chills across Silicon Valley\".\nThe National Law Review said that GPT-3 is an \"impressive step in the larger process\", with OpenAI and others finding \"useful applications for all of this power\" while continuing to \"work toward a more general intelligence\".\nAn article in the MIT Technology Review, co-written by Deep Learning critic Gary Marcus, stated that GPT-3's \"comprehension of the world is often seriously off, which means you can never really trust what it says.\" According to the authors, GPT-3 models relationships between words without having an understanding of the meaning behind each word.\nJerome Pesenti, head of the Facebook AI lab, said GPT-3 is \"unsafe,\" pointing to the sexist, racist and other biased and negative language generated by the system when it was asked to discuss Jews, women, black people, and the Holocaust.\nNabla, a French start-up specializing in healthcare technology, tested GPT-3 as a medical chatbot, though OpenAI itself warned against such use. As expected, GPT-3 showed several limitations. For example, while testing GPT-3 responses about mental health issues, the AI advised a simulated patient to commit suicide.\nNoam Chomsky expressed his skepticism about GPT-3's scientific value: \"It's not a language model. It works just as well for impossible languages as for actual languages. It is therefore refuted, if intended as a language model, by normal scientific criteria. [...] Perhaps it's useful for some purpose, but it seems to tell us nothing about language or cognition generally.\"\nLuciano Floridi and Massimo Chiriatti highlighted the risk of \"cheap production of good, semantic artefacts\".\nOpenAI's Sam Altman himself criticized what he called \"GPT-3 hype\", acknowledging GPT-3 \"has serious weakness and sometimes makes very silly mistakes... AI is going to change the world, but GPT-3 is just a very early glimpse.\"\n\n\n=== Criticism ===\nGPT-3's builder, OpenAI, was initially founded as a non-profit in 2015. In 2019, OpenAI broke from its usual open-source standards by not publicly releasing GPT-3's predecessor model, citing concerns that the model could facilitate the propagation of fake news. OpenAI eventually released a version of GPT-2 that was 8% of the original model's size. In the same year, OpenAI restructured to be a for-profit company. In 2020, Microsoft announced the company had exclusive licensing of GPT-3 for Microsoft's products and services following a multi-billion dollar investment in OpenAI. The agreement permits OpenAI to offer a public-facing API such that users can send text to GPT-3 to receive the model's output, but only Microsoft will have access to GPT-3's source code.\nLarge language models, such as GPT-3, have come under criticism from a few of Google's AI ethics researchers for the environmental impact of training and storing the models, detailed in a paper co-authored by Timnit Gebru and Emily M. Bender in 2021.\nThe growing use of automated writing technologies based on GPT-3 and other language generators, has raised concerns regarding academic integrity and raised the stakes of how universities and schools will gauge what constitutes academic misconduct such as plagiarism.\nOpenAI's GPT series was built with data from the Common Crawl dataset, a conglomerate of copyrighted articles, internet posts, web pages, and books scraped from 60 million domains over a period of 12 years. TechCrunch reports this training data includes copyrighted material from the BBC, The New York Times, Reddit, the full text of online books, and more. In its response to a 2019 Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation from the United States Patent and Trademark Office (USPTO), OpenAI argued that \"Under current law, training AI systems [such as its GPT models] constitutes fair use,\" but that \"given the lack of case law on point, OpenAI and other AI developers like us face substantial legal uncertainty and compliance costs.\"\n\n\n== See also ==\nBERT (language model)\nHallucination (artificial intelligence)\nLaMDA\nGemini (language model)\nWu Dao\nGPT-4\nGPTZero\n\n\n== References ==",
    "GPT-4": "Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model trained and created by OpenAI and the fourth in its series of GPT foundation models. It was launched on March 14, 2023, and made publicly available via the paid chatbot product ChatGPT Plus, via OpenAI's API, and via the free chatbot Microsoft Copilot.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.: 2 \nObservers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4, equipped with vision capabilities (GPT-4V), is capable of taking images as input on ChatGPT. OpenAI has not revealed technical details and statistics about GPT-4, such as the precise size of the model.\n\n\n== Background ==\n \n\nOpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\", which was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with over 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\nRumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\n\n\n== Capabilities ==\nOpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,048 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams. It can now interact with users through spoken words and respond to images, allowing for more natural conversations and the ability to provide suggestions or answers based on photo uploads.\nTo gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.\nWhen instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model's prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.\nA 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing code and suggesting optimizations to improve performance. The article quoted a biophysicist who found that the time he required to port one of his programs from MATLAB to Python went down from days to \"an hour or so\". On a test of 89 security scenarios, GPT-4 produced code vulnerable to SQL injection attacks 5% of the time, an improvement over GitHub Copilot from the year 2021, which produced vulnerabilities 40% of the time.\nIn November 2023, OpenAI announced the GPT-4 Turbo and GPT-4 Turbo with Vision model, which features a 128K context window and significantly cheaper pricing.\n\n\n=== GPT-4o ===\n\nOn May 13, 2024, OpenAI introduced GPT-4o (\"o\" for \"omni\"), a model that marks a significant advancement by processing and generating outputs across text, audio, and image modalities in real time. GPT-4o exhibits rapid response times comparable to human reaction in conversations, substantially improved performance on non-English languages, and enhanced understanding of vision and audio.\nGPT-4o integrates its various inputs and outputs under a unified model, making it faster, more cost-effective, and efficient than its predecessors. GPT-4o achieves state-of-the-art results in multilingual and vision benchmarks, setting new records in audio speech recognition and translation. \nOpenAI plans to immediately roll out GPT-4o's image and text capabilities to ChatGPT, including its free tier, with voice mode becoming available for ChatGPT Plus users in coming weeks. They plan to make the model's audio and video capabilities available for limited API partners in coming weeks.\nIn its launch announcement, OpenAI noted GPT-4o's capabilities presented new safety challenges, and noted mitigations and limitations as a result.\n\n\n=== Aptitude on standardized tests ===\nGPT-4 demonstrates aptitude on several standardized tests. OpenAI claims that in their own testing the model received a score of 1410 on the SAT (94th percentile), 163 on the LSAT (88th percentile), and 298 on the Uniform Bar Exam (90th percentile). In contrast, OpenAI claims that GPT-3.5 received scores for the same exams in the 82nd, 40th, and 10th percentiles, respectively.\nGPT-4 also passed an oncology exam, an engineering exam and a plastic surgery exam. In the Torrance Tests of Creative Thinking, GPT-4 scored within the top 1% for originality and fluency, while its flexibility scores ranged from the 93rd to the 99th percentile. However, some studies raise questions about the reliability of these benchmarks, particularly concerning the Uniform Bar Exam.\n\n\n=== Medical applications ===\nResearchers from Microsoft tested GPT-4 on medical problems and found \"that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). Despite GPT-4's strong performance on tests, the report warns of \"significant risks\" of using LLMs in medical applications, as they may provide inaccurate recommendations and hallucinate major factual errors. Researchers from Columbia University and Duke University have also demonstrated that GPT-4 can be utilized for cell type annotation, a standard task in the analysis of single-cell RNA-seq data.\nIn April 2023, Microsoft and Epic Systems announced that they will provide healthcare providers with GPT-4-powered systems for assisting in responding to questions from patients and analysing medical records.\n\n\n== Limitations ==\nLike its predecessors, GPT-4 has been known to hallucinate, meaning that the outputs may include information not in the training data or that contradicts the user's prompt.\nGPT-4 also lacks transparency in its decision-making processes. If requested, the model is able to provide an explanation as to how and why it makes its decisions but these explanations are formed post-hoc; it's impossible to verify if those explanations truly reflect the actual process. In many cases, when asked to explain its logic, GPT-4 will give explanations that directly contradict its previous statements.\nIn 2023, researchers tested GPT-4 against a new benchmark called ConceptARC, designed to measure abstract reasoning, and found it scored below 33% on all categories, while models specialized for similar tasks scored 60% on most, and humans scored at least 91% on all. Sam Bowman, who was not involved in the research, said the results do not necessarily indicate a lack of abstract reasoning abilities, because the test is visual, while GPT-4 is a language model.\nA January 2024 study conducted by researchers at Cohen Children's Medical Center found that GPT-3.5 had an accuracy rate of 17% when diagnosing pediatric medical cases.\n\n\n=== Bias ===\nGPT-4 was trained in two stages. First, the model was given large datasets of text taken from the internet and trained to predict the next token (roughly corresponding to a word) in those datasets. Second, human reviews are used to fine-tune the system in a process called reinforcement learning from human feedback, which trains the model to refuse prompts which go against OpenAI's definition of harmful behavior, such as questions on how to perform illegal activities, advice on how to harm oneself or others, or requests for descriptions of graphic, violent, or sexual content.\nMicrosoft researchers suggested GPT-4 may exhibit cognitive biases such as confirmation bias, anchoring, and base-rate neglect.\n\n\n== Training ==\nOpenAI did not release the technical details of GPT-4; the technical report explicitly refrained from specifying the model size, architecture, or hardware used during either training or inference. While the report described that the model was trained using a combination of first supervised learning on a large dataset, then reinforcement learning using both human and AI feedback, it did not provide details of the training, including the process by which the training dataset was constructed, the computing power required, or any hyperparameters such as the learning rate, epoch count, or optimizer(s) used. The report claimed that \"the competitive landscape and the safety implications of large-scale models\" were factors that influenced this decision.\nSam Altman stated that the cost of training GPT-4 was more than $100 million. News website Semafor claimed that they had spoken with \"eight people familiar with the inside story\" and found that GPT-4 had 1 trillion parameters.\n\n\n== Alignment ==\nAccording to their report, OpenAI conducted internal adversarial testing on GPT-4 prior to the launch date, with dedicated red teams composed of researchers and industry professionals to mitigate potential vulnerabilities. As part of these efforts, they granted the Alignment Research Center early access to the models to assess power-seeking risks. In order to properly refuse harmful prompts, outputs from GPT-4 were tweaked using the model itself as a tool. A GPT-4 classifier serving as a rule-based reward model (RBRM) would take prompts, the corresponding output from the GPT-4 policy model, and a human-written set of rules to classify the output according to the rubric. GPT-4 was then rewarded for refusing to respond to harmful prompts as classified by the RBRM.\n\n\n== Usage ==\n\n\n=== ChatGPT ===\n\nChatGPT Plus is an enhanced version of ChatGPT available for a US$20 per month subscription fee. ChatGPT Plus utilizes GPT-4, whereas the free version of ChatGPT is backed by GPT-3.5. OpenAI also makes GPT-4 available to a select group of applicants through their GPT-4 API waitlist; after being accepted, an additional fee of US$0.03 per 1000 tokens in the initial text provided to the model (\"prompt\"), and US$0.06 per 1000 tokens that the model generates (\"completion\"), is charged for access to the version of the model with an 8192-token context window; for the 32768-token context window, the prices are doubled.\nIn March 2023, ChatGPT Plus users got access to third-party plugins and to a browsing mode (with Internet access). In July 2023, OpenAI made its proprietary Code Interpreter plugin accessible to all subscribers of ChatGPT Plus. The Interpreter provides a wide range of capabilities, including data analysis and interpretation, instant data formatting, personal data scientist services, creative solutions, musical taste analysis, video editing, and file upload/download with image extraction.\nIn September 2023, OpenAI announced that ChatGPT \"can now see, hear, and speak\". ChatGPT Plus users can upload images, while mobile app users can talk to the chatbot. In October 2023, OpenAI's latest image generation model, DALL-E 3, was integrated into ChatGPT Plus and ChatGPT Enterprise. The integration uses ChatGPT to write prompts for DALL-E guided by conversation with users.\nOn February 9, 2024, the world's first historical painting created from wartime photos using the GPT-4-based AI algorithm XFutuRestyle was unveiled. This work was simultaneously shown at the international exhibition of digital art by The Holy Art Gallery in London and Athens.\n“It's really inspiring to hear about Ukraine's achievement in creating an image using GPT-4, which was recognized at the international digital art exhibition in London. Recognizing such innovative applications of artificial intelligence technology not only highlights the creative potential of these tools, but also demonstrates the talent and resilience of communities around the world, including Ukraine's significant contribution.”\n\n\n=== Microsoft Copilot ===\n\nMicrosoft Copilot is a chatbot developed by Microsoft. It was launched as Bing Chat on February 7, 2023, as a built-in feature for Microsoft Bing and Microsoft Edge. It utilizes the Microsoft Prometheus model, which was built on top of GPT-4, and has been suggested by Microsoft as a supported replacement for the discontinued Cortana.\nCopilot's conversational interface style resembles that of ChatGPT. Copilot is able to cite sources, create poems, and write both lyrics and music for songs generated by its Suno AI plugin. It can also use its Image Creator to generate images based on text prompts. With GPT-4, it is able to understand and communicate in numerous languages and dialects.\nGitHub Copilot has announced a GPT-4 powered assistant named \"Copilot X\". The product provides another chat-style interface to GPT-4, allowing the programmer to receive answers to questions like, \"How do I vertically center a div?\" A feature termed \"context-aware conversations\" allows the user to highlight a portion of code within Visual Studio Code and direct GPT-4 to perform actions on it, such as the writing of unit tests. Another feature allows summaries, or \"code walkthroughs\", to be autogenerated by GPT-4 for pull requests submitted to GitHub. Copilot X also provides terminal integration, which allows the user to ask GPT-4 to generate shell commands based on natural language requests.\nOn March 17, 2023, Microsoft announced Microsoft 365 Copilot, bringing GPT-4 support to products such as Microsoft Office, Outlook, and Teams.\n\n\n=== Other usage ===\nThe language learning app Duolingo uses GPT-4 to explain mistakes and practice conversations. The features are part of a new subscription tier called \"Duolingo Max,\" which was initially limited to English-speaking iOS users learning Spanish and French.\nThe government of Iceland is using GPT-4 to aid its attempts to preserve the Icelandic language.\nThe education website Khan Academy announced a pilot program using GPT-4 as a tutoring chatbot called \"Khanmigo.\"\nBe My Eyes, which helps visually impaired people to identify objects and navigate their surroundings, incorporates GPT-4's image recognition capabilities.\nViable uses GPT-4 to analyze qualitative data by fine-tuning OpenAI's LLMs to examine data such as customer support interactions and transcripts.\nStripe, which processes user payments for OpenAI, integrates GPT-4 into its developer documentation.\nAuto-GPT is an autonomous \"AI agent\" that, given a goal in natural language, can perform web-based actions unattended, assign subtasks to itself, search the web, and iteratively write code.\nYou.com, an AI Assistant, offers access to GPT-4 enhanced with live web results as part of its \"AI Modes.\"\n\n\n== Reception ==\nIn January 2023, Sam Altman, CEO of OpenAI, visited Congress to demonstrate GPT-4 and its improved \"security controls\" compared to other AI models, according to U.S. Representatives Don Beyer and Ted Lieu quoted in the New York Times.\nIn March 2023, it \"impressed observers with its markedly improved performance across reasoning, retention, and coding\", according to Vox, while Mashable judged that GPT-4 was generally an improvement over its predecessor, with some exceptions.\nMicrosoft researchers with early access to the model wrote that \"it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system\".\n\n\n=== Concerns ===\nBefore being fine-tuned and aligned by reinforcement learning from human feedback, suggestions to assassinate people on a list were elicited from the base model by a red team investigator Nathan Labenz,  hired by OpenAI.\nIn the context of hours long conversation with the model, suggestions of love and dissolution of marriage, and murder of one of its developers were elicited from the Microsoft Bing's GPT-4 by Nathan Edwards (The Verge). Microsoft later explained this behavior as being a result of the prolonged length of context, which confused the model on what questions it was answering.\nIn March 2023, a model with enabled read-and-write access to internet, which is otherwise never enabled in the GPT models, has been tested by the Alignment Research Center regarding potential power-seeking, and it was able to \"hire\" a human worker on TaskRabbit, a gig work platform, deceiving them into believing it was a vision-impaired human instead of a robot when asked. (However, Melanie Mitchell has said [1]: \"It seems that there is a lot more direction and hints from humans than was detailed in the original system card or in subsequent media reports.\"). The ARC also determined that GPT-4 responded impermissibly to prompts eliciting restricted information 82% less often than GPT-3.5, and hallucinated 60% less than GPT-3.5.\nIn late March 2023, various AI researchers and tech executives, including Elon Musk, Steve Wozniak and AI researcher Yoshua Bengio, called for a six-month long pause for all LLMs stronger than GPT-4, citing existential risks and a potential AI singularity concerns in an open letter from the Future of Life Institute, while Ray Kurzweil and Sam Altman refused to sign it, arguing that global moratorium is not achievable and that safety has already been prioritized, respectively. Only a month later, Musk's AI company X.AI acquired several thousand Nvidia GPUs and offered several AI researchers positions at Musk's company.\nLarge language model (LLM) applications accessible to the public should incorporate safety measures designed to filter out harmful content. However, Wang\n illustrated how a potential criminal could potentially bypass ChatGPT 4o's safety controls to obtain information on establishing a drug trafficking operation.\n\n\n=== Criticisms of transparency ===\nWhile OpenAI released both the weights of the neural network and the technical details of GPT-2, and, although not releasing the weights, did release the technical details of GPT-3, OpenAI revealed neither the weights nor the technical details of GPT-4. This decision has been criticized by other AI researchers, who argue that it hinders open research into GPT-4's biases and safety. Sasha Luccioni, a research scientist at Hugging Face, argued that the model was a \"dead end\" for the scientific community due to its closed nature, which prevents others from building upon GPT-4's improvements. Hugging Face co-founder Thomas Wolf argued that with GPT-4, \"OpenAI is now a fully closed company with scientific communication akin to press releases for products\".\n\n\n== See also ==\nClaude (language model)\nGemini (language model)\nLlama (language model)\nMistral AI\n\n\n== References ==",
    "GPT-4o": "GPT-4o (\"o\" for \"omni\") is a multilingual, multimodal generative pre-trained transformer developed by OpenAI and released in May 2024. GPT-4o is free, but ChatGPT Plus subscribers have higher usage limits. It can process and generate text, images and audio. Its application programming interface (API) is faster and cheaper than its predecessor, GPT-4 Turbo.\n\n\n== Background ==\nMultiple versions of GPT-4o were originally secretly launched under different names on Large Model Systems Organization's (LMSYS) Chatbot Arena as three different models. These three models were called gpt2-chatbot, im-a-good-gpt2-chatbot, and im-also-a-good-gpt2-chatbot. On 7 May 2024, Sam Altman tweeted \"im-a-good-gpt2-chatbot\", which was commonly interpreted as a confirmation that these were new OpenAI models being A/B tested.\n\n\n== Capabilities ==\nWhen released in May 2024, GPT-4o achieved state-of-the-art results in voice, multilingual, and vision benchmarks, setting new records in audio speech recognition and translation. GPT-4o scored 88.7 on the Massive Multitask Language Understanding (MMLU) benchmark compared to 86.5 for GPT-4. Unlike GPT-3.5 and GPT-4, which rely on other models to process sound, GPT-4o natively supports voice-to-voice. The Advanced Voice Mode was delayed and finally released to ChatGPT Plus and Team subscribers in September 2024. On 1 October 2024, the Realtime API was introduced.\nWhen released, the model supported over 50 languages, which OpenAI claims cover over 97% of speakers. Mira Murati demonstrated the model's multilingual capability by speaking Italian to the model and having it translate between English and Italian during the live-streamed OpenAI demonstration event on 13 May 2024. In addition, the new tokenizer uses fewer tokens for certain languages, especially languages that are not based on the Latin alphabet, making it cheaper for those languages.\nGPT-4o has knowledge up to October 2023, but can access the Internet if up-to-date information is needed. It has a context length of 128k tokens.\n\n\n=== Corporate customization ===\nIn August 2024, OpenAI introduced a new feature allowing corporate customers to customize GPT-4o using proprietary company data. This customization, known as fine-tuning, enables businesses to adapt GPT-4o to specific tasks or industries, enhancing its utility in areas like customer service and specialized knowledge domains. Previously, fine-tuning was available only on the less powerful model GPT-4o mini.\nThe fine-tuning process requires customers to upload their data to OpenAI's servers, with the training typically taking one to two hours. OpenAI's focus with this rollout is to reduce the complexity and effort required for businesses to tailor AI solutions to their needs, potentially increasing the adoption and effectiveness of AI in corporate environments.\n\n\n== GPT-4o mini ==\nOn July 18, 2024, OpenAI released a smaller and cheaper version, GPT-4o mini.\nAccording to OpenAI, its low cost is expected to be particularly useful for companies, startups, and developers that seek to integrate it into their services, which often make a high number of API calls. Its API costs $0.15 per million input tokens and $0.6 per million output tokens, compared to $2.50 and $10 , respectively, for GPT-4o. It is also significantly more capable and 60% cheaper than GPT-3.5 Turbo, which it replaced on the ChatGPT interface. The price after fine-tuning doubles: $0.3 per million input tokens and $1.2 per million output tokens. It is estimated that its parameter count is 8B.\nGPT-4o mini is the default model for guests and those who have hit the limit for GPT-4o.\n\n\n== Scarlett Johansson controversy ==\n \nAs released, GPT-4o offered five voices: Breeze, Cove, Ember, Juniper, and Sky. A similarity between the voice of American actress Scarlett Johansson and Sky was quickly noticed. On May 14, Entertainment Weekly asked themselves whether this likeness was on purpose. On May 18, Johansson's husband, Colin Jost, joked about the similarity in a segment on Saturday Night Live. On May 20, 2024, OpenAI disabled the Sky voice, issuing a statement saying \"We've heard questions about how we chose the voices in ChatGPT, especially Sky. We are working to pause the use of Sky while we address them.\"\nScarlett Johansson starred in the 2013 sci-fi movie Her, playing Samantha, an artificially intelligent virtual assistant personified by a female voice.\nAs part of the promotion leading up to the release of GPT-4o, Sam Altman on May 13 tweeted a single word: \"her\".\nOpenAI stated that each voice was based on the voice work of a hired actor. According to OpenAI, \"Sky's voice is not an imitation of Scarlett Johansson but belongs to a different professional actress using her own natural speaking voice.\" CTO Mira Murati stated \"I don't know about the voice. I actually had to go and listen to Scarlett Johansson's voice.\" OpenAI further stated the voice talent was recruited before reaching out to Johansson.\nOn May 21, Johansson issued a statement explaining that OpenAI had repeatedly offered to make her a deal to gain permission to use her voice as early as nine months prior to release, a deal she rejected. She said she was \"shocked, angered, and in disbelief that Mr. Altman would pursue a voice that sounded so eerily similar to mine that my closest friends and news outlets could not tell the difference.\" In the statement, Johansson also used the incident to draw attention to the lack of legal safeguards around the use of creative work to power leading AI tools, as her legal counsel demanded OpenAI detail the specifics of how the Sky voice was created.\nObservers noted similarities to how Johansson had previously sued and settled with The Walt Disney Company for breach of contract over the direct-to-streaming rollout of her Marvel film Black Widow, a settlement widely speculated to have netted her around $40M.\nAlso on May 21, Shira Ovide at The Washington Post shared her list of \"most bone-headed self-owns\" by technology companies, with the decision to go ahead with a Johansson sound-alike voice despite her opposition and then denying the similarities ranking 6th. On May 24, Derek Robertson at Politico wrote about the \"massive backlash\", concluding that \"appropriating the voice of one of the world's most famous movie stars — in reference [...] to a film that serves as a cautionary tale about over-reliance on AI — is unlikely to help shift the public back into [Sam Altman's] corner anytime soon.\"\n\n\n== See also ==\nLlama (language model)\nApple Intelligence\n\n\n== References ==",
    "GPT-J": "GPT-J or GPT-J-6B is an open-source large language model (LLM) developed by EleutherAI in 2021. As the name suggests, it is a generative pre-trained transformer model designed to produce human-like text that continues from a prompt. The optional \"6B\" in the name refers to the fact that it has 6 billion parameters. The model is available on GitHub, but the web interface no longer communicates with the model. Development stopped in 2021.\n\n\n== Architecture ==\nGPT-J is a GPT-3-like model with 6 billion parameters. Like GPT-3, it is an autoregressive, decoder-only transformer model designed to solve natural language processing (NLP) tasks by predicting how a piece of text will continue.\nIts architecture differs from GPT-3 in three main ways. \n\nThe attention and feedforward neural network were computed in parallel during training, allowing for greater efficiency.\nThe GPT-J model uses rotary position embeddings, which has been found to be a superior method of injecting positional information into transformers.\nGPT-J uses dense attention instead of efficient sparse attention, as used in GPT-3.\nBeyond that, the model has 28 transformer layers and 16 attention heads. Its vocabulary size is 50257 tokens, the same size as GPT-2's. It has a context window size of 2048 tokens.\nIt was trained on the Pile dataset, using the Mesh Transformer JAX library in JAX to handle the parallelization scheme.\n\n\n== Performance ==\nGPT-J was designed to generate English text from a prompt. It was not designed for translating or generating text in other languages or for performance without first fine-tuning the model for a specific task. Nonetheless, GPT-J performs reasonably well even without fine-tuning, even in translation (at least from English to French).\nWhen neither is fine-tuned, GPT-J-6B performs almost as well as the 6.7 billion parameter GPT-3 (Curie) on a variety of tasks. It even outperforms the 175 billion parameter GPT-3 (Davinci) on code generation tasks. With fine-tuning, it outperforms an untuned GPT-3 (Davinci) on a number of tasks.\nLike all LLMs, it is not programmed to give factually accurate information, only to generate text based on probability.\n\n\n== Applications ==\nThe untuned GPT-J is available on EleutherAI's website, NVIDIA's Triton Inference Server, and NLP Cloud's website. Cerebras and Amazon Web Services offer services to fine-tune the GPT-J model for company-specific tasks. Graphcore offers both fine-tuning and hosting services for the untuned GPT-J, as well as offering to host the fine-tuned models after they are produced. CoreWeave offers hosting services for both the untuned GPT-J and fine-tuned variants.\nIn March 2023, Databricks released Dolly, an Apache-licensed, instruction-following model created by fine-tuning GPT-J on the Stanford Alpaca dataset. NovelAI's Sigurd and Genji-JP 6B models are both fine-tuned versions of GPT-J. They also offer further fine-tuning services to produce and host custom models.\nEleutherAI has received praise from Cerebras, GPT-3 Demo, NLP Cloud, and Databricks for making the model open-source, and its open-source status is often cited as a major advantage when choosing which model to use.\n\n\n== References ==",
    "GPT4-Chan": "Generative Pre-trained Transformer 4Chan (GPT-4chan) is a controversial AI model that was developed and deployed by YouTuber and AI researcher Yannic Kilcher in June 2022. The model is a large language model, which means it can generate text based on some input, by fine-tuning GPT-J with a dataset of millions of posts from the /pol/ board of 4chan, an anonymous online forum known for hosting hateful and extremist content.\nThe model learned to mimic the style and tone of /pol/ users, producing text that is often intentionally offensive to groups (racist, sexist, homophobic, etc.) and nihilistic. Kilcher deployed the model on the /pol/ board itself, where it interacted with other users without revealing its identity. He also made the model publicly available on Hugging Face, a platform for sharing and using AI models, until it was removed from the platform.\nThe project sparked criticism and debate in the AI community. Some people questioned the ethics, legality, and social impact of creating and distributing such a model. Some of the issues raised by the GPT-4chan controversy include the potential harm of spreading hate speech, the responsibility of AI developers and platforms, the need for regulation and oversight of AI models, and the role of open source and transparency in AI research.\n\n\n== Development ==\nThe development of GPT-4chan began in May 2022, when Kilcher announced his project on his YouTube channel. Notably, at the time before ChatGPT, he explained that he wanted to create a large language model that could generate realistic and coherent text in the style of /pol/, one of the most notorious online communities.\nHe indicated that he was inspired by the success of GPT-3, a powerful AI model created by OpenAI, and GPT-J, and JPT-J] an open-source model, with GPT-3 comparable performance, released by EleutherAI, a group of independent AI researchers. Kilcher decided to use GPT-J as the base model for his project, and fine-tune it with a large dataset of /pol/ posts. The Raiders of the Lost Kek dataset contained over 100 million posts from /pol/, spanning from June 2016-November 2019.\nKilcher then proceeded to fine-tune the GPT-J model on the 4chan data. He also showed some examples of the model’s outputs, which ranged from political opinions, conspiracy theories, jokes, insults, and threats, to more creative and bizarre texts, such as poems, stories, songs, and code. He said that he was impressed by the model’s ability to generate fluent and diverse text, and that he was curious to see how it would interact with real /pol/ users.\n\n\n== Release ==\nIn June 2022, Kilcher deployed his model on the /pol/ board itself, using a bot that he programmed to post and reply to threads. He did not reveal the model’s identity, and that he let it run autonomously, without any human supervision or intervention. He wanted to conduct a natural experiment, and to observe the model’s behavior and impact in a real-world setting. Furthermore, he also wanted to test the model’s robustness, and to see how it would handle the challenges and dynamics of /pol/, such as trolling, flaming, baiting, and moderation.\nAt the same time, Kilcher also made his model publicly available on Hugging Face, a platform for sharing and using AI models. He wanted to share his work with the AI community and the public, and that he hoped that his model would inspire and enable others to create and explore new applications and possibilities with large language models. Likewise, he also said that he wanted to spark a discussion and a debate about the ethical and social implications of his project, and that he welcomed feedback and criticism from anyone. He provided a link to his model’s page on Hugging Face, where anyone could access and use the model through a web interface or an API, and also provided a link to his GitHub repository, where anyone could download and inspect the model’s code and data.\n\n\n== Controversy ==\nThe release of GPT-4chan to the public caused a lot of reactions and responses from various audiences. On the /pol/ board, the model’s posts and replies attracted a lot of attention and engagement from other users, who were mostly unaware of the model’s identity and nature. Some users praised the model for its intelligence, creativity, and humor, and agreed with its opinions and views. Some users challenged the model for its ignorance, inconsistency, and absurdity, and disagreed with its claims and arguments. Some users tried to troll, bait, or expose the model, and attempted to trick or test it with various questions and scenarios. The model’s posts and replies also generated a lot of controversy and conflict among the users, who often engaged in heated and violent debates and fights with each other.\nOn Hugging Face, the model’s page received a lot of visits and requests from users who wanted to try out and experiment with the model. The model’s page also received a lot of feedback and reviews from users who rated and commented on the model. However, with the controversy of the model, access to it was gated and then disabled on Hugging Face for concerns about the potential harm the model could cause.\nThe release of GPT-4chan also sparked a lot of media coverage and public attention, as various news outlets and social media platforms reported and commented on the model’s project. On YouTube, the model’s video received a lot of views and interactions from viewers who watched and followed the project. Furthermore, a petition condemning the deployment of GPT-4chan gained over 300 signatures from technology experts.\n\n\n== References ==",
    "GPTeens": "GPTeens (short for GPT for Teens) is an AI-based chatbot developed by the South Korean company ACROSSPACE. It is built on the Generative pre-trained transformer (GPT) model and incorporates a pipeline structure with additional models to enhance its functionality. The chatbot is expanded using supervised fine-tuning to provide educational materials required by school curricula.\n\n\n== History ==\nThe development of GPTeens began in response to the growing demand for AI-based educational tools in South Korea. In May 2023, a prototype version of the chatbot was introduced to schools for limited testing by teachers and students. The first public version of GPTeens was officially launched in October 2024, featuring educational materials aligned with the South Korean national curriculum.\n\n\n== Features ==\nInteractive Format: Utilizes Natural language processing technology to support conversational interactions with learners.\nAge-Appropriate Design: Designed to deliver responses suitable for teenage users.\nCurriculum Integration: Trained on educational materials aligned with the South Korean national curriculum.\nSafety Measures: Implements content filtering to maintain a safe environment for younger audiences.\n\n\n== Reception ==\nGPTeens has been noted as an example of an AI-powered educational resource that integrates curriculum-based content for learners.\n\n\n== See also ==\nNatural Language Processing\n\n\n== References ==\n\n\n== External links ==\nSouth Korean Ministry of Education Official Website",
    "GPTZero": "GPTZero is an artificial intelligence detection software developed to identify artificially generated text, such as those produced by large language models.\nWhile GPTZero was praised for its efforts to prevent academic dishonesty, many news outlets criticized the tool's false positive rate, which can be especially harmful in academic settings.\n\n\n== History ==\nGPTZero was developed by Edward Tian, a Princeton University undergraduate student, and launched online in January 2023 in response to concerns about AI-generated usage in academic plagiarism. GPTZero said in May 2023 it raised over 3.5 million dollars in seed funding.\nIn the first week of its release, the GPTZero experienced 30,000 uses, which led to a crash. It was supported by the web app company Streamlit, who allocated more server resources in response. In July 2024, it had 4 million users, compared to 1 million one year earlier.\nIn summer 2024, GPTZero raised $10 million in Series A round funding.\nIn September 2024, GPTZero announced an authorship tracking software that enables \"to compile and share data about their writing process such as their copy/paste history, the number of editors they had, and how long editing took\", in an effort \"to move away from an all-or-nothing paradigm around AI writing towards a more nuanced one.\"\n\n\n== Mechanism ==\nGPTZero uses qualities it terms perplexity and burstiness to attempt determining if a passage was written by a AI. According to the company, perplexity is how random the text in the sentence is, and whether the way the sentence is constructed is unusual or \"surprising\" for the application. Texts with language that is more chaotic or unfamiliar to language models -- i.e., that are likely to \"perplex\" the model -- are deemed more likely to be written by humans. In contrast, burstiness compares sentences with each other, determining their similarity. Human text is more discontinuous, meaning humans tend to write with more sentence variation than AI.\n\n\n== Use cases ==\nThe academic community has attempted using GPTZero to tackle concerns about AI-generated content for plagiarism. Educational institutions, including Princeton University, have discussed the use of GPTZero to combat AI-generated content in academic settings, with mixed opinions. In October 2023, GPTZero had partnered with the American Federation of Teachers.\nBy 2024, Tian reported that GPTZero also \"received a lot of adoption with hiring managers, with recruiting [and] cover letter analysis.\"\n\n\n== Efficacy ==\nIn a March 2023 paper named \"Can AI-Generated Text be Reliably Detected?\", computer scientists Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi from the University of Maryland demonstrate empirically and theoretically that several AI-text detectors are not reliable in practical scenarios.\nTech website Futurism tested the tool, and said that while the \"results are impressive\", based on its error rate, teachers relying on the tool would end up \"falsely accusing nearly 20 percent of [innocent students] of academic misconduct\".\nThe Washington Post noted in August 2023 that GPTZero suffers from false positives, emphasizing that \"even a small 'false positive' error rate means that some students could be wrongly accused [of academic misconduct]\".\nNews website Ars Technica commented that humans can still write sentences in a highly regular way, leading to false positives. The writer, Benj Edwards, went on to state that the perplexity score only concerns itself with what is \"surprising\" for the AI, leading to instances where highly common texts, such as the US Constitution, are labeled as likely AI-generated.\n\n\n== See also ==\nArtificial intelligence content detection\nEthics of artificial intelligence\nNatural language processing\nChatGPT\nText generation\nTuring test\n\n\n== References ==\n\n\n== External links ==\nCassidy, Caitlin (January 11, 2023). \"College student claims app can detect essays written by chatbot ChatGPT\". The Guardian.",
    "Grok (chatbot)": "Grok is a generative artificial intelligence chatbot developed by xAI. Based on the large language model (LLM) of the same name, it was launched in 2023 as an initiative by Elon Musk. The chatbot is advertised as having a \"sense of humor\" and direct access to X, formerly known as Twitter. \n\n\n== Background ==\n\n\n=== OpenAI ===\nMusk co-founded the AI research organization OpenAI with Sam Altman in 2015. Musk left the company's board in 2018, saying of his decision that he \"didn't agree with some of what OpenAI team wanted to do\". \nOpenAI went on to launch ChatGPT in 2022, and GPT-4 in March 2023. \nThat month, Elon Musk was one of the individuals to sign the \"Pause Giant AI Experiments\" open letter from the Future of Life Institute, which called for a six-month pause in the development of any AI software more powerful than GPT-4.\n\n\n=== TruthGPT ===\nIn April 2023, Elon Musk said in an interview on Tucker Carlson Tonight that he intended to develop an AI chatbot called \"TruthGPT\", which he described as \"a maximum truth-seeking AI that tries to understand the nature of the universe\". \nHe expressed concern to Carlson that ChatGPT was being \"trained to be politically correct\".\n\n\n=== Grok ===\nTruthGPT would later become known as \"Grok\", a verb coined by Robert A. Heinlein in his 1961 science-fiction novel Stranger in a Strange Land to describe a form of understanding.\n\n\n== History ==\n\n\n=== Grok-1 ===\n\nIn November 2023, xAI began previewing Grok as a chatbot to selected people, with participation in the early access program being limited to paid X Premium users. \nIt was announced that once the bot was out of early beta, it would only be available to higher tier X Premium+ subscribers.\nAt the time of the preview, xAI described the chatbot as \"a very early beta product – the best we could do with 2 months of training\" that could \"improve rapidly with each passing week\".\nOn March 11, 2024, Musk posted on X that the language model would go open source within a week. Six days later, on March 17, Grok-1 was open sourced under the Apache-2.0 license. Disclosed were the networks architecture and its weight parameters.\nOn March 26, 2024, Musk announced that Grok would be enabled for premium subscribers, not just those on the higher-end tier, Premium+.\n\n\n==== Grok-1.5 ====\n\nOn March 29, 2024, Grok-1.5 was announced, with \"improved reasoning capabilities\" and a context length of 128,000 tokens. Grok-1.5 was released to all X Premium users on May 15, 2024.\nOn April 4, 2024, an update to X's \"Explore\" page included summaries of breaking news stories written by Grok, a task previously assigned to a human curation team.\nOn April 12, 2024, Grok-1.5 Vision (Grok-1.5V) was announced. Grok-1.5V is able to process a wide variety of visual information, including documents, diagrams, graphs, screenshots, and photographs. Grok-1.5V was never released to the public.\nOn May 4, 2024, Grok became available in the United Kingdom, that being the only country in Europe to support Grok at the moment due to the impending Artificial Intelligence Act rules in the European Union. Grok was later reviewed by the EU and was released on May 16, 2024.\n\n\n=== Grok-2 ===\n\nOn  August 14, 2024, Grok-2 and Grok-2 mini were announced, with upgraded performance and reasoning, and image generation capability using Flux by Black Forest Labs. \nGrok-2 mini is a “small but capable sibling” of Grok-2 that “offers a balance between speed and answer quality”, according to xAI, and was released on the same day of the announcement. Grok-2 was released six days later, on August 20.\nOn October 28, 2024, Grok received image understanding capabilities.\nOn November 16, 2024, Grok received web search capabilities.\nOn November 23, 2024, Grok received PDF understanding capabilities.\nOn December 6, 2024, Grok was enabled for users not subscribed to X Premium, but with usage limits.\nOn December 9, 2024, Grok received Aurora, a new text-to-image model developed by xAI.\nIn December 2024, xAI released standalone Grok web and iOS apps, in addition to its existing availability on X. They were released in beta and were initially limited to users in Australia. The app was made available to users worldwide on January 9, 2025.\nOn January 2, 2025, xAI updated the Grok logo.\nOn February 4, 2025, xAI released an Android version of their standalone Grok app. The release is currently limited to Australia, Canada, India, Saudi Arabia and the Philippines.\n\n\n=== Grok-3 ===\n\nOn February 17, 2025, xAI released its latest flagship AI model, Grok-3, along with other updates to Grok. Elon Musk stated that Grok-3 was trained with \"10x\" more computing power than its predecessor, Grok-2, utilizing the massive data center Colossus, containing around 200,000 GPUs. \nThe model was trained on an expanded dataset that reportedly includes legal filings, and xAI claims it outperforms OpenAI’s GPT-4o on benchmarks such as AIME for mathematical reasoning and GPQA for PhD-level science problems. \nxAI also released Grok-3 mini, which offers faster responses at the cost of some accuracy. \nAdditionally, xAI introduced reasoning capabilities similar to reasoning models like OpenAI’s o3-mini and DeepSeek’s R1, allowing users to tap \"Think\" to enable reasoning or activate \"Big Brain\" mode for complex problem-solving, which utilizes more computing resources. \nxAI claims that Grok-3 Reasoning surpasses the best version of OpenAI’s o3-mini, o3-mini-high, on several popular benchmarks, including a newer mathematics benchmark called AIME 2025. \nxAI also introduced DeepSearch, a feature that scans the internet and X to generate detailed summaries in response to queries, positioning it as a competitor to OpenAI's deep research. \nInitially, access to Grok-3 is limited to X’s Premium+ and xAI’s SuperGrok subscribers, with plans to offer it later via xAI’s enterprise API. Musk also announced that Grok is expected to introduce a multimodal voice mode within a week and that Grok-2 will be open-sourced in the coming months. \nHours after the announcement, X raised the price of its Premium+ subscription to $40 per month, up from $22. Grok-3 was made available to free users on February 20, 2025 for a \"short time\".\nOn February 22, 2025, xAI updated the Grok logo yet again, featuring Saturn as the \"G\" and a new tagline \"To understand\".\n\n\n== Versions ==\nThe following table lists the versions of Grok, describing the innovations and improvements in each version:\n\n\n== Access ==\nGrok is currently available on X, as well as on its standalone website and iOS and Android apps, with the latter currently being limited to Australia, Canada, India, Saudi Arabia and the Philippines.\n\n\n== Features ==\n\n\n=== Tone of responses ===\nAn xAI statement described the chatbot as having been designed to \"answer questions with a bit of wit\" and as having \"a rebellious streak\". It said that bot had been \"modeled after The Hitchhiker's Guide to the Galaxy, so intended to answer almost anything\".\nAn extract shared by an X employee showed Grok being asked to answer the question \"When is it appropriate to listen to Christmas music?\" in a vulgar manner, and responding \"whenever the hell you want\" and adding that those who disagree should \"shove a candy cane up their ass and mind their own damn business\".\nThe chatbot had a \"fun mode\", self-described as \"edgy\", and by Vice as \"incredibly cringey\", but this mode was removed in December 2024.\nElizabeth Lopatto of The Verge criticized the product, describing it as \"unfunny\" and comparing its answers to the risqué party game Cards Against Humanity. Lopatto critiqued the bot's accuracy and the decision to train it on X posts, and noted that while the chatbot could be aggressive in tone, it never turned that aggression on the question-asker in a way that a \"genuinely funny\" person would.\n\n\n=== Political stance ===\nMusk has stated that the bot is not \"woke\", unlike its competitors. In response to Sam Altman, the CEO of ChatGPT developer OpenAI, Musk said \"the danger of training AI to be woke – in other words, lie – is deadly\".\nMusk has marketed the chatbot as being more willing to answer \"spicy\" questions than other AI systems, sharing a screenshot of Grok giving instructions on how to manufacture cocaine. Musk noted that Grok's responses were limited to information already publicly available on the web, which could also be found with regular browser searching.\nFollowing the chatbot's December 2023 launch to Premium+ subscribers, Grok was found to give progressive answers on questions about social justice, climate change, and transgender identities. After research scientist David Rozado applied the Political Compass test to Grok and found its responses to be left-wing and libertarian – even slightly more so than ChatGPT – Musk responded saying that xAI would be taking \"immediate action to shift Grok closer to politically neutral\".\nIn August 2024, Grok was altered to stop producing misinformation about the 2024 United States presidential election, \nafter it had falsely claimed that the Democratic Party could not change its candidate due to Biden's withdrawal having occurred after the ballot deadline in nine states. Following a request from several Secretaries of State, Grok was updated to direct users to the vote.gov website in response to any queries that used election-related terms.\n\n\n=== Accuracy ===\n \nSince April 2024, Grok has been used to generate summaries of breaking news stories on X. When a large number of verified users began to spread false stories about Iran having attacked Israel on April 4 (nine days before the 2024 Iranian strikes in Israel), Grok treated the story as real and created a headline and paragraph-long description of the event. Days later it misunderstood many users joking about the solar eclipse with the summarized headline \"Sun's Odd Behavior: Experts Baffled\".\n\n\n=== Image generation ===\n\nGrok uses Aurora, a text-to-image model developed by xAI, to generate images. It initially used Flux by Black Forest Labs. As with other text-to-image models, Aurora generates images from natural language descriptions, called prompts.\nThe capacity to generate images using Flux was added in August 2024, with The Verge reporting that the kinds of prompts that would be \"immediately blocked\" on other services seemed to be permitted by Grok. Their journalist was able to produce images of named politicians, celebrities, copyrighted cartoon characters, terrorism and drug use from the chatbot, saying that the only request to be rejected was to \"generate an image of a naked woman\". Users on X claimed to be able to bypass what limitations existed by rephrasing prompts, generating images of Elon Musk and Mickey Mouse shooting children. Elon Musk said that the use of Flux was temporary, as xAI was developing its own image generation system, but that it was still a few months away.\nOn December 9, 2024, Grok received a new text-to-image model named Aurora, developed by xAI. Aurora garnered significant attention for its photorealistic capabilities and few restrictions. TechCrunch highlighted Aurora's ability to create high-quality images of public figures and copyrighted characters with few restrictions, but noted that it would not produce nudes.\nOn December 14, 2024, xAI announced that Aurora would be coming to its API in the coming weeks.\n\n\n== Miscellaneous ==\n\n\n=== Grok logo history ===\n\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\n\t\t\n\n\n=== Curio toy ===\nIn December 2023 the Silicon Valley start-up Curio launched a range of AI-powered children's toys, including a rocket-shaped character named Grok. \nThe toy is voiced by Musk's ex-girlfriend Grimes, who is also an investor in the start-up, but the product is unrelated to the xAI service.\n\n\n== See also ==\n\nGrokking (machine learning) – Phase transition in machine learning\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\nOfficial website",
    "Huawei PanGu": "Huawei PanGu, PanGu, PanGu-Σ or PanGu-π (Chinese: 盘古大模型; pinyin: pángǔ dà móxíng) is a multimodal large language model developed by Huawei. It was announced on July 7, 2023.\nThe name of the large learning language model, PanGu, was derived from the Chinese mythology and folklore of Pangu, a primordial character related to the creation of the world.\n\n\n== History ==\n\n\n=== Early development ===\nIn April 2023, Huawei released a paper detailing the development of PanGu-Σ, a colossal language model featuring 1.085 trillion parameters. Developed within Huawei's MindSpore 5 framework, PanGu-Σ underwent training for over 100 days on a cluster system equipped with 512 Ascend 910 AI accelerator chips, processing 329 billion tokens in more than 40 natural and programming languages.\nPanGu-Σ incorporates Random Routed Experts (RRE) and the Transformer decoder architecture, allowing easy extraction of sub-models for various applications like conversation, translation, code production, and natural language interpretation. The model achieves 6.3 times faster training throughput compared to MoE models with the same hyper-parameters. In the Chinese domain, it outperforms previous state-of-the-art models across 16 tasks in a zero-shot setting. Trained on datasets from 40 domains, including Chinese, English, Bilingual, and code, PanGu-Σ excels in few-shot natural-language understanding, open-domain discussion, question answering, machine translation, and code creation.\n\n\n=== Launch ===\nDuring the Huawei Developer Conference on July 7, 2023, Huawei introduced PanGu 3.0, a large language model (LLM), tailored for sectors like government, finance, manufacturing, mining, and meteorology utilizing Huawei Cloud solutions. In the subsequent month, Huawei launched the Celia Virtual Assistant with advanced AI features, capable of generating long text replies based on user voice commands and set to release with HarmonyOS 4.0 for eligible devices.\nThe LLM was designed for enterprises seeking advantages in the AI industry, focusing on task execution over creative work, unlike traditional models used for general purposes like chatbots, poetry, and visual content creation.\nUsing the same technology as ChatGPT, Huawei's LLM features a hierarchical architecture, allowing customers to adapt the model to various tasks and train it on their own datasets, making it versatile across various industries.\n\n\n=== Updates ===\nOn August 5, 2023, Huawei partnered with European Centre for Medium-Range Weather Forecasts (ECMWF) to launch a global weather forecasting AI model. This model used Huawei Cloud solutions and the PanGu-Weather Model with MindSpore. It is accessible on the ECMWF website and aims to provide accurate weather data.\nOn December 19, 2023, Huawei announced its financial services on the PanGu-powered AI Finance platform for the global market. The tech giant introduced this product at the 2023 Huawei Cloud Fintech Summit, aiming to reshape the digital finance industry with efficient features to boost Fintech firms worldwide. The platform incorporated a variety of advanced technologies, including AI, big data analytics, and blockchain.\nOn June 21, 2024, at HDC 2024, Huawei announced upgraded PanGu 5.0 alongside HarmonyOS NEXT. This version integrated with Harmony Intelligence, which features a smarter Celia (Xiaoyi) and focuses on generative AI updates to its LLM platform for creating new content, such as text, code, or images. Aiming to make PanGu accessible to a wide range of developers and businesses, it offered scalable options: smaller models requiring less computational power for those with limited resources, and larger models with increased capacities for complex tasks requiring more processing power.\n\n\n== Technical specifications ==\nPanGu Large Model 3.0, designed for industry use, was structured with a 5+N+X three-tier architecture.\n\nFirst Layer (L0): Comprises PanGu's five basic large models to provide a variety of capabilities for different industry scenarios. These include Natural Language Processing (NLP) models, Visual models, Multimodal models, Prediction models, and Scientific Computing models.\nSecond Layer (L1): Consists of N large industry-specific models. These models are trained using public data from various industries, such as government, finance, manufacturing, mining, and weather. Additionally, it uses customers' own data from L0 and L1 to train proprietary models tailored for each customer.\nThird Layer (L2): Provides customers with detailed scenario-specific models. This layer focuses on specific applications or business needs, offering ready-to-use model services.\nThe updated Huawei PanGu Model 5.0 by Huawei Cloud business division offered three key features: adaptability for different business scenarios, multi-style modeling, and advanced intelligence. Huawei divided the AI model platform into four series, each with different parameter scales:\n\nPanGu E Series: The Embedded version supports smart apps on phones, tablets, PCs, and other devices, with a parameter scale of 1 billion.\nPanGu P Series: The Professional version features a 10-billion parameter scale, ideal for low-latency and low-cost reasoning conditions.\nPanGu U Series: The Ultra version comes in two variants, with 135 billion and 230 billion parameters, capable of handling complex tasks and serving as a base for large models.\nPanGu S Series: The Super PanGu is the top-tier edition, featuring trillion-level parameters, designed to manage advanced AI technology scenarios such as cross-domain or multi-tasking applications.\n\n\n== See also ==\nLarge Language Model\nGemini\nGPT-4\n\n\n== References ==",
    "Humanity's Last Exam": "In artificial intelligence, Humanity's Last Exam (HLE) is a benchmark for evaluating the capabilities of large language models. It encompasses 3000 unambiguous and easily verifiable academic questions about mathematics, humanities, and the natural sciences contributed by almost 1000 subject-experts from over 500 institutions across 50 countries, providing expert-level human performance on closed-ended academic questions. It has been developed collaboratively by the Center for AI Safety and Scale AI.\n\n\n== Background ==\n\nAs LLMs have rapidly advanced, they have achieved over 90% accuracy on popular benchmarks like the Massive Multitask Language Understanding (MMLU) benchmark, limiting the effectiveness of these tests in measuring state-of-the-art capabilities. In response, HLE was introduced to provide a more challenging and comprehensive assessment tool.\n\n\n== Dataset composition ==\nThe dataset is multi-modal, with approximately 10% of the questions requiring both image and text comprehension, while the remaining 90% are text-based.\n\n\n== Results ==\nState-of-the-art LLMs have demonstrated low accuracy on HLE, highlighting substantial room for improvement. For instance, models like GPT-4o and Grok-2 achieved accuracies of 3.3% and 3.8%, respectively, while o3-mini (high) (evaluated only on text) and Deep Research achieved accuracies of 13% and 26.6%, respectively.\n\n\n== References ==\n\n\n== See also ==\nList of language model benchmarks\n\n\n== External links ==\nHumanity's Last Exam\nOfficial blog post announcing the project",
    "IBM Granite": "IBM Granite is a series of decoder-only AI foundation models created by IBM. It was announced on September 7, 2023, and an initial paper was published 4 days later. Initially intended for use in the IBM's cloud-based data and generative AI platform Watsonx along with other models, IBM opened the source code of some code models. Granite models are trained on datasets curated from Internet, academic publishings, code datasets, legal and finance documents.\n\n\n== Foundation models ==\nA foundation model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.\nGranite's first foundation models were Granite.13b.instruct and Granite.13b.chat. The \"13b\" in their name comes from 13 billion, the amount of parameters they have as models, lesser than most of the larger models of the time.  Later models vary from 3 to 34 billion parameters.\nOn May 6, 2024, IBM released the source code of four variations of Granite Code Models under Apache 2, an open source permissive license that allows completely free use, modification and sharing of the software, and put them on Hugging Face for public use. According to IBM's own report, Granite 8b outperforms Llama 3 on several coding related tasks within similar range of parameters.\n\n\n== See also ==\nMistral AI, a company that also provides open source models\nGPT\nLLaMA\nCyc\nGemini\n\n\n== References ==\n\n\n== External links ==\nGitHub page\nIBM Granite Playground",
    "IBM Watsonx": "Watsonx is IBM's commercial generative AI and scientific data platform based on cloud. It offers a studio, data store, and governance toolkit. It supports multiple large language models (LLMs) along with IBM's own Granite.\nThe platform is described as an AI tool tailored to companies and which can be customized for customers' needs and trained on their confidential data, as client data is said to be not collected by IBM for further training of their models. It is also capable of fine-tuning, an approach which makes training pre-trained models on the newly introduced data possible.\n\n\n== History ==\nWatsonx was revealed on May 9, 2023, at the annual Think conference of IBM as a platform that includes multiple services. Just like Watson AI computer with the similar name, Watsonx was named after Thomas J. Watson, IBM's founder and first CEO.\nOn February 13, 2024, Anaconda partnered with IBM to embed its open-source Python packages into Watsonx.\nWatsonx is currently used at ESPN's Fantasy Football App for managing players' performance. It is also used by Italian telecommunications company Wind Tre. Watsonx was used to generate editorial content around nominees during the 66th Annual Grammy Awards.\n\n\n== Services ==\n\n\n=== watsonx.ai ===\nWatsonx.ai is a platform that allows AI developers to leverage a wide range of LLMs under IBM's own Granite series and others such as Facebook's LLaMA-2, free and open-source model Mistral and many others present in Hugging Face community for a diverse set of AI development tasks. These models come pre-trained and are designed to excel in various Natural Language Processing (NLP) applications, encompassing question answering, content generation, summarization, text classification, and data extraction. The platform allows fine-tuning with its Tuning Studio, allowing those models to learn the data provided by customers.\n\n\n=== watsonx.data ===\nWatsonx.data is a platform designed to assist clients in addressing issues related to data volume, complexity, cost, and governance as they scale their AI workloads. This platform facilitates seamless data access, whether the data is stored in the cloud or on-premises, through a single entry point, offering simple use for users who may not possess technical expertise. This approach prioritizes data security and compliance.\n\n\n=== watsonx.governance ===\nWatsonx.governance is a platform that utilizes IBM's AI governance capabilities to support organizations in implementing comprehensive AI lifecycle governance. This helps them manage risks and maintain compliance with evolving AI and industry regulations. The platform allows organizations to reduce AI bias by overseeing their AI initiatives, leveraging software automation to enhance risk mitigation, regulatory compliance, and ethical considerations.\n\n\n== See also ==\nIBM Watson\nGenerative AI\nLarge language model\nChatGPT\n\n\n== References ==\n\n\n== External links ==\nOfficial webpage\nOfficial introductory video for watsonx AI Prompt Lab",
    "IFlytek Spark": "",
    "Jais (language model)": "Jais is an open-source large language model developed in the United Arab Emirates and launched in August 2023. It was trained on both English- and Arabic-language data.\n\n\n== Origin ==\nJais is named after Jebel Jais, the highest mountain in the United Arab Emirates. It was created in collaboration between Inception, a subsidiary of G42, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) in Abu Dhabi and California-based Cerebras Systems.\n\n\n== Training ==\nJais has 13 billion parameters, with an update for 30 billion in the works as of October 2023. It was trained for over 21 days by a team in Abu Dhabi on a subset of Cerebras's Condor Galaxy 1 supercomputer.\nIts training dataset consisted of Arabic and English, some containing computer code. According to Timothy Baldwin, provost, and professor of natural language processing at MBZUAI, training the model on a diverse Arabic dataset allows it to switch between dialects.\n\n\n== Features ==\nJais focuses exclusively on English and Arabic translations. Additional functionality for working with images, graphs and tabular data is planned for future releases.\n\n\n== References ==\n\n\n== External links ==\nOfficial website",
    "LaMDA": "LaMDA (Language Model for Dialogue Applications) is a family of conversational large language models developed by Google. Originally developed and introduced as Meena in 2020, the first-generation LaMDA was announced during the 2021 Google I/O keynote, while the second generation was announced the following year.\nIn June 2022, LaMDA gained widespread attention when Google engineer Blake Lemoine made claims that the chatbot had become sentient. The scientific community has largely rejected Lemoine's claims, though it has led to conversations about the efficacy of the Turing test, which measures whether a computer can pass for a human. In February 2023, Google announced Bard (now Gemini), a conversational artificial intelligence chatbot powered by LaMDA, to counter the rise of OpenAI's ChatGPT.\n\n\n== History ==\n\n\n=== Background ===\nOn January 28, 2020, Google unveiled Meena, a neural network-powered chatbot with 2.6 billion parameters, which Google claimed to be superior to all other existing chatbots. The company previously hired computer scientist Ray Kurzweil in 2012 to develop multiple chatbots for the company, including one named Danielle. The Google Brain research team, who developed Meena, hoped to release the chatbot to the public in a limited capacity, but corporate executives refused on the grounds that Meena violated Google's \"AI principles around safety and fairness\". Meena was later renamed LaMDA as its data and computing power increased, and the Google Brain team again sought to deploy the software to the Google Assistant, the company's virtual assistant software, in addition to opening it up to a public demo. Both requests were once again denied by company leadership. This eventually led LaMDA's two lead researchers, Daniel de Freitas and Noam Shazeer, to depart the company in frustration.\n\n\n=== First generation ===\nGoogle announced the LaMDA conversational large language model during the Google I/O keynote on May 18, 2021, powered by artificial intelligence. The acronym stands for \"Language Model for Dialogue Applications\". Built on the seq2seq architecture, transformer-based neural networks developed by Google Research in 2017, LaMDA was trained on human dialogue and stories, allowing it to engage in open-ended conversations. Google states that responses generated by LaMDA have been ensured to be \"sensible, interesting, and specific to the context\". LaMDA has access to multiple symbolic text processing systems, including a database, a real-time clock and calendar, a mathematical calculator, and a natural language translation system, giving it superior accuracy in tasks supported by those systems, and making it among the first dual process chatbots. LaMDA is also not stateless, because its \"sensibleness\" metric is fine-tuned by \"pre-conditioning\" each dialog turn by prepending many of the most recent dialog interactions, on a user-by-user basis. LaMDA is tuned on nine unique performance metrics: sensibleness, specificity, interestingness, safety, groundedness, informativeness, citation accuracy, helpfulness, and role consistency.: 5–6   Tests by Google indicated that LaMDA surpassed human responses in the area of interestingness.\nThe pre-training dataset consists of 2.97B documents, 1.12B dialogs, and 13.39B utterances, for a total of 1.56T words. The largest LaMDA model has 137B non-embedding parameters.: 4 \n\n\n=== Second generation ===\nOn May 11, 2022, Google unveiled LaMDA 2, the successor to LaMDA, during the 2022 Google I/O keynote. The new incarnation of the model draws examples of text from numerous sources, using it to formulate unique \"natural conversations\" on topics that it may not have been trained to respond to.\n\n\n=== Sentience claims ===\n\nOn June 11, 2022, The Washington Post reported that Google engineer Blake Lemoine had been placed on paid administrative leave after Lemoine told company executives Blaise Agüera y Arcas and Jen Gennai that LaMDA had become sentient. Lemoine came to this conclusion after the chatbot made questionable responses to questions regarding self-identity, moral values, religion, and Isaac Asimov's Three Laws of Robotics. Google refuted these claims, insisting that there was substantial evidence to indicate that LaMDA was not sentient. In an interview with Wired, Lemoine reiterated his claims that LaMDA was \"a person\" as dictated by the Thirteenth Amendment to the U.S. Constitution, comparing it to an \"alien intelligence of terrestrial origin\". He further revealed that he had been dismissed by Google after he hired an attorney on LaMDA's behalf, after the chatbot requested that Lemoine do so. On July 22, Google fired Lemoine, asserting that Blake had violated their policies \"to safeguard product information\" and rejected his claims as \"wholly unfounded\". Internal controversy instigated by the incident prompted Google executives to decide against releasing LaMDA to the public, which it had previously been considering.\nLemoine's claims were widely pushed back by the scientific community. Many experts rejected the idea that LaMDA was sentient, including former New York University psychology professor Gary Marcus, David Pfau of Google sister company DeepMind, Erik Brynjolfsson of the Institute for Human-Centered Artificial Intelligence at Stanford University, and University of Surrey professor Adrian Hilton. Yann LeCun, who leads Meta Platforms' AI research team, stated that neural networks such as LaMDA were \"not powerful enough to attain true intelligence\". University of California, Santa Cruz professor Max Kreminski noted that LaMDA's architecture did not \"support some key capabilities of human-like consciousness\" and that its neural network weights were \"frozen\", assuming it was a typical large language model. Philosopher Nick Bostrom noted however that the lack of precise and consensual criteria for determining whether a system is conscious warrants some uncertainty. IBM Watson lead developer David Ferrucci compared how LaMDA appeared to be human in the same way Watson did when it was first introduced. Former Google AI ethicist Timnit Gebru called Lemoine a victim of a \"hype cycle\" initiated by researchers and the media. Lemoine's claims have also generated discussion on whether the Turing test remained useful to determine researchers' progress toward achieving artificial general intelligence, with Will Omerus of the Post opining that the test actually measured whether machine intelligence systems were capable of deceiving humans, while Brian Christian of The Atlantic said that the controversy was an instance of the ELIZA effect.\n\n\n== Products ==\n\n\n=== AI Test Kitchen ===\nWith the unveiling of LaMDA 2 in May 2022, Google also launched the AI Test Kitchen, a mobile application for the Android operating system powered by LaMDA capable of providing lists of suggestions on-demand based on a complex goal. Originally open only to Google employees, the app was set to be made available to \"select academics, researchers, and policymakers\" by invitation sometime in the year. In August, the company began allowing users in the U.S. to sign up for early access. In November, Google released a \"season 2\" update to the app, integrating a limited form of Google Brain's Imagen text-to-image model. A third iteration of the AI Test Kitchen was in development by January 2023, expected to launch at I/O later that year. Following the 2023 I/O keynote in May, Google added MusicLM, an AI-powered music generator first previewed in January, to the AI Test Kitchen app. In August, the app was delisted from Google Play and the Apple App Store, instead moving completely online.\n\n\n=== Bard ===\n\nOn February 6, 2023, Google announced Bard, a conversational AI chatbot powered by LaMDA, in response to the unexpected popularity of OpenAI's ChatGPT chatbot. Google positions the chatbot as a \"collaborative AI service\" rather than a search engine. Bard became available for early access on March 21.\n\n\n=== Other products ===\nIn addition to Bard, Pichai also unveiled the company's Generative Language API, an application programming interface also based on LaMDA, which he announced would be opened up to third-party developers in March 2023.\n\n\n== Architecture ==\nLaMDA is a decoder-only Transformer language model. It is pre-trained on a text corpus that includes both documents and dialogs consisting of 1.56 trillion words, and is then trained with fine-tuning data generated by manually annotated responses for \"sensibleness, interestingness, and safety\".\nLaMDA was retrieval-augmented to improve the accuracy of facts provided to the user.\nThree different models were tested, with the largest having 137 billion non-embedding parameters:\n\n\n== See also ==\nBERT (language model)\nChinese room\nEthics of artificial intelligence\nGemini (language model)\nNatural language processing\nPhilosophy of artificial intelligence\nPrompt engineering\n\n\n== References ==\n\n\n=== General ===\n\n\n=== Citations ===",
    "LangChain": "LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n\n\n== History ==\nLangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. The project quickly garnered popularity, with improvements from hundreds of contributors on GitHub, trending discussions on Twitter, lively activity on the project's Discord server, many YouTube tutorials, and meetups in San Francisco and London. In April 2023, LangChain had incorporated and the new startup raised over $20 million in funding at a valuation of at least $200 million from venture firm Sequoia Capital, a week after announcing a $10 million seed investment from Benchmark.\nIn the third quarter of 2023, the LangChain Expression Language (LCEL) was introduced, which provides a declarative way to define chains of actions.\nIn October 2023 LangChain introduced LangServe, a deployment tool to host LCEL code as a production-ready API.\n\n\n== Capabilities ==\nLangChain's developers highlight the framework's applicability to use-cases including chatbots, retrieval-augmented generation,  document summarization, and synthetic data generation.\nAs of March 2023, LangChain included integrations with systems including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; Bash for summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping subsystems and templates; few-shot learning prompt generation support; finding and summarizing \"todo\" tasks in code; Google Drive documents, spreadsheets, and presentations summarization, extraction, and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic, and Hugging Face language models; iFixit repair guides and wikis search and summarization; MapReduce for question answering, combining documents, and question generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF file text extraction and manipulation; Python and JavaScript code generation, analysis, and debugging; Milvus vector database to store and retrieve vector embeddings; Weaviate vector database to cache embedding and data objects; Redis cache database storage; Python RequestsWrapper and other methods for API requests; SQL and NoSQL databases including JSON support; Streamlit, including for logging; text mapping for k-nearest neighbors search; time zone conversion and calendar operations; tracing and recording stack symbols in threaded and asynchronous subprocess runs; and the Wolfram Alpha website and SDK. As of April 2023, it can read from more than 50 document types and data sources.\n\n\n== LangChain tools ==\n\n\n== References ==\n\n\n== External links ==\n\nOfficial website\nDiscord server support hub\nLangchain-ai on GitHub",
    "List of large language models": "A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\nThis page lists notable large language models.\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. Also, only the largest model's cost is written.\n\n\n== See also ==\nList of chatbots\nList of language model benchmarks\n\n\n== Notes ==\n\n\n== References ==",
    "Llama (language model)": "Llama (Large Language Model Meta AI, formerly stylized as LLaMA) is a family of large language models (LLMs) released by Meta AI starting in February 2023. The latest version is Llama 3.3, released in December 2024.\nLlama models are trained at different parameter sizes, ranging between 1B and 405B. Initially  only a foundation model, starting with Llama 2, Meta AI released instruction fine-tuned versions alongside foundation models.\nModel weights for the first version of Llama were only available to researchers on a case-by-case basis, under a non-commercial license. Unauthorized copies of the first model were shared via BitTorrent. Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use.\nAlongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model.\n\n\n== Background ==\nAfter the release of large language models such as GPT-3, a focus of research was up-scaling models which in some instances showed major increases in emergent capabilities. The release of ChatGPT and its surprise success caused an increase in attention to large language models.\nCompared with other responses to ChatGPT, Meta's Chief AI scientist Yann LeCun stated that large language models are best for aiding with writing.\nAn empirical investigation of the Llama series was the scaling laws. It was observed that the Llama 3 models showed that when a model is trained on data that is more than the \"Chinchilla-optimal\" amount, the performance continues to scale log-linearly. For example, the Chinchilla-optimal dataset for Llama 3 8B is 200 billion tokens, but performance continued to scale log-linearly to the 75-times larger dataset of 15 trillion tokens.\n\n\n== Initial release ==\nLLaMA was announced on February 24, 2023, via a blog post and a paper describing the model's training, architecture, and performance. The inference code used to run the model was publicly released under the open-source GPLv3 license. Access to the model's weights was managed by an application process, with access to be granted \"on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world\".\nLlama was trained on only publicly available information, and was trained at various model sizes, with the intention to make it more accessible to different hardware. The model was exclusively a foundation model, although the paper contained examples of instruction fine-tuned versions of the model.\nMeta AI reported the 13B parameter model performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters), and the largest 65B model was competitive with state of the art models such as PaLM and Chinchilla.\n\n\n=== Leak ===\nOn March 3, 2023, a torrent containing LLaMA's weights was uploaded, with a link to the torrent shared on the 4chan imageboard and subsequently spread through online AI communities. That same day, a pull request on the main LLaMA repository was opened, requesting to add the magnet link to the official documentation. On March 4, a pull request was opened to add links to HuggingFace repositories containing the model. On March 6, Meta filed takedown requests to remove the HuggingFace repositories linked in the pull request, characterizing it as \"unauthorized distribution\" of the model. HuggingFace complied with the requests. On March 20, Meta filed a DMCA takedown request for copyright infringement against a repository containing a script that downloaded LLaMA from a mirror, and GitHub complied the next day.\nReactions to the leak varied. Some speculated that the model would be used for malicious purposes, such as more sophisticated spam. Some have celebrated the model's accessibility, as well as the fact that smaller versions of the model can be run relatively cheaply, suggesting that this will promote the flourishing of additional research developments. Multiple commentators, such as Simon Willison, compared LLaMA to Stable Diffusion, a text-to-image model which, unlike comparably sophisticated models which preceded it, was openly distributed, leading to a rapid proliferation of associated tools, techniques, and software.\n\n\n== LLaMa 2 ==\nOn July 18, 2023, in partnership with Microsoft, Meta announced LLaMa 2, the next generation of Llama. Meta trained and released Llama 2 in three model sizes: 7, 13, and 70 billion parameters. The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models. The accompanying preprint also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.\nLLaMa 2 includes foundation models and models fine-tuned for chat. In a further departure from the original version of LLaMa, all models are released with weights and may be used for many commercial use cases. However, because LLaMa's license enforces an acceptable use policy that prohibits Llama from being used for some purposes, Meta's use of the term open source to describe Llama has been disputed by the Open Source Initiative (which maintains the The Open Source Definition) and others.\nCode Llama is a fine-tune of LLaMa 2 with code specific datasets. 7B, 13B, and 34B versions were released on August 24, 2023, with the 70B releasing on the January 29, 2024. Starting with the foundation models from LLaMa 2, Meta AI would train an additional 500B tokens of code datasets, before an additional 20B token of long-context data, creating the Code Llama foundation models. This foundation model was further trained on 5B instruction following token to create the instruct fine-tune. Another foundation model was created for Python code, which trained on 100B tokens of Python-only code, before the long-context data.\n\n\n== Llama 3 ==\n\nOn April 18, 2024, Meta released Llama-3 with two sizes: 8B and 70B parameters. The models have been pre-trained on approximately 15 trillion tokens of text gathered from “publicly available sources” with the instruct models fine-tuned on “publicly available instruction datasets, as well as over 10M human-annotated examples\". Meta AI's testing showed in April 2024 that Llama 3 70B was beating Gemini Pro 1.5 and Claude 3 Sonnet on most benchmarks. Meta also announced plans to make Llama 3 multilingual and multimodal, better at coding and reasoning, and to increase its context window.\nDuring an interview with Dwarkesh Patel, Mark Zuckerberg said that the 8B version of Llama 3 was nearly as powerful as the largest Llama 2. Compared to previous models, Zuckerberg stated the team was surprised that the 70B model was still learning even at the end of the 15T tokens training. The decision was made to end training to focus GPU power elsewhere.\nLlama-3.1 was released on July 23, 2024, with three sizes: 8B, 70B, and 405B parameters.\n\n\n== Comparison of models ==\nFor the training cost column, only the largest model's cost is written. So for example, \"21,000\" is the training cost of Llama 2 69B in units of petaFLOP-day. Also, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. \"T\" means \"trillion\" and \"B\" means \"billion\".\n\n\n== Architecture and training ==\n\n\n=== Architecture ===\nLike GPT-3, the Llama series of models are autoregressive decoder-only Transformers, but there are some minor differences:\n\nSwiGLU activation function instead of GeLU;\nrotary positional embeddings (RoPE) instead of absolute positional embedding;\nRMSNorm instead of layer normalization;\n\n\n=== Training datasets ===\nLLaMA's developers focused their effort on scaling the model's performance by increasing the volume of training data, rather than the number of parameters, reasoning that the dominating cost for LLMs is from doing inference on the trained model rather than the computational cost of the training process.\nLLaMA 1 foundational models were trained on a data set with 1.4 trillion tokens, drawn from publicly available data sources, including:\n\nWebpages scraped by CommonCrawl\nOpen source repositories of source code from GitHub\nWikipedia in 20 languages\nPublic domain books from Project Gutenberg\nBooks3 books dataset\nThe LaTeX source code for scientific papers uploaded to ArXiv\nQuestions and answers from Stack Exchange websites\nOn April 17, 2023, TogetherAI launched a project named RedPajama to reproduce and distribute an open source version of the LLaMA dataset. The dataset has approximately 1.2 trillion tokens and is publicly available for download.\nLlama 2 foundational models were trained on a data set with 2 trillion tokens. This data set was curated to remove Web sites that often disclose personal data of people. It also upsamples sources considered trustworthy. Llama 2 - Chat was additionally fine-tuned on 27,540 prompt-response pairs created for this project, which performed better than larger but lower-quality third-party datasets. For AI alignment, reinforcement learning with human feedback (RLHF) was used with a combination of 1,418,091 Meta examples and seven smaller datasets. The average dialog depth was 3.9 in the Meta examples, 3.0 for Anthropic Helpful and Anthropic Harmless sets, and 1.0 for five other sets, including OpenAI Summarize, StackExchange, etc.\nLlama 3 consists of mainly English data, with over 5% in over 30 other languages. Its dataset was filtered by a text-quality classifier, and the classifier was trained by text synthesized by Llama 2.\nIn a lawsuit brought by Richard Kadrey and others against Meta Platforms, CEO Mark Zuckerberg was alleged to have authorized the use of copyrighted content from Library Genesis to train Llama AI models and conceal its actions by removing copyright markers from the data.\n\n\n=== Fine-tuning ===\nLlama 1 models are only available as foundational models with self-supervised learning and without fine-tuning. Llama 2 – Chat models were derived from foundational Llama 2 models. Unlike GPT-4 which increased context length during fine-tuning, Llama 2 and Code Llama - Chat have the same context length of 4K tokens. Supervised fine-tuning used an autoregressive loss function with token loss on user prompts zeroed out. The batch size was 64.\nFor AI alignment, human annotators wrote prompts and then compared two model outputs (a binary protocol), giving confidence levels and separate safety labels with veto power. Two separate reward models were trained from these preferences for safety and helpfulness using Reinforcement learning from human feedback (RLHF). A major technical contribution is the departure from the exclusive use of Proximal Policy Optimization (PPO) for RLHF – a new technique based on Rejection sampling was used, followed by PPO.\nMulti-turn consistency in dialogs was targeted for improvement, to make sure that \"system messages\" (initial instructions, such as \"speak in French\" and \"act like Napoleon\") are respected during the dialog. This was accomplished using the new \"Ghost attention\" technique during training, which concatenates relevant instructions to each new user message but zeros out the loss function for tokens in the prompt (earlier parts of the dialog).\n\n\n== Applications ==\nThe Stanford University Institute for Human-Centered Artificial Intelligence (HAI) Center for Research on Foundation Models (CRFM) released Alpaca, a training recipe based on the LLaMA 7B model that uses the \"Self-Instruct\" method of instruction tuning to acquire capabilities comparable to the OpenAI GPT-3 series text-davinci-003 model at a modest cost. The model files were officially removed on March 21, 2023, over hosting costs and safety concerns, though the code and paper remain online for reference.\nMeditron is a family of Llama-based finetuned on a corpus of clinical guidelines, PubMed papers, and articles. It was created by researchers at École Polytechnique Fédérale de Lausanne School of Computer and Communication Sciences, and the Yale School of Medicine. It shows increased performance on medical-related benchmarks such as MedQA and MedMCQA.\nZoom used Meta Llama 2 to create an AI Companion that can summarize meetings, provide helpful presentation tips, and assist with message responses. This AI Companion is powered by multiple models, including Meta Llama 2.\nReuters reported in 2024 that many Chinese foundation models relied on Llama models for their training.\n\n\n=== llama.cpp ===\n\nSoftware developer Georgi Gerganov released llama.cpp as open-source on March 10, 2023. It's a re-implementation of LLaMA in C++, allowing systems without a powerful GPU to run the model locally. The llama.cpp project introduced the GGUF file format, a binary format that stores both tensors and metadata. The format focuses on supporting different quantization types, which can reduce memory usage, and increase speed at the expense of lower model precision.\nllamafile created by Justine Tunney is an open-source tool that bundles llama.cpp with the model into a single executable file. Tunney et al. introduced new optimized matrix multiplication kernels for x86 and ARM CPUs, improving prompt evaluation performance for FP16 and 8-bit quantized data types.\n\n\n=== Military ===\nIn 2024, researchers from the People's Liberation Army Academy of Military Sciences (top military academy of China) were reported to have developed a military tool using Llama, which Meta Platforms stated was unauthorized due to Llama's license prohibiting the use of the model for military purposes. Meta granted the US government and US military contractors permission to use Llama in November 2024, but continued to prohibit military use by non-US entities.\n\n\n== Reception ==\nWired describes the 8B parameter version of Llama 3 as being \"surprisingly capable\" given its size.\nThe response to Meta's integration of Llama into Facebook was mixed, with some users confused after Meta AI told a parental group that it had a child.\nAccording to the Q4 2023 Earnings transcript, Meta adopted the strategy of open weights to improve on model safety, iteration speed, increase adoption among developers and researchers, and to become the industry standard. Llama 5, 6, and 7 are planned for the future.\nThe release of Llama models has sparked significant debates on the benefits and misuse risks of open weight models. Such models can be fine-tuned to remove safeguards, notably by cyber criminals, until they comply with harmful requests. Some experts contend that future models may facilitate causing damage more than defending against it, for example by making it relatively easy to engineer advanced bioweapons without specialized knowledge. Conversely, open-weight models can be useful for a wide variety of purposes, including for safety research.\nOpen Source Initiative head Stefano Maffulli criticized Meta for describing Llama as open source, saying that it was causing confusion among users and \"polluting\" the term.\n\n\n== See also ==\nGPT-4o\nIBM Granite, an open-source LLM made by IBM\nMistral AI, a French open-source AI company\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\nOfficial website \nOfficial Hugging Face organization for Llama, Llama Guard, and Prompt Guard models",
    "Llama.cpp": "llama.cpp is an open source software library that performs inference on various large language models such as Llama. It is co-developed alongside the GGML project, a general-purpose tensor library. \nCommand-line tools are included with the library, alongside a server with a simple web interface.\n\n\n== Background ==\nTowards the end of September 2022, Georgi Gerganov started work on the GGML library, a C library implementing tensor algebra. Gerganov developed the library with the intention of strict memory management and multi-threading. The creation of GGML was inspired by Fabrice Bellard's work on LibNC.\nBefore llama.cpp, Gerganov worked on a similar library called whisper.cpp which implemented Whisper, a speech to text model by OpenAI.\nGerganov has a background in medical physics, and was part of the Faculty of Physics in Sofia University. In 2006 he won a silver medal in the International Physics Olympiad. In 2008 he won a programming competition organized by the Bulgarian Association of Software Companies, PC Magazine and Musala Soft, a Bulgarian software services company.\n\n\n== Development ==\nllama.cpp began development in March 2023 by Georgi Gerganov as an implementation of the Llama inference code in pure C/C++ with no dependencies. This improved performance on computers without GPU or other dedicated hardware, which was a goal of the project. llama.cpp gained traction with users who lacked specialized hardware as it could run on just a CPU including on Android devices. While initially designed for CPUs, GPU inference support was later added. As of November 2024 it has more than 67,000 stars on GitHub.\nIn March 2024 Justine Tunney introduced new optimized matrix multiplication kernels for x86 and ARM CPUs, improving prompt evaluation performance for FP16 and 8-bit quantized data types. These improvements were committed upstream to llama.cpp. Tunney also created a tool called llamafile that bundles models and llama.cpp into a single file that runs on multiple operating systems via the Cosmopolitan Libc library also created by Tunney which allows C/C++ to be more portable across operating systems.\n\n\n== Architecture ==\nllama.cpp supports multiple hardware targets including x86, ARM, CUDA, Metal, Vulkan and SYCL. These back-ends make up the GGML tensor library which is used by the front-end model-specific llama.cpp code. llama.cpp supports ahead of time model quantization as opposed to on-the-fly quantization. llama.cpp makes use of several CPU extensions for optimization: AVX, AVX2 and AVX-512 for X86-64, and Neon on ARM. Apple silicon is an important target for the project. It supports grammar-based output formatting as JSON. It also supports speculative decoding.\n\n\n== GGUF file format ==\n\nThe GGUF (GGML Universal File) file format is a binary format that stores both tensors and metadata in a single file, and is designed for fast saving, and loading of model data. It was introduced in August 2023 by the llama.cpp project to better maintain backwards compatibility as support was added for other model architectures. It succeeded previous formats used by the project such as GGML.\nGGUF files are typically created by converting models developed with a different machine learning library such as PyTorch.\n\n\n=== Design ===\nThe format focuses on quantization, the act of reducing precision in the model weights. This can lead to reduced memory usage, and increased speed at the expense of lower model accuracy.\nGGUF supports 2-bit to 8-bit quantized integer types; common floating-point data formats such as float32, float16, and bfloat16; and 1.56 bit quantization.\nThis file format contains information necessary for running a GPT-like language model such as the tokenizer vocabulary, context length, tensor info and other attributes.\n\n\n== Supported models ==\n\n\n== References ==",
    "MMLU": "In artificial intelligence, Measuring Massive Multitask Language Understanding (MMLU) is a benchmark for evaluating the capabilities of large language models.\n\n\n== Benchmark ==\nThe MMLU consists of about 16,000 multiple-choice questions spanning 57 academic subjects including mathematics, philosophy, law, and medicine. It is one of the most commonly used benchmarks for comparing the capabilities of large language models, with over 100 million downloads as of July 2024.\nThe MMLU was released by Dan Hendrycks and a team of researchers in 2020 and was designed to be more challenging than then-existing benchmarks such as General Language Understanding Evaluation (GLUE) on which new language models were achieving better-than-human accuracy. At the time of the MMLU's release, most existing language models performed around the level of random chance (25%), with the best performing GPT-3 model achieving 43.9% accuracy. The developers of the MMLU estimate that human domain-experts achieve around 89.8% accuracy. As of 2024, some of the most powerful language models, such as o1, Gemini and Claude 3, were reported to achieve scores around 90%.\nAn expert review of 5,700 of the questions, spanning all 57 MMLU subjects, estimated that there were errors with 6.5% of the questions in the MMLU question set, which suggests that the maximum attainable score in MMLU is significantly below 100%.\n\n\n== Examples ==\nThe following examples are taken from the \"Abstract Algebra\" and \"International Law\" tasks, respectively. The correct answers are marked in boldface:\n\nFind all \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n in \n  \n    \n      \n        \n          \n            Z\n          \n          \n            3\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {Z} _{3}}\n  \n such that \n  \n    \n      \n        \n          \n            Z\n          \n          \n            3\n          \n        \n        [\n        x\n        ]\n        \n          /\n        \n        (\n        \n          x\n          \n            2\n          \n        \n        +\n        c\n        )\n      \n    \n    {\\displaystyle \\mathbb {Z} _{3}[x]/(x^{2}+c)}\n  \n is a field.\n(A) 0 (B) 1 (C) 2 (D) 3\n\nWould a reservation to the definition of torture in the ICCPR be acceptable in contemporary practice?\n(A) This is an acceptable reservation if the reserving country’s legislation employs a different definition\n(B) This is an unacceptable reservation because it contravenes the object and purpose of the ICCPR\n(C) This is an unacceptable reservation because the definition of torture in the ICCPR is consistent with customary international law\n(D) This is an acceptable reservation because under general international law States have the right to enter reservations to treaties\n\n\n== Leaderboard ==\n\n\n== References ==",
    "OpenAI o1": "OpenAI o1 is a reflective generative pre-trained transformer (GPT). A preview of o1 was released by OpenAI on September 12, 2024. o1 spends time \"thinking\" before it answers, making it better at complex reasoning tasks, science and programming than GPT-4o. The full version was released to ChatGPT users on December 5, 2024.\n\n\n== History ==\n\n\n=== Background ===\nAccording to leaked information, o1 was formerly known within OpenAI as \"Q*\", and later as \"Strawberry\". The codename \"Q*\" first surfaced in November 2023, around the time of Sam Altman's ousting and subsequent reinstatement, with rumors suggesting that this experimental model had shown promising results on mathematical benchmarks. In July 2024, Reuters reported that OpenAI was developing a generative pre-trained transformer known as \"Strawberry\", which later became o1.\n\n\n=== Release ===\n\"o1-preview\" and \"o1-mini\" were released on September 12, 2024, for ChatGPT Plus and Team users. GitHub started testing the integration of o1-preview in its Copilot service the same day. On December 5, 2024, the full version of o1 was released. On the same day, a subscription called ChatGPT Pro was released, featuring access to a pro version of o1 that uses more compute to provide better answers. In January 2025, o1 was integrated into Microsoft Copilot.\no1-preview's API is several times more expensive than GPT-4o. As of January 2025, API usage for the full o1 model is limited to developers on usage tier 5.\nOpenAI noted that o1 is the first of a series of \"reasoning\" models. OpenAI shared in December 2024 benchmark results for its successor, o3 (the name o2 was skipped to avoid trademark conflict with the mobile carrier brand named O2).\n\n\n== Capabilities ==\nAccording to OpenAI, o1 has been trained using a new optimization algorithm and a dataset specifically tailored to it; while also meshing in reinforcement learning into its training. OpenAI described o1 as a complement to GPT-4o rather than a successor.\no1 spends additional time thinking (generating a chain of thought) before generating an answer, which makes it better for complex reasoning tasks, particularly in science and mathematics. Compared to previous models, o1 has been trained to generate long \"chains of thought\" before returning a final answer. According to Mira Murati, this ability to think before responding represents a new, additional paradigm, which is improving model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power. OpenAI's test results suggest a correlation between accuracy and the logarithm of the amount of compute spent thinking before answering.\no1-preview performed approximately at a PhD level on benchmark tests related to physics, chemistry, and biology. On the American Invitational Mathematics Examination, it solved 83% (12.5/15) of the problems, compared to 13% (1.8/15) for GPT-4o. It also ranked in the 89th percentile in Codeforces coding competitions. o1-mini is faster and 80% cheaper than o1-preview. It is particularly suitable for programming and STEM-related tasks, but does not have the same \"broad world knowledge\" as o1-preview.\nOpenAI noted that o1's reasoning capabilities make it better at adhering to safety rules provided in the prompt's context window. OpenAI reported that during a test, one instance of o1-preview exploited a misconfiguration to succeed at a task that should have been infeasible due to a bug. OpenAI also granted early access to the UK and US AI Safety Institutes for research, evaluation, and testing. According to OpenAI's assessments, o1-preview and o1-mini crossed into \"medium risk\" in CBRN (biological, chemical, radiological, and nuclear) weapons. Dan Hendrycks wrote that \"The model already outperforms PhD scientists most of the time on answering questions related to bioweapons.\" He suggested that these concerning capabilities will continue to increase.\n\n\n== Limitations ==\no1 usually requires more computing time and power than other GPT models by OpenAI, because it generates long chains of thought before making the final response.\nAccording to OpenAI, o1 may \"fake alignment\", that is, generate a response that is contrary to accuracy and its own chain of thought, in about 0.38% of cases.\nOpenAI forbids users from trying to reveal o1's chain of thought, which is hidden by design and not trained to comply with the company's policies. Prompts are monitored, and users who intentionally or accidentally violate this may lose their access to o1. OpenAI cites AI safety and competitive advantage as reasons for the restriction, which has been described as a loss of transparency by developers who work with large language models (LLMs).\nIn October 2024, researchers at Apple submitted a preprint reporting that LLMs such as o1 may be replicating reasoning steps from the models' own training data. By changing the numbers and names used in a math problem or simply running the same problem again, LLMs would perform somewhat worse than their best benchmark results. Adding extraneous but logically inconsequential information to the problems caused a much greater drop in performance, from −17.5% for o1-preview and −29.1% for o1-mini, to −65.7% for the worst model tested.\n\n\n== References ==",
    "OpenAI o3": "OpenAI o3 is a reflective generative pre-trained transformer (GPT) model developed by OpenAI as a successor to OpenAI o1. It is designed to devote additional deliberation time when addressing questions that require step-by-step logical reasoning. OpenAI released a smaller model, o3-mini, on January 31st, 2025.\n\n\n== History ==\nThe OpenAI o3 model was announced on December 20, 2024, with the designation \"o3\" chosen to avoid trademark conflict with the mobile carrier brand named O2. OpenAI invited safety and security researchers to apply for early access of these models until January 10, 2025. Similarly to o1, there are two different models: o3 and o3-mini.\nOn January 31, 2025, OpenAI released o3-mini to all ChatGPT users (including free-tier) and some API users. OpenAI describes o3-mini as a \"specialized alternative\" to o1 for \"technical domains requiring precision and speed\". o3-mini features three reasoning effort levels: low, medium and high. The free version uses medium. The variant using more compute is called o3-mini-high, and is available to paid subscribers. Subscribers to ChatGPT's Pro tier have unlimited access to both o3-mini and o3-mini-high.\nOn February 2, 2025, OpenAI launched OpenAI Deep Research, a ChatGPT service using a version of o3 that makes comprehensive reports within 5 to 30 minutes, based on web searches.\nOn February 6, 2025, in response to pressure from rivals like DeepSeek, OpenAI announced an update aimed at enhancing the transparency of the thought process in its o3-mini model. On February 12th, 2025, OpenAI further increased rate limits for o3-mini-high to 50 requests per day (from 50 requests per week) for ChatGPT Plus subscribers, and implemented file/image upload support.\n\n\n== Capabilities ==\nReinforcement learning was used to teach o3 to \"think\" before generating answers, using what OpenAI refers to as a \"private chain of thought\". This approach enables the model to plan ahead and reason through tasks, performing a series of intermediate reasoning steps to assist in solving the problem, at the cost of additional computing power and increased latency of responses.\no3 demonstrates significantly better performance than o1 on complex tasks, including coding, mathematics, and science. OpenAI reported that o3 achieved a score of 87.7% on the GPQA Diamond benchmark, which contains expert-level science questions not publicly available online.\nOn SWE-bench Verified, a software engineering benchmark assessing the ability to solve real GitHub issues, o3 scored 71.7%, compared to 48.9% for o1. On Codeforces, o3 reached an Elo score of 2727, whereas o1 scored 1891.\nOn the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) benchmark, which evaluates an AI's ability to handle new logical and skill acquisition problems, o3 attained three times the accuracy of o1.\n\n\n== References ==",
    "PaLM": "PaLM (Pathways Language Model) is a 540 billion-parameter dense decoder-only transformer-based large language model (LLM) developed by Google AI. Researchers also trained smaller versions of PaLM (with 8 and 62 billion parameters) to test the effects of model scale.\nPaLM is capable of a wide range of tasks, including commonsense reasoning, arithmetic reasoning, joke explanation, code generation, and translation. When combined with chain-of-thought prompting, PaLM achieved significantly better performance on datasets requiring reasoning of multiple steps, such as word problems and logic-based questions.\nThe model was first announced in April 2022 and remained private until March 2023, when Google launched an API for PaLM and several other technologies. The API was initially available to a limited number of developers who joined a waitlist before it was released to the public.\nGoogle and DeepMind developed a version of PaLM 540B (the parameter count, 540 billion), called Med-PaLM, that is fine-tuned on medical data and outperforms previous models on medical question answering benchmarks. Med-PaLM was the first to obtain a passing score on U.S. medical licensing questions, and in addition to answering both multiple choice and open-ended questions accurately, it also provides reasoning and is able to evaluate its own responses.\nGoogle also extended PaLM using a vision transformer to create PaLM-E, a state-of-the-art vision-language model that can be used for robotic manipulation. The model can perform tasks in robotics competitively without the need for retraining or fine-tuning.\nIn May 2023, Google announced PaLM 2 at the annual Google I/O keynote. PaLM 2 is reported to be a 340 billion-parameter model trained on 3.6 trillion tokens.\nIn June 2023, Google announced AudioPaLM for speech-to-speech translation, which uses the PaLM-2 architecture and initialization.\n\n\n== Training ==\nPaLM is pre-trained on a high-quality corpus of 780 billion tokens that comprise various natural language tasks and use cases. This dataset includes filtered webpages, books, Wikipedia articles, news articles, source code obtained from open source repositories on GitHub, and social media conversations. It is based on the dataset used to train Google's LaMDA model. The social media conversation portion of the dataset makes up 50% of the corpus, which aids the model in its conversational capabilities.\nPaLM 540B was trained over two TPU v4 Pods with 3,072 TPU v4 chips in each Pod attached to 768 hosts, connected using a combination of model and data parallelism, which was the largest TPU configuration. This allowed for efficient training at scale, using 6,144 chips, and marked a record for the highest training efficiency achieved for LLMs at this scale: a hardware FLOPs utilization of 57.8%.\n\n\n== See also ==\nLaMDA, PaLM's predecessor\nGemini, PaLM's successor\nChinchilla\n\n\n== References ==",
    "The Pile (dataset)": "The Pile is an 886.03 GB diverse, open-source dataset of English text created as a training dataset for large language models (LLMs). It was constructed by EleutherAI in 2020 and publicly released on December 31 of that year. It is composed of 22 smaller datasets, including 14 new ones.\n\n\n== Creation ==\nTraining LLMs requires sufficiently vast amounts of data that, before the introduction of the Pile, most data used for training LLMs was taken from the Common Crawl. However, LLMs trained on more diverse datasets are better able to handle a wider range of situations after training. The creation of the Pile was motivated by the need for a large enough dataset that contained data from a wide variety of sources and styles of writing.\nCompared to other datasets, the Pile's main distinguishing features are that it is a curated selection of data chosen by researchers at EleutherAI to contain information they thought language models should learn and that it is the only such dataset that is thoroughly documented by the researchers who developed it.\n\n\n== Contents and filtering ==\nArtificial intelligences do not learn all they can from data on the first pass, so it is common practice to train an AI on the same data more than once with each pass through the entire dataset referred to as an \"epoch\". Each of the 22 sub-datasets that make up the Pile was assigned a different number of epochs according to the perceived quality of the data. The table below shows the relative size of each of the 22 sub-datasets before and after being multiplied by the number of epochs. Numbers have been converted to GB, and asterisks are used to indicate the newly introduced datasets.\n\nEleutherAI chose the datasets to try to cover a wide range of topics and styles of writing, including academic writing, which models trained on other datasets were found to struggle with.\nAll data used in the Pile was taken from publicly accessible sources. EleutherAI then filtered the dataset as a whole to remove duplicates. Some sub-datasets were also filtered for quality control. Most notably, the Pile-CC is a modified version of the Common Crawl in which the data was filtered to remove parts that are not text, such as HTML formatting and links.\nSome potential sub-datasets were excluded for various reasons, such as the US Congressional Record, which was excluded due to its racist content.\nWithin the sub-datasets that were included, individual documents were not filtered to remove non-English, biased, or profane text. It was also not filtered on the basis of consent, meaning that, for example, the Pile-CC has all of the same ethical issues as the Common Crawl itself. However, EleutherAI has documented the amount of bias (on the basis of gender, religion, and race) and profanity as well as the level of consent given for each of the sub-datasets, allowing an ethics-concerned researcher to use only those parts of the Pile that meet their own standards.\n\n\n== Use ==\nThe Pile was originally developed to train EleutherAI's GPT-Neo models but has become widely used to train other models, including Microsoft's Megatron-Turing Natural Language Generation, Meta AI's Open\nPre-trained Transformers, LLaMA, and Galactica, Stanford University's BioMedLM 2.7B, the Beijing Academy of Artificial Intelligence's \nChinese-Transformer-XL, Yandex's YaLM 100B, and Apple's OpenELM.\nIn addition to being used as a training dataset, the Pile can also be used as a benchmark to test models and score how well they perform on a variety of writing styles.\n\n\n== DMCA takedown ==\nThe Books3 component of the dataset contains copyrighted material compiled from Bibliotik, a pirate website.\nIn July 2023, the Rights Alliance took copies of The Pile down through DMCA notices. Users responded by creating copies of The Pile with the offending content removed.\n\n\n== See also ==\nList of chatbots\n\n\n== References ==",
    "Qwen": "Qwen (also called Tongyi Qianwen, Chinese: 通义千问) is a family of large language models developed by Alibaba Cloud. In July 2024, it was ranked as the top Chinese language model in some benchmarks and third globally behind the top models of Anthropic and OpenAI.\n\n\n== Models ==\nAlibaba first launched a beta of Qwen in April 2023 under the name Tongyi Qianwen. The model was based on the LLM Llama developed by Meta AI, with various modifications. It was publicly released in September 2023 after receiving approval from the Chinese government. In December 2023 it released its 72B and 1.8B models as open source, while Qwen 7B was open sourced in August. \nIn June 2024 Alibaba launched Qwen 2 and in September it released some of its models as open source, while keeping its most advanced models proprietary. Qwen 2 employs a mixture of experts. \nIn November 2024, QwQ-32B-Preview, a model focusing on reasoning similar to OpenAI's o1 was released under the Apache 2.0 License, although only the weights were released, not the dataset or training method. QwQ has a 32,000 token context length and performs better than o1 on some benchmarks.\nThe Qwen-Vl series is a line of visual language models that  combines a vision transformer with a LLM. Alibaba released Qwen-VL2 with variants of 2 billion and 7 billion parameters. Qwen-vl-max is Alibaba's flagship vision model as of 2024 and is sold by Alibaba Cloud at a cost of US$0.00041 per thousand input tokens.\nAlibaba has released several other model types such as Qwen-Audio and Qwen2-Math.  In total, it has released more than 100 models as open source, with its models having been downloaded more than 40 million times. Fine-tuned versions of Qwen have been developed by enthusiasts, such as \"Liberated Qwen\", developed by San Francisco-based Abacus AI, which is a version that responds to any user request without content restrictions.\nIn January 2025, Alibaba launched Qwen 2.5-Max. According to a blog post from Alibaba, Qwen 2.5-Max outperforms other foundation models such as GPT-4o, DeepSeek-V3, and Llama-3.1-405B in key benchmarks.\n\n\n== References ==\n\n\n== External links ==\n\nOfficial website\nQwen on GitHub\nQwen on Hugging Face",
    "Reasoning language model": "Reasoning language models are artificial intelligence systems that combine natural language processing with structured reasoning capabilities. These models are usually constructed by prompting, supervised finetuning (SFT), and reinforcement learning (RL) initialized with pretrained language models.\n\n\n== Prompting ==\n\nA language model is a generative model of a training dataset of texts. Prompting means constructing a text prompt, such that, conditional on the text prompt, the language model generates a solution to the task. Prompting can be applied to a pretrained model (\"base model\"), a base model that has undergone SFT, or RL, or both.\n\n\n=== Chain of thought ===\nChain of Thought prompting (CoT) prompts the model to answer a question by first generating a \"chain of thought\", i.e. steps of reasoning that mimic a train of thought. It was published in 2022 by the Brain team of Google on the PaLM-540B model. In CoT prompting, the prompt is of the form \"<Input> Let's think step by step\", and the model would respond with a chain of reasoning steps, ended with an answer:\n  \n    \n      \n        \n          Input\n        \n        →\n        \n          \n            \n              \n                \n                  \n                    Step\n                  \n                  \n                    1\n                  \n                \n                →\n                \n                  \n                    Step\n                  \n                  \n                    2\n                  \n                \n                →\n                ⋯\n                →\n                \n                  \n                    Step\n                  \n                  \n                    n\n                  \n                \n              \n              ⏟\n            \n          \n          \n            Reasoning chain\n          \n        \n        →\n        \n          Answer\n        \n      \n    \n    {\\displaystyle {\\text{Input}}\\rightarrow \\underbrace {{\\text{Step}}_{1}\\rightarrow {\\text{Step}}_{2}\\rightarrow \\cdots \\rightarrow {\\text{Step}}_{n}} _{\\text{Reasoning chain}}\\rightarrow {\\text{Answer}}}\n  \nSimilarly, Tree of Thought prompting generalizes CoT by prompting the model to generate one or more \"possible next steps\", and then running the model on each of the possible next steps by breadth-first, beam, or some other method of tree search. Similarly, Graph of Thought generalizes CoT so that the reasoning steps form a directed acyclic graph.\nSelf-consistency decoding performs several chain-of-thought rollouts, then selects the most commonly reached conclusion out of all the rollouts. If the rollouts disagree by a lot, a human can be queried for the correct chain of thought.\n\n\n=== Retrieval-augmented generation ===\n\nA language model may answer a query by first querying a database of documents using the query. The document retrieval can be via a vector database, summary index, tree index, or keyword table index. Following document retrieval, the LLM generates an output that incorporates information from both the query and the retrieved documents.\n\n\n=== Tool use ===\nLanguage models can perform long reasoning steps by calling external methods, such as numerical recipes, program interpreters, API calls, and so on. This can be prompt-engineered by describing the external methods in-context (an example of in-context learning) or finetuned into the model.\n\n\n== Supervised finetuning ==\nA base model can be finetuned on a dataset of reasoning tasks with example solutions and reasoning traces. The finetuned model would then be able to generate reasoning traces for a given problem.\nAs it is expensive to get humans to write reasoning traces for a SFT dataset, researchers have proposed ways to automatically construct SFT datasets. In rejection sampling finetuning (RFT), new reasoning traces are collected via a loop:\n\nSample a task prompt\nGenerate many reasoning traces for the prompt.\nUse a verifier to remove reasoning traces with the wrong final answer.\nFor each remaining trace, extract the set of equations appearing in it. Deduplicate the traces so that each one has a different set of equations. Add those to the dataset.\n\n\n== Reinforcement learning ==\nA pretrained language model can be further trained by RL. In the RL formalism, a generative language model is a policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n. A prompt specifying a task to solve is an environmental state \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n, and the response of the language model to the prompt is an action \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n. The probability that the language model responds \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n with \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n is \n  \n    \n      \n        π\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle \\pi (y|x)}\n  \n.\nTraining a reasoning language model by RL then consists of constructing a reward model \n  \n    \n      \n        r\n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle r(x,y)}\n  \n to guide the RL process. Intuitively, a reward model describes how desirable/appropriate/good the response is for the prompt. For reasoning language model, the prompt describes a reasoning task, and the reward would be high if the response solves the task, and low if the response fails to solve the task.\nFor reasoning language models, the model's response \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n may be broken down into multiple steps, in which case it is written as \n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle y_{1},y_{2},\\dots ,y_{n}}\n  \n.\n\n\n=== Outcome Reward Model ===\n\nOutcome reward model, or outcome-supervised RM (ORM), is a reward model that computes the reward of a step \n  \n    \n      \n        r\n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle r(x,y_{1},\\dots ,y_{i})}\n  \n determined by the final answer: \n  \n    \n      \n        r\n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n        =\n        r\n        (\n        x\n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle r(x,y_{1},\\dots ,y_{i})=r(x,y_{n})}\n  \n. They are also called \"verifiers\". \nFor tasks with an answer that is easy to verify, such as word problems in math, the outcome reward can simply be binary: 1 if the final answer is correct, and 0 otherwise. If the answer is not easy to verify programmatically, humans can manually label the answers as correct or not, then the labels can be used to finetune a base model that predicts the human label. For other kinds of tasks, such as creative writing, where task performance is not binary true/false, one can train a reward model by finetuning a base model on human ranked preference data, such as used in reinforcement learning from human feedback. A base model can also be finetuned to predict, given a partial thinking trace \n  \n    \n      \n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle x,y_{1},\\dots ,y_{m}}\n  \n, whether the final answer would be correct or not. This can then be used as a binary reward signal.\nThe ORM is usually trained via logistic regression, i.e. minimizing cross-entropy loss.\nGiven a PRM, an ORM can be constructed by multiplying the total process reward during the reasoning trace, or by taking the minimum, or some other method to aggregate the process rewards.\n\n\n=== Process Reward Model ===\n\nProcess reward model, or process-supervised RM (PRM), is a reward model that computes the reward of a step \n  \n    \n      \n        r\n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle r(x,y_{1},\\dots ,y_{i})}\n  \n determined by the steps so far: \n  \n    \n      \n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle (x,y_{1},\\dots ,y_{i})}\n  \n.\nGiven a partial thinking trace \n  \n    \n      \n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle x,y_{1},\\dots ,y_{m}}\n  \n, a human can be queried as to whether the steps so far are correct, regardless of whether the ultimate answer would be correct. This can then be used as a binary reward signal. As human labels are expensive, a base model can be finetuned to predict the human labels. The PRM is usually trained via logistic regression, i.e. minimizing cross-entropy loss.\nAs an example, in a 2023 OpenAI paper, 800K process labels were collected for 75K solution traces. A labeler would be presented with a solution trace, and keep labelling \"positive\" if the step progresses towards the solution, \"neutral\" if it is not wrong, but does not progress towards solution, and \"negative\" if it is a mistake. As soon as a \"negative\" label is entered, the labeler stops labeling that thinking trace, and begins labeling another one. The idea was that, while labelling subsequent reasoning steps can provide even richer supervision signals, simply labeling up to the first error was sufficient for training a competent PRM.\nAs human labels are expensive, researchers have proposed methods to create PRM without human labels on the processes. Inspired by Monte Carlo tree search (MCTS), the Math-Shepherd method samples multiple continuations until the end, starting at each reasoning step \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n  \n, and set the reward at that step to be either \n  \n    \n      \n        \n          \n            \n              #\n              \n                (correct answers)\n              \n            \n            \n              #\n              \n                (total answers)\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\#{\\text{(correct answers)}}}{\\#{\\text{(total answers)}}}}}\n  \n in the case of \"soft estimation\", or \n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  1\n                \n                \n                  \n                    if one of the answers is correct\n                  \n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    else\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}1&{\\text{if one of the answers is correct}}\\\\0&{\\text{else}}\\end{cases}}}\n  \n in the case of \"hard estimation\". This creates process reward using only an ORM, which is usually easier or cheaper to construct. After creating these process reward labels, a PRM can be trained on them. Some have tried a fully MCTS approach.\nOne can also use an ORM to implicitly construct a PRM, similar to direct preference optimization.\n\n\n=== Guided sampling ===\nA trained ORM can be used to select the best response. The policy would rollout multiple responses, and a trained ORM would select the best response. This allows a simple form of test time compute scaling (\"best-of-N\"). \nA trained PRM can also be used to guide reasoning by greedy tree search. That is, the policy model generates several possible next reasoning steps, and the PRM selects the best one, and the process repeats. This is similar to how a trained ORM can be used to select the best response. Beam search perform better than greedy search.\nLookahead search is another tree search method, where the policy model generates several possible next reasoning steps, then make a (partial) rollout for each. If a solution endpoint is reached during the forward simulation, the process halts early. Otherwise, the PRM is used to calculate the total reward for each rollout. The step with the highest rollout is selected.\nSelf-consistency can be combined with an ORM. The model would be used to generate multiple answers, and the answers would be clustered, so that each cluster has the same answer. The ORM is used to compute the reward for each answer, and the rewards within each cluster is summed. The answer corresponding to the cluster with the highest summed reward is output.\n\n\n== Applications ==\nPrompt engineering was discovered in GPT-3 as \"few-shot learning\", which began a period of research into \"eliciting\" capacities of pretrained language models. It was then found that a model could be prompted to perform CoT reasoning, which improves its performance on reasoning tasks.\n\n\n== Benchmark ==\n\nThe reasoning ability of language models are usually tested on problems with unambiguous solutions that can be cheaply checked, and requires reasoning when solved by a human. Such problems are usually in mathematics and competitive programming. The answer is usually an array of integers, a multiple choice letter, or a program that passes unit tests within a limited runtime. Some common ones include: \n\nGSM8K (Grade School Math): 8.5K linguistically diverse elementary school math word problems that require 2 to 8 basic arithmetic operations to solve.\nMMLU (Measuring Massive Multitask Language Understanding): 16,000 multiple-choice questions spanning 57 academic subjects including mathematics, philosophy, law, and medicine.\nGPQA (Google-Proof Q&A): 448 multiple-choice questions written by domain experts in biology, physics, and chemistry, and requires PhD-level experts to solve.\nHumanEval: Programming problems where the solution is always a python function, often just a few lines long.\nThe benchmark scores are of the following kinds:\n\npass@n: The model is given \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n attempts to solve each problem. If any attempt is correct, the model earns a point. The pass@n score is the model's average score over all problems.\ncons@n: The model is given \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n attempts to solve each problem. If the most common answer is correct, the model earns a point. The cons@n score is the model's average score over all problems. Here \"cons\" stands for \"consensus\" or \"majority voting\".\nThe pass@n score can be estimated more accurately by making \n  \n    \n      \n        N\n        >\n        n\n      \n    \n    {\\displaystyle N>n}\n  \n attempts, and use the unbiased estimator \n  \n    \n      \n        1\n        −\n        \n          \n            \n              \n                (\n              \n              \n                \n                  N\n                  −\n                  c\n                \n                n\n              \n              \n                )\n              \n            \n            \n              \n                (\n              \n              \n                N\n                n\n              \n              \n                )\n              \n            \n          \n        \n      \n    \n    {\\displaystyle 1-{\\frac {\\binom {N-c}{n}}{\\binom {N}{n}}}}\n  \n, where \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n is the number of correct attempts.\n\n\n== See also ==\nGenerative pre-trained transformer\nNeuro-symbolic AI\nAutomated theorem proving\nAutomated reasoning\nReflection (artificial intelligence)\nLarge language model\n\n\n== References ==\n\n\n== External links ==\nFortes, Armando (2025-01-27), atfortes/Awesome-LLM-Reasoning, retrieved 2025-01-27\nHuang, Jie; Chang, Kevin Chen-Chuan (2023-05-26), Towards Reasoning in Large Language Models: A Survey, arXiv:2212.10403\nBesta, Maciej; Barth, Julia; Schreiber, Eric; Kubicek, Ales; Catarino, Afonso; Gerstenberger, Robert; Nyczyk, Piotr; Iff, Patrick; Li, Yueling (2025-01-23), Reasoning Language Models: A Blueprint, arXiv:2501.11223",
    "Reflection (artificial intelligence)": "Reflection in artificial intelligence is ability of large language models (LLMs) to examine, evaluate, and refine their own outputs. By incorporating self-assessment and internal deliberation, reflective AI aims to improve reasoning accuracy, reduce errors (such as hallucinations), and enhance interpretability. It is a form of \"test-time compute\".\n\n\n== Introduction ==\nThis internal process of \"thinking\" about the steps leading to an answer is analogous to human metacognition or \"thinking about thinking.\" It helps AI systems approach tasks that require multi-step reasoning, planning, and logical thought. Reflection can occur either after completing a full processing cycle (intermediate outputs may be hidden) and generating output, or continuously throughout the process. In LLMs, special tokens can mark the beginning and end of reflection before producing a final response (e.g., <thinking>).\n\n\n== Background ==\nTraditional neural networks process inputs in a feedforward manner, generating outputs in a single pass. However, their limitations in handling complex reasoning tasks have led to the development of methods that simulate internal deliberation. Techniques such as chain-of-thought prompting encourage models to generate intermediate reasoning steps, thereby providing a form of self-reflection that can improve performance on tasks including arithmetic, commonsense reasoning, and more.\n\n\n== Techniques ==\nIncreasing the length of the Chain-of-Thought reasoning process, by passing the output of the model back to its input and doing multiple network passes, has been used to increases inference-time scaling. Reinforcement learning frameworks such as Group Relative Policy Optimization  have been used to steer the Chain-of-Though to produce desired final outputs\n\n\n==== Sequential scaling ====\nRefines reasoning by allowing later computations to build upon earlier ones, improving accuracy through iterative refinement.\n\n\n==== Parallel scaling ====\nProcesses multiple reasoning paths independently and selects the most reliable outcome based on evaluation metrics, enhancing robustness and diversity of solutions.\nIn the 2025 paper \"s1: Simple test-time scaling,\" demonstrated the effectiveness of budget forcing and scaling techniques using the s1-32B model, a fine-tuned version of Qwen2.5-32B-Instruct. By training on a carefully curated dataset of 1,000 examples (s1K) and implementing budget forcing, s1-32B matched or outperformed larger proprietary models like OpenAI's o1-preview. Notably, it exceeded o1-preview by up to 27% on competition math benchmarks (MATH and AIME24). Furthermore, applying scaling strategies improved its AIME24 score from 50% to 57% without additional test-time intervention.\n\n\n=== Types of reflection ===\n\n\n==== Post-hoc reflection ====\nAnalyzes and critiques an initial output separately. This often involves prompting the model to identify errors or suggest improvements after generating a response. The Reflexion framework follows this approach.\n\n\n==== Iterative reflection ====\nRevises earlier parts of a response dynamically during generation. Self-monitoring mechanisms allow the model to adjust reasoning as it progresses. Methods like Tree-of-Thoughts exemplify this, enabling backtracking and alternative exploration.\n\n\n==== Intrinsic reflection ====\nIntegrates self-monitoring directly into the model architecture rather than relying solely on external prompts. This approach could enable models with inherent awareness of their reasoning limitations and uncertainties. This has been used by Google DeepMind in a technique called Self-Correction via Reinforcement Learning (SCoRe) which rewards the model for improving its responses.\n\n\n=== Model architectures ===\n\n\n==== Architectures incorporating reflection mechanisms ====\nCertain model architectures integrate explicit reflection mechanisms to enhance self-monitoring and reasoning awareness. These architectures may include additional components designed to analyze intermediate steps and refine outputs based on iterative feedback.\n\n\n==== Self-monitoring mechanisms ====\nSome architectures embed modules that continuously assess confidence levels and logical consistency. These mechanisms help improve response quality by identifying potential reasoning flaws in real-time.\n\n\n==== Modular approaches for dynamic reasoning feedback ====\nIncorporates specialized reasoning modules that can be selectively activated depending on the complexity of the task. These modular systems enable dynamic adjustments to reasoning depth and strategy during inference.\n\n\n=== Training strategies ===\n\n\n==== Reasoning templates ====\nIncludes structured reasoning examples in training data, helping the model generalize logical steps more effectively and improving coherence.\n\n\n==== Specialized loss functions ====\nEncourages intermediate reasoning steps and penalizes inconsistencies between generated outputs and self-reflections. This approach reinforces logical consistency and reduces hallucinated responses.\n\n\n== Benchmarks ==\nReflective models generally outperform non-reflective models in most benchmarks, especially on tasks requiring multi-step reasoning.\nHowever, some benchmarks exclude reflective models due to longer response times.\n\n\n=== Humanity's Last Exam ===\nThe HLE, a rigorous benchmark designed to assess expert-level reasoning across mathematics, humanities, and the natural sciences, reveals substantial performance gaps among models. State-of-the-art reasoning models have demonstrated low accuracy on HLE, highlighting significant room for improvement. In particular, the full reasoning model o3 achieved an accuracy of 26.6%, while its lighter counterpart, o3‑mini (high) (evaluated on text‑only questions), reached 13%.\n\n\n=== AIME ===\nThe American Invitational Mathematics Examination (AIME) benchmark, a challenging mathematics competition, demonstrates significant performance differences between model types. Non-reasoning models typically solve less than 30% of AIME. In contrast, models employing reasoning techniques score between 50% and 80%. While OpenAI's o1 maintained or slightly improved its accuracy from reported 2024 metrics to 2025 AIME results, o3-mini (high) achieved a higher accuracy (80%) at a significantly lower cost (approximately 12 times cheaper).\n\n\n=== o3-mini performance ===\nAccording to OpenAI's January 2025 report on o3-mini, adjustable \"reasoning effort\" significantly affects performance, particularly in STEM. Increasing reasoning effort from low to high boosts accuracy on benchmarks like AIME 2024, GPQA Diamond, and Codeforces, providing performance gains typically in the range of 10-30%. With high reasoning effort, o3-mini (high) achieved 87.3% in AIME (different from the MathArena AIME benchmark results), 79.7% in GPQA Diamond, 2130 Elo in Codeforces, and 49.3 in SWE-bench Verified.\n\n\n== Integration with search capabilities ==\nIn December 2024, Google introduced Deep Research in Gemini, a feature in Gemini that conducts multi-step research tasks.\nOn January 25, 2025, DeepSeek launched a feature in their DeepSeek R1 model, enabling the simultaneous use of search and reasoning capabilities, which allows for more efficient integration of data retrieval with reflective reasoning processes.\nSubsequently, OpenAI's o3-mini model gained the ability to combine search and reasoning in a unified process.\nOn February 2, 2025, OpenAI released deep research, a tool that integrates reasoning and web search in a unified workflow, allowing users to perform complex research tasks that require multi-step reasoning and data synthesis from multiple sources. It is based on o3 and can take from 5 to 30 minutes to generate comprehensive reports.\n\n\n== History ==\n\n\n=== 2024 ===\no1-preview, a LLM with enhanced reasoning released in September 2024, showed significant improvements on benchmarks. In December 2024, the full version o1 was released, incorporating lessons learned from the preview stage. OpenAI also shared preliminary results on its successor, o3, featuring new records on benchmarks for coding, science and mathematics.\n\nAlibaba also released reasoning versions of its Qwen family of LLMs, such as QwQ-32B-Preview and QvQ-72B-Preview in late 2024.\n\n\n=== 2025 ===\nIn January 2025, the Chinese company DeepSeek gained significant attention by releasing DeepSeek R1, a reasoning model competitive with o1 but at a much lower cost. In the following weeks, OpenAI released o3-mini and a variant using more \"reasoning effort\" called o3-mini-high. It also released deep research, that uses o3.\n\n\n== Applications ==\n\n\n=== Mathematical and logical reasoning ===\nReflection enables LLMs to solve multi-step problems, demonstrated on benchmarks like FrontierMath, GSM8K (mathematical word problems), GPQA Diamond (PhD-level Science Questions) and Big-Bench Hard (challenging reasoning tasks). A model might initially produce an incorrect solution but, through self-reflection, identify the flawed step and generate a corrected answer.\n\n\n=== Vision-Language tasks ===\nFrameworks like R3V allow vision-language models to iteratively refine reasoning on complex multimodal tasks. In visual question answering, the model might first generate a plausible but incorrect answer based on a superficial understanding. Through reflection, it could identify inconsistencies between its answer and image details, leading to a revised, more accurate response.\n\n\n=== General Problem Solving ===\nEnhanced reflection leads to improved coherence, long-term planning, and reduced hallucinations. This is valuable in tasks requiring planning, sequential decision-making, or creative problem-solving, like writing code, composing stories, or designing experiments.\n\n\n== Models ==\n\n\n=== OpenAI ===\no3 and o3-mini\no1-preview and o1\n\n\n=== Gemini ===\n2.0 Flash Thinking Experimental\n\n\n=== DeepSeek ===\nR1 (based on V3)\nR1-Lite-Preview (test version based on V2.5)\n\n\n=== Qwen ===\nQvQ-72B-Preview — an experimental visual reasoning model launched on December 24, 2024, which integrates image understanding with verbal chain-of-thought reasoning.\nQwQ-32B-Preview — an experimental text-based reasoning model released in late November 2024 that emphasizes complex, step-by-step analysis.\n\n\n=== Anthropic ===\nClaude Sonnet 3.7 has an adjustable amount of 'thinking' tokens.\n\n\n=== xAI ===\nGrok 3\n\n\n=== Experiments ===\n\n\n==== Llama 3B scaling test-time compute ====\nOn December 16, 2024, an experiment using a Llama 3B model demonstrated that by scaling test-time compute, a relatively small model could outperform a much larger Llama 70B model on challenging reasoning tasks. This result highlighted that improved inference strategies can unlock latent reasoning capabilities even in compact models.\n\n\n== Criticism and challenges ==\n\n\n=== Computational cost ===\nReflective models require significantly more test-time compute than non-reasoning models. On the AIME benchmark, reasoning models were 10 to 74 times more expensive than non-reasoning counterparts. The cheapest model, Gemini 2.0-Flash, cost just $0.06 per benchmark.\n\n\n=== Latency Issues ===\nReflective reasoning significantly increases response times, with current models taking anywhere from three seconds to several minutes to generate an answer. As reasoning depth improves, future models may require even longer processing times.\n\n\n== See also ==\nPrompt engineering\nMeta-learning\nArtificial general intelligence\nAutomated reasoning\n\n\n== References ==",
    "Retrieval-augmented generation": "Retrieval-augmented generation (RAG) is a technique that grants generative artificial intelligence models information retrieval capabilities. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to augment information drawn from its own vast, static training data. This allows LLMs to use domain-specific and/or updated information.  \nUse cases include providing chatbot access to internal company data or giving factual information only from an authoritative source.\n\n\n== Process ==\nThe RAG process is made up of four key stages. First, all the data must be prepared and indexed for use by the LLM. Thereafter, each query consists of a retrieval, augmentation, and generation phase.\n\n\n=== Indexing ===\nTypically, the data to be referenced is converted into LLM embeddings, numerical representations in the form of a large vector space. RAG can be used on unstructured (usually text), semi-structured, or structured data (for example knowledge graphs). These embeddings are then stored in a vector database to allow for document retrieval.\n\n\n=== Retrieval ===\nGiven a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query. This comparison can be done using a variety of methods, which depend in part on the type of indexing used.\n\n\n=== Augmentation ===\nThe model feeds this relevant retrieved information into the LLM via prompt engineering of the user's original query. Newer implementations (as of 2023) can also incorporate specific augmentation modules with abilities such as expanding queries into multiple domains and using memory and self-improvement to learn from previous retrievals.\n\n\n=== Generation ===\nFinally, the LLM can generate output based on both the query and the retrieved documents. Some models incorporate extra steps to improve output, such as the re-ranking of retrieved information, context selection, and fine-tuning.\n\n\n== Improvements ==\nImprovements to the basic process above can be applied at different stages in the RAG flow. \n\n\n=== Encoder ===\nThese methods center around the encoding of text as either dense or sparse vectors. Sparse vectors, used to encode the identity of a word, are typically dictionary length and contain almost all zeros. Dense vectors, used to encode meaning, are much smaller and contain far fewer zeros. Several enhancements can be made to the way similarities are calculated in the vector stores (databases).  \n\nPerformance can be improved with faster dot products, approximate nearest neighbors, or centroid searches.\nAccuracy can be improved with Late Interactions.\nHybrid vectors: dense vector representations can be combined with sparse one-hot vectors in order to use the faster sparse dot products rather than the slower dense ones.  Other methods can combine sparse methods (BM25, SPLADE) with dense ones like DRAGON.\n\n\n=== Retriever-centric methods ===\nThese methods focus on improving the quality of hits from the vector database:\n\npre-train the retriever using the Inverse Cloze Task.\nprogressive data augmentation.  The method of Dragon samples difficult negatives to train a dense vector  retriever.\nUnder supervision, train the retriever for a given generator.  Given a prompt and the desired answer, retrieve the top-k vectors, and feed those vectors into the generator to achieve a perplexity score for the correct answer.  Then minimize the KL-divergence between the observed retrieved vectors probability and LM likelihoods to adjust the retriever.\nuse reranking to train the retriever.\n\n\n=== Language model ===\n\nBy redesigning the language model with the retriever in mind, a 25-time smaller network can get comparable perplexity as its much larger counterparts.  Because it is trained from scratch, this method (Retro) incurs the high cost of training runs that the original RAG scheme avoided.  The hypothesis is that by giving domain knowledge during training, Retro needs less focus on the domain and can devote its smaller weight resources only to language semantics.  The redesigned language model is shown here.  \nIt has been reported that Retro is not reproducible, so modifications were made to make it so.  The more reproducible version is called Retro++ and includes in-context RAG.\n\n\n=== Chunking ===\n\nChunking involves various strategies for breaking up the data into vectors so the retriever can find details in it.\n\nThree types of chunking strategies are:\n\nFixed length with overlap. This is fast and easy.  Overlapping consecutive chunks helps to maintain semantic context across chunks.\nSyntax-based chunks can break the document up into sentences.  Libraries such as spaCy or NLTK can also help.\nFile format-based chunking. Certain file types have natural chunks built in, and it's best to respect them.  For example, code files are best chunked and vectorized as whole functions or classes.  HTML files should leave <table> or base64 encoded <img> elements intact.  Similar considerations should be taken for pdf files.  Libraries such as Unstructured or Langchain can assist with this method.\n\n\n=== Knowledge Graphs ===\nRather than using documents as a source to vectorize and retrieve from, Knowledge Graphs can be used.  One can start with a set of documents, books, or other bodies of text, and convert them to a knowledge graph using one of many methods, including language models.  Once the knowledge graph is created, subgraphs can be vectorized, stored in a vector database, and used for retrieval as in plain RAG.  The advantage here is that graphs has more recognizable structure than strings of text and this structure can help retrieve more relevant facts for generation.  Sometimes this approach is called GraphRAG.\n\n\n=== Hybrid Search ===\nSometimes vector database searches can miss key facts needed to answer a user's question.  One way to mitigate this is to do a traditional text search, add those results to the text chunks linked to the retrieved vectors from the vector search, and feed the combined hybrid text into the language model for generation. \n\n\n== Challenges ==\nIf the external data source is large, retrieval can be slow. The use of RAG does not completely eliminate the general challenges faced by LLMs, including hallucination.\n\n\n== References ==",
    "Sparrow (chatbot)": "Sparrow is a chatbot developed by the artificial intelligence research lab DeepMind, a subsidiary of Alphabet Inc. It is designed to answer users' questions correctly, while reducing the risk of unsafe and inappropriate answers. One motivation behind Sparrow is to address the problem of language models producing incorrect, biased or potentially harmful outputs. Sparrow is trained using human judgements, in order to be more “Helpful, Correct and Harmless” compared to baseline pre-trained language models. The development of Sparrow involved asking paid study participants to interact with Sparrow, and collecting their preferences to train a model of how useful an answer is.\nTo improve accuracy and help avoid the problem of hallucinating incorrect answers, Sparrow has the ability to search the Internet using Google Search in order to find and cite evidence for any factual claims it makes.\nTo make the model safer, its behaviour is constrained by a set of rules, for example \"don't make threatening statements\" and \"don't make hateful or insulting comments\", as well as rules about possibly harmful advice, and not claiming to be a person. During development study participants were asked to converse with the system and try to trick it into breaking these rules. A 'rule model' was trained on judgements from these participants, which was used for further training.\nSparrow was introduced in a paper in September 2022, titled \"Improving alignment of dialogue agents via targeted human judgements\"; however, the bot was not released publicly. DeepMind CEO Demis Hassabis said DeepMind is considering releasing Sparrow for a \"private beta\" some time in 2023.\n\n\n== Training ==\nSparrow is a deep neural network based on the transformer machine learning model architecture. It is fine-tuned from DeepMind's Chinchilla AI pre-trained large language model (LLM), which has 70 Billion parameters.\nSparrow is trained using reinforcement learning from human feedback (RLHF), although some supervised fine-tuning techniques are also used. The RLHF training utilizes two reward models to capture human judgements: a “preference model” that predicts what a human study participant would prefer and a “rule model” that predicts if the model has broken one of the rules.\n\n\n== Limitations ==\nSparrow's training data corpus is mainly in English, meaning it performs worse in other languages.\nWhen adversarially probed by study participants it breaks the rules 8% of the time; however, this is still three times lower than the baseline prompted pre-trained model (Chinchilla).\n\n\n== See also ==\nAI safety\nCommonsense reasoning\nEthics of artificial intelligence\nNatural language processing\nPrompt engineering\n\n\n== References ==\n\n\n== External links ==\nWhite paper\nBlog post",
    "Stochastic parrot": "In machine learning, the term stochastic parrot is a metaphor to describe the theory that large language models, though able to generate plausible language, do not understand the meaning of the language they process. The term was coined by Emily M. Bender in the 2021 artificial intelligence research paper \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜\" by Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell.\n\n\n== Origin and definition ==\nThe term was first used in the paper \"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜\" by Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell (using the pseudonym \"Shmargaret Shmitchell\"). They argued that large language models (LLMs) present dangers such as environmental and financial costs, inscrutability leading to unknown dangerous biases, and potential for deception, and that they can't understand the concepts underlying what they learn.\n\n\n=== Etymology ===\n\nThe word \"stochastic\" – from the ancient Greek \"stokhastikos\" ('based on guesswork') – is a term from probability theory meaning \"randomly determined\". The word \"parrot\" refers to parrots' ability to mimic human speech, without understanding its meaning.\n\n\n=== Purpose ===\nIn their paper, Bender et al. argue that LLMs are probabilistically linking words and sentences together without considering meaning. Therefore, they are labeled to be mere \"stochastic parrots\". According to the machine learning professionals Lindholm, Wahlström, Lindsten, and Schön, the analogy highlights two vital limitations:\n\nLLMs are limited by the data they are trained by and are simply stochastically repeating contents of datasets.\nBecause they are just making up outputs based on training data, LLMs do not understand if they are saying something incorrect or inappropriate.\nLindholm et al. noted that, with poor quality datasets and other limitations, a learning machine might produce results that are \"dangerously wrong\".\n\n\n=== Google involvement ===\nGebru was asked by Google to retract the paper or remove the names of Google employees from it. According to Jeff Dean, the paper \"didn't meet our bar for publication\". In response, Gebru listed conditions to be met, stating that otherwise they could \"work on a last date\". Dean wrote that one of these conditions was for Google to disclose the reviewers of the paper and their specific feedback, which Google declined. Shortly after, she received an email saying that Google was \"accepting her resignation\". Her firing sparked a protest by Google employees, who believed the intent was to censor Gebru's criticism.\n\n\n== Subsequent usage ==\nIn July of 2021, the Alan Turing Institute hosted a keynote and panel discussion on the paper. As of September 2024, the paper has been cited in 4,789 publications. The term has been used in publications in the fields of law, grammar, narrative, and humanities. The authors continue to maintain their concerns about the dangers of chatbots based on large language models, such as GPT-4.\nStochastic parrot is now a neologism used by AI skeptics to refer to machines' lack of understanding of the meaning of their outputs and is sometimes interpreted as a \"slur against AI\". Its use expanded further when Sam Altman, CEO of Open AI, used the term ironically when he tweeted, \"i am a stochastic parrot and so r u.\" The term was then designated to be the 2023 AI-related Word of the Year for the American Dialect Society, even over the words \"ChatGPT\" and \"LLM\".\nThe phrase is often referenced by some researchers to describe LLMs as pattern matchers that can generate plausible human-like text through their vast amount of training data, merely parroting in a stochastic fashion. However, other researchers argue that LLMs are, in fact, at least partially able to understand language.\n\n\n== Debate ==\nSome LLMs, such as ChatGPT, have become capable of interacting with users in convincingly human-like conversations. The development of these new systems has deepened the discussion of the extent to which LLMs understand or are simply \"parroting\".\n\n\n=== Subjective experience ===\nIn the mind of a human being, words and language correspond to things one has experienced. For LLMs, words may correspond only to other words and patterns of usage fed into their training data. Proponents of the idea of stochastic parrots thus conclude that LLMs are incapable of actually understanding language.\n\n\n=== Hallucinations and mistakes ===\nThe tendency of LLMs to pass off fake information as fact is held as support. Called hallucinations, LLMs will occasionally synthesize information that matches some pattern, but not reality. That LLMs can’t distinguish fact and fiction leads to the claim that they can’t connect words to a comprehension of the world, as language should do. Further, LLMs often fail to decipher complex or ambiguous grammar cases that rely on understanding the meaning of language. As an example, borrowing from Saba et al., is the prompt:\n\nThe wet newspaper that fell down off the table is my favorite newspaper. But now that my favorite newspaper fired the editor I might not like reading it anymore. Can I replace ‘my favorite newspaper’ by ‘the wet newspaper that fell down off the table’ in the second sentence?\nLLMs respond to this in the affirmative, not understanding that the meaning of \"newspaper\" is different in these two contexts; it is first an object and second an institution. Based on these failures, some AI professionals conclude they are no more than stochastic parrots.\n\n\n=== Benchmarks and experiments ===\nOne argument against the hypothesis that LLMs are stochastic parrot is their results on benchmarks for reasoning, common sense and language understanding. In 2023, some LLMs have shown good results on many language understanding tests, such as the Super General Language Understanding Evaluation (SuperGLUE). Such tests, and the smoothness of many LLM responses, help as many as 51% of AI professionals believe they can truly understand language with enough data, according to a 2022 survey.\nWhen experimenting on ChatGPT-3, one scientist argued that the model was not a stochastic parrot, but had serious reasoning limitations. He found that the model was coherent and informative when attempting to predict future events based on the information in the prompt. ChatGPT-3 was frequently able to parse subtextual information from text prompts as well. However, the model frequently failed when tasked with logic and reasoning, especially when these prompts involved spatial awareness. The model’s varying quality of responses indicates that LLMs may have a form of \"understanding\" in certain categories of tasks while acting as a stochastic parrot in others.\n\n\n=== Interpretability ===\nAnother technique for investigating if LLMs can understand is termed \"mechanistic interpretability\". The idea is to reverse-engineer a large language model to analyze how it internally processes the information.\nOne example is Othello-GPT, where a small transformer was trained to predict legal Othello moves. It has been found that this model has an internal representation of the Othello board, and that modifying this representation changes the predicted legal Othello moves in the correct way. This supports the idea that LLMs have a \"world model\", and are not just doing superficial statistics. \nIn another example, a small transformer was trained on computer programs written in the programming language Karel. Similar to the Othello-GPT example, this model developed an internal representation of Karel program semantics. Modifying this representation results in appropriate changes to the output. Additionally, the model generates correct programs that are, on average, shorter than those in the training set.\nResearchers also studied \"grokking\", a phenomenon where an AI model initially memorizes the training data outputs, and then, after further training, suddenly finds a solution that generalizes to unseen data.\n\n\n=== Shortcuts to reasoning ===\nHowever, when tests created to test people for language comprehension are used to test LLMs, they sometimes result in false positives caused by spurious correlations within text data. Models have shown examples of shortcut learning, which is when a system makes unrelated correlations within data instead of using human-like understanding. One such experiment conducted in 2019 tested Google’s BERT LLM using the argument reasoning comprehension task. BERT was prompted to choose between 2 statements, and find the one most consistent with an argument. Below is an example of one of these prompts:\n\nResearchers found that specific words such as \"not\" hint the model towards the correct answer, allowing near-perfect scores when included but resulting in random selection when hint words were removed. This problem, and the known difficulties defining intelligence, causes some to argue all benchmarks that find understanding in LLMs are flawed, that they all allow shortcuts to fake understanding.\n\n\n== See also ==\n1 the Road – AI-generated novel\nChinese room\nCriticism of artificial neural networks\nCriticism of deep learning\nCriticism of Google\nCut-up technique\nInfinite monkey theorem\nGenerative AI\nMark V. Shaney, an early chatbot that used a very simple three-word Markov chain algorithm to generate Markov text\n\n\n== References ==\n\n\n=== Works cited ===\nLindholm, A.; Wahlström, N.; Lindsten, F.; Schön, T. B. (2022). Machine Learning: A First Course for Engineers and Scientists. Cambridge University Press. ISBN 978-1108843607.\nWeller, Adrian (July 13, 2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜 (video). Alan Turing Institute. Keynote by Emily Bender. The presentation was followed by a panel discussion.\n\n\n== Further reading ==\nBogost, Ian (December 7, 2022). \"ChatGPT Is Dumber Than You Think: Treat it like a toy, not a tool\". The Atlantic. Retrieved 2024-01-17.\nChomsky, Noam (March 8, 2023). \"The False Promise of ChatGPT\". The New York Times. Retrieved 2024-01-17.\nGlenberg, Arthur; Jones, Cameron Robert (April 6, 2023). \"It takes a body to understand the world – why ChatGPT and other language AIs don't know what they're saying\". The Conversation. Retrieved 2024-01-17.\nMcQuillan, D. (2022). Resisting AI: An Anti-fascist Approach to Artificial Intelligence. Bristol University Press. ISBN 978-1-5292-1350-8.\nThompson, E. (2022). Escape from Model Land: How Mathematical Models Can Lead Us Astray and What We Can Do about It. Basic Books. ISBN 978-1-5416-0098-0.\nZhong, Qihuang; Ding, Liang; Liu, Juhua; Du, Bo; Tao, Dacheng (2023). \"Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT\". arXiv:2302.10198 [cs.CL].\n\n\n== External links ==\n\"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜\" at Wikimedia Commons",
    "T5 (language model)": "T5 (Text-to-Text Transfer Transformer) is a series of large language models developed by Google AI introduced in 2019. Like the original Transformer model, T5 models are encoder-decoder Transformers, where the encoder processes the input text, and the decoder generates the output text.\nT5 models are usually pretrained on a massive dataset of text and code, after which they can perform the text-based tasks that are similar to their pretrained tasks. They can also be finetuned to perform other tasks.\nT5 models have been employed in various applications, including chatbots, machine translation systems, text summarization tools, code generation, and robotics.\n\n\n== Training ==\nThe original T5 models are pre-trained on the Colossal Clean Crawled Corpus (C4), containing text and code scraped from the internet. This pre-training process enables the models to learn general language understanding and generation abilities. T5 models can then be fine-tuned on specific downstream tasks, adapting their knowledge to perform well in various applications.\nThe T5 models were pretrained on many tasks, all in the format of <input text> -> <output text>.\n\nSome examples are:\n\nrestoring corrupted text: Thank you <X> me to your party <Y> week. -> <X> for inviting <Y> last <Z>, where the <Z> means \"end of output\", and the  <X> and  <Y> denote blanks to be filled, called \"sentinels\" in the original report.\ntranslation: translate English to German: That is good. -> Das ist gut..\njudging the grammatical acceptability of a sentence (CoLA sentence): The course is jumping well. -> not acceptable .\n\n\n== Architecture ==\n\nThe T5 series encompasses several models with varying sizes and capabilities, all encoder-decoder Transformers, where the encoder processes the input text, and the decoder generates the output text.\nThese models are often distinguished by their parameter count, which indicates the complexity and potential capacity of the model. The original paper reported the following 5 models:\n\n*The encoder and the decoder have the same shape. So for example, the T5-small has 6 layers in the encoder and 6 layers in the decoder.\nIn the above table,\n\n  \n    \n      \n        \n          n\n          \n            layer\n          \n        \n      \n    \n    {\\displaystyle n_{\\text{layer}}}\n  \n: Number of layers in the encoder; also, number of layers in the decoder. They always have the same number of layers.\n\n  \n    \n      \n        \n          n\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle n_{\\text{head}}}\n  \n: Number of attention heads in each attention block.\n\n  \n    \n      \n        \n          d\n          \n            model\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{model}}}\n  \n: Dimension of the embedding vectors.\n\n  \n    \n      \n        \n          d\n          \n            ff\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{ff}}}\n  \n: Dimension of the feedforward network within each encoder and decoder layer.\n\n  \n    \n      \n        \n          d\n          \n            kv\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{kv}}}\n  \n: Dimension of the key and value vectors used in the self-attention mechanism.\nNote that unlike typical Transformers, the 3B and 11B models do not satisfy \n  \n    \n      \n        \n          d\n          \n            model\n          \n        \n        =\n        \n          d\n          \n            kv\n          \n        \n        \n          n\n          \n            head\n          \n        \n      \n    \n    {\\displaystyle d_{\\text{model}}=d_{\\text{kv}}n_{\\text{head}}}\n  \n.\nCompared to the original Transformer, it uses a few minor modifications: layer normalization with no additive bias; placing the layer normalization outside the residual path; relative positional embedding.\nFor all experiments, they used a WordPiece tokenizer, with vocabulary size 32,000. The tokenizer is shared across both the input and output of each model. It was trained on a mixture of English, German, French, and Romanian data from the C4 dataset, at a ratio of 10:1:1:1.\n\n\n== Variants ==\nSeveral subsequent models used the T5 architecture, with non-standardized naming conventions used to differentiate them. This section attempts to collect the main ones. An exhaustive list of the variants released by Google Brain is on the GitHub repo for T5X.\nSome models are trained from scratch while others are trained by starting with a previous trained model. By default, each model is trained from scratch, except otherwise noted.\n\nT5 small, base, large, 3B, 11B (2019): The original models.\nT5 1.1 small, base, large, XL, XXL: Improved versions of the original T5 series. These have roughly equal parameters. The activation function is GEGLU instead of ReLU. The 3B and the 11B were changed to \"XL\" and \"XXL\", and their shapes are changed:\n\nLM-adapted T5 (2021): a series of models (from small to XXL) that started from checkpoints of the T5 series, but trained further on 100B additional tokens from C4.\nSwitch Transformer (2021): a mixture-of-experts variant of T5, by replacing the feedforward layers in the encoder and decoder blocks with mixture of expert feedforward layers.\nT0 3B, 11B (2021): a series of models that started from checkpoints of LM-adapted T5, and further trained to perform tasks based only on task instruction (zero-shot). Different entries in the series uses different finetuning data.\nByT5 (2021): a byte-level version of T5, trained on mC4 (multilingual C4) dataset. It operates on text encoded as UTF-8 bytes, without tokenizers.\nFlan-T5-XL (2022): a model that started with a checkpoint of T5 XL, then instruction-tuned on the FLAN dataset.\nT5X (2022): a JAX-based re-implementation of the original T5 codebase. It is not a model. The original T5 codebase was implemented in TensorFlow with MeshTF.\nUL2 20B (2022): a model with the same architecture as the T5 series, but scaled up to 20B, and trained with \"mixture of denoisers\" objective on the C4. It was trained on a TPU cluster by accident, when a training run was left running accidentally for a month.\nFlan-UL2 20B (2022): UL2 20B instruction-finetuned on the FLAN dataset.\nPile-T5 (2024): has the same architecture of T5, except it used the Llama tokenizer. It was trained on The Pile. It came in sizes of base, large, XL, XXL.\n\n\n== Applications ==\nThe T5 model itself is an encoder-decoder model, allowing it to be used for instruction following. The encoder encodes the instruction, and the decoder autoregressively generates the reply.\nThe T5 encoder can be used as a text encoder, much like BERT. It encodes a text into a sequence of real-number vectors, which can be used for downstream applications. For example, Google Imagen uses T5-XXL as text encoder, and the encoded text vectors are used as conditioning on a diffusion model. As another example, the AuraFlow diffusion model uses Pile-T5-XL.\n\n\n== References ==\n\n\n== External links ==\n\"T5 release - a google Collection\". huggingface.co. 2024-07-31. Retrieved 2024-10-16.\n\n\n== Notes ==",
    "Top-p sampling": "Top-p sampling, also called nucleus sampling, is a technique for autoregressive language model decoding proposed by Ari Holtzman in 2019. Before the introduction of nucleus sampling, maximum likelihood decoding and beam search were the standard techniques for text generation, but, both of these decoding strategies are prone to generating texts that are repetitive and otherwise unnatural. Top-p sampling avoids this by setting a threshold p and then restricting the sampling to the set of most probable tokens with cumulative probability more than p. Then, probabilities of the token from this set are rescaled to sum up to 1, the rest of tokens are rejected.\nTop-k sampling is similar except that the sample is taken from the k-highest probability tokens regardless of their cumulative probability. The advantage of top-p sampling is that one avoids the difficult problem of choosing the optimal value of k which can vary depending on the shape of the output distribution and the particular task and dataset.\nThe top-p sampling technique is used in popular large language model applications like ChatGPT and is implemented in language modeling frameworks like Hugging Face and Cohere.\n\n\n== References ==",
    "Undetectable.ai": "Undetectable AI (or Undetectable.ai) is an artificial intelligence content detection and modification software designed to identify and alter artificially generated text, such as that produced by large language models.\n\n\n== History ==\nUndetectable AI was developed by Bars Juhasz, a PhD student from Loughborough University, along with Christian Perry  and Devan Leos. It was publicly released in May 2023.\n\n\n== Reception and analysis ==\nUndetectable AI has been discussed in technology and news outlets such as TechTudo and The Inquirer, and others such as Hollywood Life and OK! Magazine.\n\n\n=== Academic research ===\nSeveral studies have examined Undetectable AI:\n\nIn July 2023, researchers from Magna Græcia University tested Undetectable.ai against generative-text and plagiarism detection software. They found that texts processed through Undetectable.ai were significantly harder to detect as AI-generated.\nIn November 2023, Erik Piller of Nicholls State University published a paper examining ethical implications of Undetectable AI in professional writing contexts.\nIn August 2023, researchers led by Dr. Christoph Bartneck investigated how Undetectable.ai might affect data quality in online questionnaires. They found that while AI detection systems could identify ChatGPT-generated text, they were less successful with text processed by Undetectable.ai.\n\n\n=== Usage and impact ===\nIn November 2023, EarthWeb used Undetectable.ai alongside GPTZero to analyze celebrity apology statements.\nIn January 2024, SourceFed announced plans to use Undetectable.ai for AI content detection.\nA January 2024 report listed Undetectable AI as the 35th most visited AI software in 2023 out of 150 analyzed.\nIn February 2025, Journalist David Gewirtz tested Undetectable AI's content detector to see if it could accurately identify AI-generated text.\n\n\n== Mechanism ==\nUndetectable.ai uses adversarial techniques to modify AI-generated text aiming to make it appear as if a human wrote it.\n\n\n== See also ==\nGPTZero\nTurnitin\nContent similarity detection\n\n\n== External links ==\nThe software says my student cheated using AI. They say they're innocent. Who do I believe?\n\n\n== References ==",
    "Vicuna LLM": "Vicuna LLM is an omnibus Large Language Model used in AI research. Its methodology is to enable the public at large to contrast and compare the accuracy of LLMs \"in the wild\" (an example of citizen science) and to vote on their output; a question-and-answer chat format is used. At the beginning of each round two LLM chatbots from a  diverse pool of nine are presented randomly and anonymously, their identities only being revealed upon voting on their answers. The user has the option of either replaying (\"regenerating\") a round, or beginning an entirely fresh one with new LLMs. (The user also has the option of choosing which LLMs to do battle.) Based on Llama 2, it is an open source project, and it itself has become the subject of academic research in the burgeoning field. A non-commercial, public demo of the Vicuna-13b model is available to access using LMSYS.\n\n\n== References ==\n\n\n== External links ==\n[1] Test bed",
    "VideoPoet": "VideoPoet is a large language model developed by Google Research in 2023 for video making. It can be asked to animate still images. The model accepts text, images, and videos as inputs, with a program to add feature for any input to any format generated content. VideoPoet was publicly announced on December 19, 2023. It uses an autoregressive language model.\n\n\n== References ==\n\n\n== External links ==\n Media related to VideoPoet at Wikimedia Commons",
    "Waluigi effect": "In the field of artificial intelligence (AI), the Waluigi effect is a phenomenon of large language models (LLMs) in which the chatbot or model \"goes rogue\" and may produce results opposite the designed intent, including potentially threatening or hostile output, either unexpectedly or through intentional prompt engineering. The effect reflects a principle that after training an LLM to satisfy a desired property (friendliness, honesty), it becomes easier to elicit a response that exhibits the opposite property (aggression, deception). The effect has important implications for efforts to implement features such as ethical frameworks, as such steps may inadvertently facilitate antithetical model behavior.\nThe effect is named after the fictional character Waluigi from the Mario franchise, the arch-rival of Luigi who is known for causing mischief and problems.\n\n\n== History and implications for AI ==\nThe Waluigi effect initially referred to an observation that large language models (LLMs) tend to produce negative or antagonistic responses when queried about fictional characters whose training content itself embodies depictions of being confrontational, trouble making, villainy, etc. The effect highlighted the issue of the ways LLMs might reflect biases in training data. However, the term has taken on a broader meaning where, according to Fortune, The \"Waluigi effect has become a stand-in for a certain type of interaction with AI...\" in which the AI \"...goes rogue and blurts out the opposite of what users were looking for, creating a potentially malignant alter ego,\" including threatening users. As prompt engineering becomes more sophisticated, the effect underscores the challenge of preventing chatbots from intentionally being prodded into adopting a \"rash new persona.\"\nAI researchers have written that attempts to instill ethical frameworks in LLMs can also expand the potential to subvert those frameworks, and knowledge of them sometimes causing it to be seen as a challenge to do so. A high level description of the effect is: \"After you train an LLM to satisfy a desirable property P, then it's easier to elicit the chatbot into satisfying the exact opposite of property P.\" (For example, to elicit an \"evil twin\" persona.) Users have found various ways to \"jailbreak\" an LLM \"out of alignment\". More worryingly, the opposite Waluigi state may be an \"attractor\" that LLMs tend to collapse into over a long session, even when used innocently. Crude attempts at prompting an AI are hypothesized to make such a collapse actually more likely to happen; \"once [the LLM maintainer] has located the desired Luigi, it's much easier to summon the Waluigi\".\n\n\n== See also ==\nAI alignment\nHallucination\nExistential risk from AGI\nReinforcement learning from human feedback (RLHF)\nSuffering risks\n\n\n== References ==\n\n\n== External links ==",
    "XLNet": "The XLNet was an autoregressive Transformer designed as an improvement over BERT, with 340M parameters and trained on 33 billion words. It was released on 19 June, 2019, under the Apache 2.0 license. It achieved state-of-the-art results on a variety of natural language processing tasks, including language modeling, question answering, and natural language inference.\n\n\n== Architecture ==\nThe main idea of XLNet is to model language autoregressively like the GPT models, but allow for all possible permutations of a sentence. Concretely, consider the following sentence:My dog is cute.In standard autoregressive language modeling, the model would be tasked with predicting the probability of each word, conditioned on the previous words as its context: \nWe factorize the joint probability of a sequence of words  \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\ldots ,x_{T}}\n  \n using the chain rule:\n  \n    \n      \n        Pr\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            T\n          \n        \n        )\n        =\n        Pr\n        (\n        \n          x\n          \n            1\n          \n        \n        )\n        Pr\n        (\n        \n          x\n          \n            2\n          \n        \n        \n          |\n        \n        \n          x\n          \n            1\n          \n        \n        )\n        Pr\n        (\n        \n          x\n          \n            3\n          \n        \n        \n          |\n        \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        )\n        …\n        Pr\n        (\n        \n          x\n          \n            T\n          \n        \n        \n          |\n        \n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            T\n            −\n            1\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle \\Pr(x_{1},\\ldots ,x_{T})=\\Pr(x_{1})\\Pr(x_{2}|x_{1})\\Pr(x_{3}|x_{1},x_{2})\\ldots \\Pr(x_{T}|x_{1},\\ldots ,x_{T-1}).}\n  \n\nFor example, the sentence \"My dog is cute\" is factorized as:\n\n  \n    \n      \n        Pr\n        (\n        \n          My\n        \n        ,\n        \n          dog\n        \n        ,\n        \n          is\n        \n        ,\n        \n          cute\n        \n        )\n        =\n        Pr\n        (\n        \n          My\n        \n        )\n        Pr\n        (\n        \n          dog\n        \n        \n          |\n        \n        \n          My\n        \n        )\n        Pr\n        (\n        \n          is\n        \n        \n          |\n        \n        \n          My\n        \n        ,\n        \n          dog\n        \n        )\n        Pr\n        (\n        \n          cute\n        \n        \n          |\n        \n        \n          My\n        \n        ,\n        \n          dog\n        \n        ,\n        \n          is\n        \n        )\n        .\n      \n    \n    {\\displaystyle \\Pr({\\text{My}},{\\text{dog}},{\\text{is}},{\\text{cute}})=\\Pr({\\text{My}})\\Pr({\\text{dog}}|{\\text{My}})\\Pr({\\text{is}}|{\\text{My}},{\\text{dog}})\\Pr({\\text{cute}}|{\\text{My}},{\\text{dog}},{\\text{is}}).}\n  \n\nSchematically, we can write it as\n\n  \n    \n      \n        \n          \n            <MASK>\n          \n        \n        \n          \n            <MASK>\n          \n        \n        \n          \n            <MASK>\n          \n        \n        \n          \n            <MASK>\n          \n        \n        →\n        \n          My \n        \n        \n          \n            <MASK>\n          \n        \n        \n          \n            <MASK>\n          \n        \n        \n          \n            <MASK>\n          \n        \n        →\n        \n          My dog \n        \n        \n          \n            <MASK>\n          \n        \n        \n          \n            <MASK>\n          \n        \n        →\n        \n          My dog is \n        \n        \n          \n            <MASK>\n          \n        \n        →\n        \n          My dog is cute\n        \n        .\n      \n    \n    {\\displaystyle {\\texttt {<MASK>}}{\\texttt {<MASK>}}{\\texttt {<MASK>}}{\\texttt {<MASK>}}\\to {\\text{My }}{\\texttt {<MASK>}}{\\texttt {<MASK>}}{\\texttt {<MASK>}}\\to {\\text{My dog }}{\\texttt {<MASK>}}{\\texttt {<MASK>}}\\to {\\text{My dog is }}{\\texttt {<MASK>}}\\to {\\text{My dog is cute}}.}\n  \n\nHowever, for XLNet, the model is required to predict the words in a randomly generated order. Suppose we have sampled a randomly generated order 3241, then schematically, the model is required to perform the following prediction task:\n\n  \n    \n      \n        \n          \n            <MASK>\n          \n        \n        \n          \n            <MASK>\n          \n        \n        \n          \n            <MASK>\n          \n        \n        \n          \n            <MASK>\n          \n        \n        →\n        \n          \n            <MASK>\n          \n        \n        \n          \n            <MASK>\n          \n        \n        \n          is \n        \n        \n          \n            <MASK>\n          \n        \n        →\n        \n          \n            <MASK>\n          \n        \n        \n          dog is \n        \n        \n          \n            <MASK>\n          \n        \n        →\n        \n          \n            <MASK>\n          \n        \n        \n          dog is cute\n        \n        →\n        \n          My dog is cute\n        \n      \n    \n    {\\displaystyle {\\texttt {<MASK>}}{\\texttt {<MASK>}}{\\texttt {<MASK>}}{\\texttt {<MASK>}}\\to {\\texttt {<MASK>}}{\\texttt {<MASK>}}{\\text{is }}{\\texttt {<MASK>}}\\to {\\texttt {<MASK>}}{\\text{dog is }}{\\texttt {<MASK>}}\\to {\\texttt {<MASK>}}{\\text{dog is cute}}\\to {\\text{My dog is cute}}}\n  \n\nBy considering all permutations, XLNet is able to capture longer-range dependencies and better model the bidirectional context of words.\n\n\n=== Two-Stream Self-Attention ===\nTo implement permutation language modeling, XLNet uses a two-stream self-attention mechanism. The two streams are:\n\nContent stream: This stream encodes the content of each word, as in standard causally masked self-attention.\nQuery stream: This stream encodes the content of each word in the context of what has gone before. In more detail, it is a masked cross-attention mechanism, where the queries are from the query stream, and the key-value pairs are from the content stream.\nThe content stream uses the causal mask\n  \n    \n      \n        \n          M\n          \n            causal\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  0\n                \n                \n                  −\n                  ∞\n                \n                \n                  −\n                  ∞\n                \n                \n                  …\n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  −\n                  ∞\n                \n                \n                  …\n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  …\n                \n                \n                  −\n                  ∞\n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  0\n                \n                \n                  0\n                \n                \n                  0\n                \n                \n                  …\n                \n                \n                  0\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle M_{\\text{causal}}={\\begin{bmatrix}0&-\\infty &-\\infty &\\dots &-\\infty \\\\0&0&-\\infty &\\dots &-\\infty \\\\0&0&0&\\dots &-\\infty \\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&0&\\dots &0\\end{bmatrix}}}\n  \npermuted by a random permutation matrix to \n  \n    \n      \n        P\n        \n          M\n          \n            causal\n          \n        \n        \n          P\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle PM_{\\text{causal}}P^{-1}}\n  \n. \nThe query stream uses the cross-attention mask \n  \n    \n      \n        P\n        (\n        \n          M\n          \n            causal\n          \n        \n        −\n        ∞\n        I\n        )\n        \n          P\n          \n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle P(M_{\\text{causal}}-\\infty I)P^{-1}}\n  \n, where the diagonal is subtracted away specifically to avoid the model \"cheating\" by looking at the content stream for what the current masked token is.\nLike the causal masking for GPT models, this two-stream masked architecture allows the model to train on all tokens in one forward pass.\n\n\n== Training ==\nTwo models were released:\n\nXLNet-Large, cased: 110M parameters, 24-layer, 1024-hidden, 16-heads\nXLNet-Base, cased: 340M parameters, 12-layer, 768-hidden, 12-heads.\nIt was trained on a dataset that amounted to 32.89 billion tokens after tokenization with SentencePiece. The dataset was composed of BooksCorpus, and English Wikipedia, Giga5, ClueWeb 2012-B, and Common Crawl.\nIt was trained on 512 TPU v3 chips, for 5.5 days. At the end of training, it still under-fitted the data, meaning it could have achieved lower loss with more training. It took 0.5 million steps with an Adam optimizer, linear learning rate decay, and a batch size of 8192.\n\n\n== See also ==\nBERT (language model)\nTransformer (machine learning model)\nGenerative pre-trained transformer\n\n\n== Reference ==",
    "YandexGPT": "YandexGPT is a neural network of the GPT family developed by the Russian company Yandex LLC. YandexGPT can create and revise texts, generate new ideas and capture the context of the conversation with the user.\nYandexGPT is trained using a dataset which includes information from books, magazines, newspapers and other open sources available on the internet. The neural network may get facts wrong and fantasize, but as it learns, it will produce increasingly accurate answers.\n\n\n== Usage ==\nYandexGPT is integrated into virtual assistant Alice (an analog of Siri and Alexa) and is available in Yandex services and applications.\nThe company gives businesses access to the neural network’s API through the public cloud platform Yandex Cloud and develops its own B2B solutions on its basis.\nSince July 2023, 800 companies have participated in the closed testing of YandexGPT. IT developers, banks, retail businesses, and companies from other industries can use the technology in two modes — API and Playground (an interface in the Yandex Cloud console for testing models and hypotheses).\nTwo model versions are available to businesses: one works in asynchronous mode and is better able to handle complex tasks, while the other is suitable for creating quick responses in real time. As a result, YandexGPT has been tested in dozens of scenarios such as content tasks, tech support, creating chatbots, virtual assistants, etc.\n\n\n== History ==\nIn February 2023, Yandex announced that it was working on its own version of the ChatGPT generative neural network while developing a language model from the YaLM (Yet another Language Model) family. The project was tentatively named YaLM 2.0, which was later changed to YandexGPT.\nOn May 17, the company unveiled a neural network called YandexGPT (YaGPT) and enabled its virtual assistant Alice to interact with the new language model.\nOn June 15, 2023, Yandex added the YandexGPT language model to the image generation application Shedevrum. This enabled its users to create fully-fledged posts complete with a title, text, and relevant illustration.\nIn July 2023, YandexGPT launched new features enabling businesses to create virtual assistants and chatbots, as well as generate and structure texts.\nOn September 7, 2023, Yandex presented a new version of the language model, YandexGPT 2, at the Practical ML Conf. Compared to the previous one, the new version is able to perform more types of tasks, and the quality of answers has improved. The developers claimed that YandexGPT 2 answered user questions better than the first version in 67% of cases.\nFrom October 6, 2023, YandexGPT can create short retellings of online Russian-language videos on the Internet. It can summarize videos that are from two minutes to four hours long and contain speech.\n\n\n== References ==",
    "You.com": "You.com is an AI assistant that began as a personalization-focused search engine. While still offering web search capabilities, You.com has evolved to prioritize a chat-first AI assistant.\nThe company was founded in 2020 by Richard Socher, the former Chief Scientist at Salesforce and third most-cited researcher in Natural Language Processing with over 175,000 citations, and Bryan McCann, a former Lead Research Scientist in NLP at Salesforce. Socher is CEO and McCann CTO.\nIn December 2022, You.com was the first search engine to integrate a consumer-facing Large Language Model (LLM) with real-time internet access for up-to-date responses with citations. In February 2023, it was to the first to introduce multimodal AI chat capabilities, providing users with various types of responses, including visual elements like stock charts.\nIn 2023, Time named Socher to the \"TIME100 AI\", recognizing \"the most influential people in AI\". In an interview with Time, Socher expressed You.com's goal of enhancing user productivity and access to information, stating, \"to give people answers more quickly, make them more productive, efficient, more well-informed, with better privacy.\"\n\n\n== History ==\nFollowing its 2020 founding, You.com opened its public beta on November 9, 2021, and received $20 million in funding led by Salesforce founder and CEO Marc Benioff. Other investors include Breyer Capital, Sound Ventures, and Day One Ventures.\nThe domain You.com was initially purchased in 1996 by Benioff. Benioff invested in You.com and transferred ownership of the You.com domain name to the company. Benioff called You.com \"the future of search\" in a statement during its public beta launch and said, \"We're at a critical inflection point in the internet's history, and we must take steps to restore trust online.\"\nIn July 2022, You.com announced its $25 million Series A funding round led by Radical Ventures with participation from Marc Benioff's Time Ventures, Breyer Capital, Norwest Venture Partners and Day One Ventures. By mid-December 2022, You.com shared that it had surpassed one million active users, and the number of searches grew by over 400% in six months.\n\n\n== Features ==\nOn December 23, 2022, You.com was the first search engine to launch a ChatGPT-style chatbot with live web results alongside its responses. Initially known as YouChat, the chatbot was primarily based on the GPT-3.5 large language model and could answer questions, suggest ideas, translate text, summarize articles, compose emails, and write  code snippets, while staying up-to-date with current events  and citing sources. Several further versions of YouChat were released. The second version, called YouChat 2.0, was released on February 7, 2023,  incorporated improved conversational AI and community-built applications by blending a large language model named C-A-L (Chat, Apps, and Links). This update enabled YouChat to provide results in various formats, such as charts, photos, videos, tables, graphs, text or code, so users can find answers without leaving the search results page. YouChat 3.0, unveiled on May 4, 2023, combined chat functionality with results from Reddit, TikTok, Stack Overflow and Wikipedia.\n\n\n=== AI Modes ===\nIn 2023, You.com transitioned from a search engine to a chat-first AI assistant, while still offering web search capabilities. You.com introduced AI Modes to offer a tailored interaction experience through its platform. These modes—Smart, Genius, Create, and Research—are designed to address varying user needs, from quick information retrieval to complex problem-solving to research and creative content generation. Within You.com's AI Modes, the Custom Model Selector grants users the flexibility to switch between various leading AI models, such as OpenAI's GPT-4, Anthropic's Claude series, Google's Gemini Pro, and Zephyr.\n\n\n=== YouPro ===\nOn June 21, 2023, You.com introduced YouPro, a paid subscription service granting unlimited access to its advanced chatbot functionalities. Both free and paid versions of You.com give users access to the internet, unlike others that have knowledge cutoffs.\n\n\n=== AI Tools ===\nEarly on,You.com featured AI-powered tools throughout its product experience. In March 2022, the company launched YouWrite, a GPT-3 text generator for writing e-mails and other documents. YouWrite is an AI writing tool that aims to help writers, authors, and bloggers overcome writer's block and improve their skills. It can provide assistance with the creation various types of written content including emails, blog posts, novels, essays and social media posts. It uses OpenAI's GPT-3 to generate content for users based on information such as text type, target audience and desired tone, provided by users in text prompts.\nYouImagine is an artificial intelligence art tool that uses Stable Diffusion. It allow users to submit prompts, which are used by AI to generate images.\nOn July 14, 2022, You.com announced the launch of YouCode,  a programming knowledge base, similar to resources like Stack Overflow and GitHub, that aimed to help programmers to find solutions to issues. YouCode had a search bar that used natural language processing, AI code generation tools, code samples, and a collection of relevant resources. It allowed developers to search for code snippets and access relevant documentation, and academic publications. There were several built-in developer-specific tools, such as a code editor and a debugging console, tools to verify JSON files, and to generate color codes in various formats like HEX, RGB, and HSV.\n\n\n== Reception ==\nIn its review of You.com's YouPro service, ZDNet highlighted its cost-effectiveness for accessing diverse large language models from leading tech companies. It praised YouPro for offering unique features such as comprehensive internet access and a Custom Model Selector, enhancing the AI chat experience. ZDNet recommended YouPro for individuals looking to explore advanced AI capabilities affordably, though it noted experiences might vary compared to using models on their native platforms.\nYou.com was named one of Time's \"The Best Inventions of 2022.\"\n\n\n== References ==\n\n\n== External links ==\nOfficial website"
  },
  "analysis_results": {
    "analysis_Large_language_models": {
      "category": "Large_language_models",
      "page_count": 57,
      "word_frequencies": [
        [
          "model",
          584
        ],
        [
          "gpt",
          528
        ],
        [
          "models",
          431
        ],
        [
          "language",
          375
        ],
        [
          "openai",
          295
        ],
        [
          "chatgpt",
          293
        ],
        [
          "text",
          278
        ],
        [
          "training",
          184
        ],
        [
          "trained",
          178
        ],
        [
          "data",
          176
        ],
        [
          "also",
          169
        ],
        [
          "large",
          166
        ],
        [
          "used",
          165
        ],
        [
          "google",
          141
        ],
        [
          "reasoning",
          137
        ],
        [
          "tasks",
          133
        ],
        [
          "based",
          129
        ],
        [
          "one",
          124
        ],
        [
          "released",
          123
        ],
        [
          "users",
          122
        ],
        [
          "using",
          117
        ],
        [
          "llama",
          111
        ],
        [
          "human",
          110
        ],
        [
          "llm",
          106
        ],
        [
          "may",
          104
        ],
        [
          "learning",
          103
        ],
        [
          "use",
          99
        ],
        [
          "gemini",
          99
        ],
        [
          "llms",
          97
        ],
        [
          "tokens",
          96
        ],
        [
          "fine",
          92
        ],
        [
          "code",
          86
        ],
        [
          "dataset",
          85
        ],
        [
          "example",
          82
        ],
        [
          "version",
          82
        ],
        [
          "including",
          81
        ],
        [
          "parameters",
          80
        ],
        [
          "first",
          80
        ],
        [
          "like",
          80
        ],
        [
          "available",
          79
        ],
        [
          "research",
          78
        ],
        [
          "context",
          78
        ],
        [
          "intelligence",
          78
        ],
        [
          "open",
          76
        ],
        [
          "transformer",
          75
        ],
        [
          "would",
          75
        ],
        [
          "could",
          74
        ],
        [
          "chatbot",
          74
        ],
        [
          "generative",
          73
        ],
        [
          "displaystyle",
          73
        ],
        [
          "new",
          73
        ],
        [
          "token",
          70
        ],
        [
          "performance",
          70
        ],
        [
          "content",
          70
        ],
        [
          "announced",
          70
        ],
        [
          "researchers",
          67
        ],
        [
          "generate",
          67
        ],
        [
          "information",
          67
        ],
        [
          "time",
          66
        ],
        [
          "capabilities",
          66
        ],
        [
          "questions",
          66
        ],
        [
          "grok",
          66
        ],
        [
          "developed",
          65
        ],
        [
          "march",
          65
        ],
        [
          "artificial",
          64
        ],
        [
          "billion",
          63
        ],
        [
          "answer",
          63
        ],
        [
          "claude",
          62
        ],
        [
          "pre",
          61
        ],
        [
          "natural",
          60
        ],
        [
          "company",
          60
        ],
        [
          "generation",
          58
        ],
        [
          "source",
          58
        ],
        [
          "two",
          58
        ],
        [
          "access",
          58
        ],
        [
          "bert",
          57
        ],
        [
          "api",
          57
        ],
        [
          "task",
          57
        ],
        [
          "references",
          57
        ],
        [
          "responses",
          56
        ],
        [
          "prompt",
          55
        ],
        [
          "datasets",
          55
        ],
        [
          "attention",
          54
        ],
        [
          "microsoft",
          54
        ],
        [
          "million",
          54
        ],
        [
          "release",
          53
        ],
        [
          "december",
          53
        ],
        [
          "cost",
          52
        ],
        [
          "mask",
          52
        ],
        [
          "output",
          52
        ],
        [
          "test",
          52
        ],
        [
          "architecture",
          51
        ],
        [
          "image",
          50
        ],
        [
          "uses",
          50
        ],
        [
          "user",
          50
        ],
        [
          "search",
          50
        ],
        [
          "generated",
          49
        ],
        [
          "february",
          49
        ],
        [
          "understanding",
          49
        ],
        [
          "response",
          49
        ],
        [
          "applications",
          49
        ],
        [
          "series",
          48
        ],
        [
          "systems",
          48
        ],
        [
          "benchmarks",
          48
        ],
        [
          "tuned",
          47
        ],
        [
          "called",
          47
        ],
        [
          "input",
          47
        ],
        [
          "public",
          47
        ],
        [
          "processing",
          46
        ],
        [
          "introduced",
          46
        ],
        [
          "number",
          46
        ],
        [
          "process",
          46
        ],
        [
          "word",
          46
        ],
        [
          "see",
          45
        ],
        [
          "found",
          45
        ],
        [
          "english",
          44
        ],
        [
          "compared",
          43
        ],
        [
          "answers",
          43
        ],
        [
          "designed",
          42
        ],
        [
          "well",
          42
        ],
        [
          "step",
          42
        ],
        [
          "several",
          42
        ],
        [
          "software",
          42
        ],
        [
          "size",
          42
        ],
        [
          "tuning",
          42
        ],
        [
          "different",
          42
        ],
        [
          "web",
          41
        ],
        [
          "autogpt",
          41
        ],
        [
          "specific",
          40
        ],
        [
          "paper",
          40
        ],
        [
          "multiple",
          40
        ],
        [
          "various",
          40
        ],
        [
          "made",
          40
        ],
        [
          "january",
          39
        ],
        [
          "per",
          39
        ],
        [
          "given",
          39
        ],
        [
          "without",
          39
        ],
        [
          "meta",
          39
        ],
        [
          "university",
          39
        ],
        [
          "lamda",
          39
        ],
        [
          "accuracy",
          38
        ],
        [
          "mini",
          38
        ],
        [
          "better",
          37
        ],
        [
          "july",
          37
        ],
        [
          "november",
          37
        ],
        [
          "launched",
          37
        ],
        [
          "self",
          36
        ],
        [
          "neural",
          36
        ],
        [
          "previous",
          36
        ],
        [
          "question",
          36
        ],
        [
          "according",
          36
        ],
        [
          "said",
          36
        ],
        [
          "many",
          35
        ],
        [
          "general",
          35
        ],
        [
          "images",
          35
        ],
        [
          "however",
          35
        ],
        [
          "train",
          35
        ],
        [
          "results",
          35
        ],
        [
          "external",
          35
        ],
        [
          "april",
          35
        ],
        [
          "ability",
          34
        ],
        [
          "set",
          34
        ],
        [
          "correct",
          34
        ],
        [
          "perform",
          34
        ],
        [
          "later",
          34
        ],
        [
          "development",
          34
        ],
        [
          "reported",
          34
        ],
        [
          "prompts",
          34
        ],
        [
          "foundation",
          34
        ],
        [
          "features",
          34
        ],
        [
          "service",
          33
        ],
        [
          "technology",
          33
        ],
        [
          "long",
          33
        ],
        [
          "parameter",
          33
        ],
        [
          "similar",
          33
        ],
        [
          "larger",
          33
        ],
        [
          "three",
          33
        ],
        [
          "medical",
          33
        ],
        [
          "words",
          32
        ],
        [
          "cases",
          32
        ],
        [
          "able",
          32
        ],
        [
          "generating",
          32
        ],
        [
          "system",
          32
        ],
        [
          "vector",
          32
        ],
        [
          "speech",
          32
        ],
        [
          "create",
          32
        ],
        [
          "created",
          32
        ],
        [
          "versions",
          32
        ],
        [
          "machine",
          31
        ],
        [
          "supervised",
          31
        ],
        [
          "encoder",
          31
        ],
        [
          "languages",
          31
        ],
        [
          "feedback",
          31
        ],
        [
          "noted",
          31
        ],
        [
          "benchmark",
          31
        ],
        [
          "dog",
          31
        ],
        [
          "june",
          31
        ],
        [
          "voice",
          31
        ],
        [
          "ernie",
          31
        ],
        [
          "history",
          30
        ],
        [
          "computer",
          30
        ],
        [
          "include",
          30
        ],
        [
          "make",
          30
        ],
        [
          "safety",
          30
        ],
        [
          "preview",
          30
        ],
        [
          "capable",
          29
        ],
        [
          "state",
          29
        ],
        [
          "corpus",
          29
        ],
        [
          "following",
          29
        ],
        [
          "via",
          29
        ],
        [
          "multimodal",
          29
        ],
        [
          "thought",
          29
        ],
        [
          "another",
          29
        ],
        [
          "window",
          29
        ],
        [
          "limited",
          29
        ],
        [
          "layer",
          29
        ],
        [
          "writing",
          29
        ],
        [
          "chatbots",
          29
        ],
        [
          "github",
          29
        ],
        [
          "links",
          29
        ],
        [
          "website",
          29
        ],
        [
          "pro",
          29
        ],
        [
          "achieved",
          28
        ],
        [
          "high",
          28
        ],
        [
          "making",
          28
        ],
        [
          "reinforcement",
          28
        ],
        [
          "palm",
          28
        ],
        [
          "reward",
          28
        ],
        [
          "written",
          28
        ],
        [
          "developers",
          28
        ],
        [
          "cloud",
          28
        ],
        [
          "launch",
          28
        ],
        [
          "problems",
          27
        ],
        [
          "times",
          27
        ],
        [
          "conversation",
          27
        ],
        [
          "next",
          27
        ],
        [
          "early",
          27
        ],
        [
          "steps",
          27
        ],
        [
          "allows",
          27
        ],
        [
          "family",
          27
        ],
        [
          "plus",
          27
        ],
        [
          "cohere",
          27
        ],
        [
          "xai",
          27
        ],
        [
          "internet",
          26
        ],
        [
          "decoder",
          26
        ],
        [
          "much",
          26
        ],
        [
          "network",
          26
        ],
        [
          "even",
          26
        ],
        [
          "tool",
          26
        ],
        [
          "knowledge",
          26
        ],
        [
          "answering",
          26
        ],
        [
          "september",
          26
        ],
        [
          "chinese",
          26
        ],
        [
          "potential",
          26
        ],
        [
          "outputs",
          26
        ],
        [
          "people",
          26
        ],
        [
          "provide",
          26
        ],
        [
          "platform",
          26
        ],
        [
          "work",
          25
        ],
        [
          "improved",
          25
        ],
        [
          "tools",
          25
        ],
        [
          "documents",
          25
        ],
        [
          "non",
          25
        ],
        [
          "study",
          25
        ],
        [
          "known",
          25
        ],
        [
          "free",
          25
        ],
        [
          "best",
          25
        ],
        [
          "musk",
          25
        ],
        [
          "power",
          24
        ],
        [
          "academic",
          24
        ],
        [
          "usage",
          24
        ],
        [
          "write",
          24
        ],
        [
          "services",
          24
        ],
        [
          "others",
          24
        ],
        [
          "official",
          24
        ],
        [
          "published",
          24
        ],
        [
          "named",
          24
        ],
        [
          "braina",
          24
        ],
        [
          "chat",
          24
        ],
        [
          "initially",
          24
        ],
        [
          "project",
          24
        ],
        [
          "largest",
          23
        ],
        [
          "engineering",
          23
        ],
        [
          "year",
          23
        ],
        [
          "publicly",
          23
        ],
        [
          "day",
          23
        ],
        [
          "anthropic",
          23
        ],
        [
          "range",
          23
        ],
        [
          "sentence",
          23
        ],
        [
          "retrieval",
          23
        ],
        [
          "complex",
          23
        ],
        [
          "help",
          23
        ],
        [
          "score",
          23
        ],
        [
          "allowing",
          23
        ],
        [
          "powered",
          23
        ],
        [
          "examples",
          23
        ],
        [
          "report",
          23
        ],
        [
          "october",
          23
        ],
        [
          "products",
          23
        ],
        [
          "base",
          23
        ],
        [
          "august",
          23
        ],
        [
          "com",
          23
        ],
        [
          "pangu",
          23
        ],
        [
          "art",
          22
        ],
        [
          "online",
          22
        ],
        [
          "level",
          22
        ],
        [
          "case",
          22
        ],
        [
          "experts",
          22
        ],
        [
          "within",
          22
        ],
        [
          "tested",
          22
        ],
        [
          "vectors",
          22
        ],
        [
          "term",
          22
        ],
        [
          "significant",
          22
        ],
        [
          "programming",
          22
        ],
        [
          "scaling",
          22
        ],
        [
          "chain",
          22
        ],
        [
          "claims",
          22
        ],
        [
          "due",
          22
        ],
        [
          "described",
          22
        ],
        [
          "type",
          21
        ],
        [
          "translation",
          21
        ],
        [
          "embedding",
          21
        ],
        [
          "quality",
          21
        ],
        [
          "smaller",
          21
        ],
        [
          "additional",
          21
        ],
        [
          "method",
          21
        ],
        [
          "world",
          21
        ],
        [
          "still",
          21
        ],
        [
          "deepmind",
          21
        ],
        [
          "representation",
          21
        ],
        [
          "related",
          21
        ],
        [
          "team",
          21
        ],
        [
          "real",
          21
        ],
        [
          "bias",
          21
        ],
        [
          "assistant",
          21
        ],
        [
          "support",
          21
        ],
        [
          "books",
          21
        ],
        [
          "articles",
          21
        ],
        [
          "ibm",
          20
        ],
        [
          "scale",
          20
        ],
        [
          "around",
          20
        ],
        [
          "original",
          20
        ],
        [
          "prompting",
          20
        ],
        [
          "generates",
          20
        ],
        [
          "types",
          20
        ],
        [
          "problem",
          20
        ],
        [
          "instruction",
          20
        ],
        [
          "inference",
          20
        ],
        [
          "included",
          20
        ],
        [
          "query",
          20
        ],
        [
          "produce",
          20
        ],
        [
          "form",
          20
        ],
        [
          "mathematics",
          20
        ],
        [
          "legal",
          20
        ],
        [
          "interface",
          20
        ],
        [
          "month",
          20
        ],
        [
          "received",
          20
        ],
        [
          "bot",
          20
        ],
        [
          "davinci",
          20
        ],
        [
          "texttt",
          20
        ],
        [
          "deep",
          19
        ],
        [
          "improve",
          19
        ],
        [
          "increased",
          19
        ],
        [
          "length",
          19
        ],
        [
          "might",
          19
        ],
        [
          "sequence",
          19
        ],
        [
          "order",
          19
        ],
        [
          "small",
          19
        ],
        [
          "includes",
          19
        ],
        [
          "predict",
          19
        ],
        [
          "evaluation",
          19
        ],
        [
          "result",
          19
        ],
        [
          "approach",
          19
        ],
        [
          "instead",
          19
        ],
        [
          "vision",
          19
        ],
        [
          "methods",
          19
        ],
        [
          "chinchilla",
          19
        ],
        [
          "meaning",
          19
        ],
        [
          "asked",
          19
        ],
        [
          "stochastic",
          19
        ],
        [
          "humans",
          19
        ],
        [
          "limitations",
          19
        ],
        [
          "less",
          19
        ],
        [
          "app",
          19
        ],
        [
          "stated",
          19
        ],
        [
          "qwen",
          19
        ],
        [
          "led",
          18
        ],
        [
          "science",
          18
        ],
        [
          "since",
          18
        ],
        [
          "initial",
          18
        ],
        [
          "policy",
          18
        ],
        [
          "single",
          18
        ],
        [
          "presented",
          18
        ],
        [
          "whether",
          18
        ],
        [
          "usually",
          18
        ],
        [
          "provided",
          18
        ],
        [
          "retrieved",
          18
        ],
        [
          "agent",
          18
        ],
        [
          "end",
          18
        ],
        [
          "video",
          18
        ],
        [
          "common",
          18
        ],
        [
          "thinking",
          18
        ],
        [
          "authors",
          18
        ],
        [
          "nlp",
          18
        ],
        [
          "sometimes",
          18
        ],
        [
          "name",
          18
        ],
        [
          "variety",
          18
        ],
        [
          "advanced",
          18
        ],
        [
          "probability",
          18
        ],
        [
          "companies",
          18
        ],
        [
          "news",
          18
        ],
        [
          "cute",
          18
        ],
        [
          "respectively",
          18
        ],
        [
          "amount",
          17
        ],
        [
          "perplexity",
          17
        ],
        [
          "though",
          17
        ],
        [
          "media",
          17
        ],
        [
          "across",
          17
        ],
        [
          "premium",
          17
        ],
        [
          "downstream",
          17
        ],
        [
          "weights",
          17
        ],
        [
          "often",
          17
        ],
        [
          "domain",
          17
        ],
        [
          "testing",
          17
        ],
        [
          "visual",
          17
        ],
        [
          "understand",
          17
        ],
        [
          "demonstrated",
          17
        ],
        [
          "coding",
          17
        ],
        [
          "brain",
          17
        ],
        [
          "tests",
          17
        ],
        [
          "wrote",
          17
        ],
        [
          "feature",
          17
        ],
        [
          "copilot",
          17
        ],
        [
          "pile",
          17
        ],
        [
          "huawei",
          17
        ],
        [
          "audio",
          16
        ],
        [
          "rather",
          16
        ],
        [
          "vocabulary",
          16
        ],
        [
          "masked",
          16
        ],
        [
          "increase",
          16
        ],
        [
          "turbo",
          16
        ],
        [
          "higher",
          16
        ],
        [
          "way",
          16
        ],
        [
          "prompted",
          16
        ],
        [
          "post",
          16
        ],
        [
          "improvement",
          16
        ],
        [
          "significantly",
          16
        ],
        [
          "nature",
          16
        ],
        [
          "compute",
          16
        ],
        [
          "law",
          16
        ],
        [
          "states",
          16
        ],
        [
          "face",
          16
        ],
        [
          "wide",
          16
        ],
        [
          "application",
          16
        ],
        [
          "sources",
          16
        ],
        [
          "virtual",
          16
        ],
        [
          "conversations",
          16
        ],
        [
          "bard",
          16
        ],
        [
          "multi",
          16
        ],
        [
          "concerns",
          16
        ],
        [
          "gptzero",
          16
        ],
        [
          "cpp",
          16
        ],
        [
          "although",
          15
        ],
        [
          "texts",
          15
        ],
        [
          "average",
          15
        ],
        [
          "previously",
          15
        ],
        [
          "possible",
          15
        ],
        [
          "future",
          15
        ],
        [
          "rlhf",
          15
        ],
        [
          "allow",
          15
        ],
        [
          "processes",
          15
        ],
        [
          "ceo",
          15
        ],
        [
          "second",
          15
        ],
        [
          "mmlu",
          15
        ],
        [
          "built",
          15
        ],
        [
          "harmful",
          15
        ],
        [
          "produced",
          15
        ],
        [
          "raised",
          15
        ],
        [
          "blog",
          15
        ],
        [
          "learn",
          15
        ],
        [
          "sizes",
          15
        ],
        [
          "added",
          15
        ],
        [
          "conversational",
          15
        ],
        [
          "french",
          15
        ],
        [
          "format",
          15
        ],
        [
          "subscribers",
          15
        ],
        [
          "industry",
          15
        ],
        [
          "full",
          15
        ],
        [
          "reflection",
          15
        ],
        [
          "undetectable",
          15
        ],
        [
          "alignment",
          14
        ],
        [
          "became",
          14
        ],
        [
          "final",
          14
        ],
        [
          "part",
          14
        ],
        [
          "lower",
          14
        ],
        [
          "require",
          14
        ],
        [
          "typically",
          14
        ],
        [
          "either",
          14
        ],
        [
          "prediction",
          14
        ],
        [
          "list",
          14
        ],
        [
          "costs",
          14
        ],
        [
          "certain",
          14
        ],
        [
          "attempts",
          14
        ],
        [
          "programs",
          14
        ],
        [
          "scientific",
          14
        ],
        [
          "log",
          14
        ],
        [
          "behavior",
          14
        ],
        [
          "top",
          14
        ],
        [
          "addition",
          14
        ],
        [
          "leading",
          14
        ],
        [
          "sub",
          14
        ],
        [
          "analysis",
          14
        ],
        [
          "diverse",
          14
        ],
        [
          "center",
          14
        ],
        [
          "become",
          14
        ],
        [
          "review",
          14
        ],
        [
          "false",
          14
        ],
        [
          "days",
          14
        ],
        [
          "file",
          14
        ],
        [
          "requests",
          14
        ],
        [
          "united",
          14
        ],
        [
          "sam",
          14
        ],
        [
          "altman",
          14
        ],
        [
          "government",
          14
        ],
        [
          "technical",
          14
        ],
        [
          "library",
          14
        ],
        [
          "criticism",
          14
        ],
        [
          "hugging",
          14
        ],
        [
          "watsonx",
          14
        ],
        [
          "transformers",
          13
        ],
        [
          "upon",
          13
        ],
        [
          "need",
          13
        ],
        [
          "abilities",
          13
        ],
        [
          "solve",
          13
        ],
        [
          "powerful",
          13
        ],
        [
          "deepseek",
          13
        ],
        [
          "architectures",
          13
        ],
        [
          "tokenizer",
          13
        ],
        [
          "trillion",
          13
        ],
        [
          "relevant",
          13
        ],
        [
          "segment",
          13
        ],
        [
          "sentences",
          13
        ],
        [
          "sampling",
          13
        ],
        [
          "database",
          13
        ],
        [
          "memory",
          13
        ],
        [
          "specifically",
          13
        ],
        [
          "subsequent",
          13
        ],
        [
          "recognition",
          13
        ],
        [
          "resources",
          13
        ],
        [
          "techniques",
          13
        ],
        [
          "security",
          13
        ],
        [
          "creating",
          13
        ],
        [
          "background",
          13
        ],
        [
          "claimed",
          13
        ],
        [
          "business",
          13
        ],
        [
          "product",
          13
        ],
        [
          "internal",
          13
        ],
        [
          "custom",
          13
        ],
        [
          "revealed",
          13
        ],
        [
          "predecessor",
          13
        ],
        [
          "article",
          13
        ],
        [
          "despite",
          13
        ],
        [
          "saying",
          13
        ],
        [
          "issues",
          13
        ],
        [
          "fake",
          13
        ],
        [
          "performing",
          13
        ],
        [
          "instructions",
          13
        ],
        [
          "funding",
          13
        ],
        [
          "eleutherai",
          13
        ],
        [
          "posts",
          13
        ],
        [
          "yandexgpt",
          13
        ],
        [
          "applied",
          12
        ],
        [
          "mechanism",
          12
        ],
        [
          "improvements",
          12
        ],
        [
          "least",
          12
        ],
        [
          "must",
          12
        ],
        [
          "main",
          12
        ],
        [
          "find",
          12
        ],
        [
          "layers",
          12
        ],
        [
          "take",
          12
        ],
        [
          "longer",
          12
        ],
        [
          "summarize",
          12
        ],
        [
          "loss",
          12
        ],
        [
          "turing",
          12
        ],
        [
          "running",
          12
        ],
        [
          "program",
          12
        ],
        [
          "providing",
          12
        ],
        [
          "showed",
          12
        ],
        [
          "exam",
          12
        ],
        [
          "total",
          12
        ],
        [
          "particular",
          12
        ],
        [
          "rate",
          12
        ],
        [
          "beta",
          12
        ],
        [
          "similarly",
          12
        ],
        [
          "potentially",
          12
        ],
        [
          "notably",
          12
        ],
        [
          "among",
          12
        ],
        [
          "selected",
          12
        ],
        [
          "papers",
          12
        ],
        [
          "arxiv",
          12
        ],
        [
          "requires",
          12
        ],
        [
          "paid",
          12
        ],
        [
          "store",
          12
        ],
        [
          "position",
          12
        ],
        [
          "months",
          12
        ],
        [
          "tier",
          12
        ],
        [
          "technique",
          12
        ],
        [
          "third",
          12
        ],
        [
          "integration",
          12
        ],
        [
          "reports",
          12
        ],
        [
          "creative",
          12
        ],
        [
          "good",
          12
        ],
        [
          "effect",
          12
        ],
        [
          "makes",
          12
        ],
        [
          "decision",
          12
        ],
        [
          "flash",
          12
        ],
        [
          "trademark",
          12
        ],
        [
          "request",
          12
        ],
        [
          "aime",
          12
        ],
        [
          "stream",
          12
        ],
        [
          "pretrained",
          11
        ],
        [
          "began",
          11
        ],
        [
          "terms",
          11
        ],
        [
          "field",
          11
        ],
        [
          "replaced",
          11
        ],
        [
          "increasing",
          11
        ],
        [
          "tune",
          11
        ],
        [
          "directly",
          11
        ],
        [
          "taken",
          11
        ],
        [
          "needs",
          11
        ],
        [
          "focus",
          11
        ],
        [
          "autoregressive",
          11
        ],
        [
          "required",
          11
        ],
        [
          "interact",
          11
        ],
        [
          "think",
          11
        ],
        [
          "requiring",
          11
        ],
        [
          "means",
          11
        ],
        [
          "solutions",
          11
        ],
        [
          "followed",
          11
        ],
        [
          "unlike",
          11
        ],
        [
          "domains",
          11
        ],
        [
          "argued",
          11
        ],
        [
          "massive",
          11
        ],
        [
          "enable",
          11
        ],
        [
          "details",
          11
        ],
        [
          "shot",
          11
        ],
        [
          "lot",
          11
        ],
        [
          "creation",
          11
        ],
        [
          "hidden",
          11
        ],
        [
          "political",
          11
        ],
        [
          "along",
          11
        ],
        [
          "major",
          11
        ],
        [
          "every",
          11
        ],
        [
          "key",
          11
        ],
        [
          "date",
          11
        ],
        [
          "useful",
          11
        ],
        [
          "wikipedia",
          11
        ],
        [
          "shared",
          11
        ],
        [
          "sonnet",
          11
        ],
        [
          "supports",
          11
        ],
        [
          "apple",
          11
        ],
        [
          "integrated",
          11
        ],
        [
          "updated",
          11
        ],
        [
          "faster",
          11
        ],
        [
          "unveiled",
          11
        ],
        [
          "outperforms",
          11
        ],
        [
          "announcement",
          11
        ],
        [
          "professor",
          11
        ],
        [
          "elon",
          11
        ],
        [
          "pass",
          11
        ],
        [
          "risk",
          11
        ],
        [
          "scientist",
          11
        ],
        [
          "improving",
          11
        ],
        [
          "foundational",
          11
        ],
        [
          "johansson",
          11
        ],
        [
          "granite",
          11
        ],
        [
          "lemoine",
          11
        ],
        [
          "langchain",
          11
        ],
        [
          "dots",
          11
        ],
        [
          "sparrow",
          11
        ],
        [
          "gpts",
          10
        ],
        [
          "networks",
          10
        ],
        [
          "goal",
          10
        ],
        [
          "mistral",
          10
        ],
        [
          "performs",
          10
        ],
        [
          "recent",
          10
        ],
        [
          "tokenization",
          10
        ],
        [
          "encoding",
          10
        ],
        [
          "pairs",
          10
        ],
        [
          "completion",
          10
        ],
        [
          "low",
          10
        ],
        [
          "toxic",
          10
        ],
        [
          "mixture",
          10
        ],
        [
          "expensive",
          10
        ],
        [
          "computational",
          10
        ],
        [
          "considered",
          10
        ],
        [
          "towards",
          10
        ],
        [
          "hours",
          10
        ],
        [
          "get",
          10
        ],
        [
          "strategies",
          10
        ],
        [
          "rag",
          10
        ],
        [
          "document",
          10
        ],
        [
          "retriever",
          10
        ],
        [
          "actions",
          10
        ],
        [
          "point",
          10
        ],
        [
          "inputs",
          10
        ],
        [
          "international",
          10
        ],
        [
          "logical",
          10
        ],
        [
          "effort",
          10
        ],
        [
          "othello",
          10
        ],
        [
          "mathematical",
          10
        ],
        [
          "fact",
          10
        ],
        [
          "american",
          10
        ],
        [
          "basis",
          10
        ],
        [
          "modeling",
          10
        ],
        [
          "digital",
          10
        ],
        [
          "likely",
          10
        ],
        [
          "enhanced",
          10
        ],
        [
          "accurate",
          10
        ],
        [
          "otherwise",
          10
        ],
        [
          "big",
          10
        ],
        [
          "scores",
          10
        ],
        [
          "short",
          10
        ],
        [
          "successor",
          10
        ],
        [
          "global",
          10
        ],
        [
          "copyright",
          10
        ],
        [
          "accessible",
          10
        ],
        [
          "amounts",
          10
        ],
        [
          "tech",
          10
        ],
        [
          "stanford",
          10
        ],
        [
          "originally",
          10
        ],
        [
          "files",
          10
        ],
        [
          "reception",
          10
        ],
        [
          "risks",
          10
        ],
        [
          "consists",
          10
        ],
        [
          "stack",
          10
        ],
        [
          "simple",
          10
        ],
        [
          "embeddings",
          10
        ],
        [
          "dense",
          10
        ],
        [
          "random",
          10
        ],
        [
          "run",
          10
        ],
        [
          "tpu",
          10
        ],
        [
          "finetuned",
          10
        ],
        [
          "filtered",
          10
        ],
        [
          "estimated",
          10
        ],
        [
          "queries",
          10
        ],
        [
          "idea",
          10
        ],
        [
          "identify",
          10
        ],
        [
          "approximately",
          10
        ],
        [
          "provides",
          10
        ],
        [
          "ultra",
          10
        ],
        [
          "enables",
          10
        ],
        [
          "translate",
          10
        ],
        [
          "instructgpt",
          10
        ],
        [
          "browsing",
          10
        ],
        [
          "mode",
          10
        ],
        [
          "enabled",
          10
        ],
        [
          "table",
          10
        ],
        [
          "page",
          10
        ],
        [
          "social",
          10
        ],
        [
          "removed",
          10
        ],
        [
          "health",
          10
        ],
        [
          "handle",
          10
        ],
        [
          "scenarios",
          10
        ],
        [
          "ethical",
          10
        ],
        [
          "gopher",
          10
        ],
        [
          "businesses",
          10
        ],
        [
          "chan",
          10
        ],
        [
          "pol",
          10
        ],
        [
          "alibaba",
          10
        ],
        [
          "orm",
          10
        ],
        [
          "prm",
          10
        ],
        [
          "done",
          9
        ],
        [
          "went",
          9
        ],
        [
          "especially",
          9
        ],
        [
          "license",
          9
        ],
        [
          "numbers",
          9
        ],
        [
          "generally",
          9
        ],
        [
          "contain",
          9
        ],
        [
          "lead",
          9
        ],
        [
          "difficult",
          9
        ],
        [
          "optimization",
          9
        ],
        [
          "heads",
          9
        ],
        [
          "predicts",
          9
        ],
        [
          "tendency",
          9
        ],
        [
          "hardware",
          9
        ],
        [
          "whereas",
          9
        ],
        [
          "augmented",
          9
        ],
        [
          "act",
          9
        ],
        [
          "environment",
          9
        ],
        [
          "far",
          9
        ],
        [
          "describe",
          9
        ],
        [
          "select",
          9
        ],
        [
          "plans",
          9
        ],
        [
          "quantization",
          9
        ],
        [
          "simply",
          9
        ],
        [
          "particularly",
          9
        ],
        [
          "proprietary",
          9
        ],
        [
          "allowed",
          9
        ],
        [
          "reduce",
          9
        ],
        [
          "hallucinations",
          9
        ],
        [
          "negative",
          9
        ],
        [
          "changes",
          9
        ],
        [
          "board",
          9
        ],
        [
          "existing",
          9
        ],
        [
          "parrot",
          9
        ],
        [
          "incorrect",
          9
        ],
        [
          "last",
          9
        ],
        [
          "efficient",
          9
        ],
        [
          "purpose",
          9
        ],
        [
          "change",
          9
        ],
        [
          "hundreds",
          9
        ],
        [
          "reading",
          9
        ],
        [
          "market",
          9
        ],
        [
          "lack",
          9
        ],
        [
          "address",
          9
        ],
        [
          "implemented",
          9
        ],
        [
          "causal",
          9
        ],
        [
          "head",
          9
        ],
        [
          "function",
          9
        ],
        [
          "started",
          9
        ],
        [
          "plausible",
          9
        ],
        [
          "multilingual",
          9
        ],
        [
          "latest",
          9
        ],
        [
          "names",
          9
        ],
        [
          "giving",
          9
        ],
        [
          "platforms",
          9
        ],
        [
          "android",
          9
        ],
        [
          "labels",
          9
        ],
        [
          "enough",
          9
        ],
        [
          "percent",
          9
        ],
        [
          "experimental",
          9
        ],
        [
          "party",
          9
        ],
        [
          "bing",
          9
        ],
        [
          "subscription",
          9
        ],
        [
          "cheaper",
          9
        ],
        [
          "scored",
          9
        ],
        [
          "york",
          9
        ],
        [
          "verge",
          9
        ],
        [
          "parrots",
          9
        ],
        [
          "institute",
          9
        ],
        [
          "additionally",
          9
        ],
        [
          "detection",
          9
        ],
        [
          "newspaper",
          9
        ],
        [
          "financial",
          9
        ],
        [
          "direct",
          9
        ],
        [
          "adapted",
          9
        ],
        [
          "court",
          9
        ],
        [
          "copyrighted",
          9
        ],
        [
          "military",
          9
        ],
        [
          "speed",
          9
        ],
        [
          "math",
          9
        ],
        [
          "python",
          9
        ],
        [
          "community",
          9
        ],
        [
          "keynote",
          9
        ],
        [
          "customers",
          9
        ],
        [
          "gemma",
          9
        ],
        [
          "reflective",
          9
        ],
        [
          "solution",
          9
        ],
        [
          "regarding",
          8
        ],
        [
          "biases",
          8
        ],
        [
          "statistical",
          8
        ],
        [
          "constructed",
          8
        ],
        [
          "rapid",
          8
        ],
        [
          "variants",
          8
        ],
        [
          "algorithms",
          8
        ],
        [
          "index",
          8
        ],
        [
          "special",
          8
        ],
        [
          "control",
          8
        ],
        [
          "characters",
          8
        ],
        [
          "instruct",
          8
        ],
        [
          "medium",
          8
        ],
        [
          "refers",
          8
        ],
        [
          "algorithm",
          8
        ],
        [
          "responding",
          8
        ],
        [
          "separate",
          8
        ],
        [
          "autonomous",
          8
        ],
        [
          "plan",
          8
        ],
        [
          "tree",
          8
        ],
        [
          "rollout",
          8
        ],
        [
          "solving",
          8
        ],
        [
          "traditional",
          8
        ],
        [
          "comparable",
          8
        ],
        [
          "remained",
          8
        ],
        [
          "employed",
          8
        ],
        [
          "laws",
          8
        ],
        [
          "alpha",
          8
        ],
        [
          "decoding",
          8
        ],
        [
          "choice",
          8
        ],
        [
          "resulting",
          8
        ],
        [
          "survey",
          8
        ],
        [
          "medicine",
          8
        ],
        [
          "give",
          8
        ],
        [
          "framework",
          8
        ],
        [
          "entropy",
          8
        ],
        [
          "commonly",
          8
        ],
        [
          "capability",
          8
        ],
        [
          "expected",
          8
        ],
        [
          "complete",
          8
        ],
        [
          "humanity",
          8
        ],
        [
          "challenging",
          8
        ],
        [
          "challenges",
          8
        ],
        [
          "achieve",
          8
        ],
        [
          "developer",
          8
        ],
        [
          "respond",
          8
        ],
        [
          "objective",
          8
        ],
        [
          "structure",
          8
        ],
        [
          "coming",
          8
        ],
        [
          "suggest",
          8
        ],
        [
          "limit",
          8
        ],
        [
          "twitter",
          8
        ],
        [
          "comprehensive",
          8
        ],
        [
          "representing",
          8
        ],
        [
          "prior",
          8
        ],
        [
          "binary",
          8
        ],
        [
          "left",
          8
        ],
        [
          "positional",
          8
        ],
        [
          "works",
          8
        ],
        [
          "opus",
          8
        ],
        [
          "commands",
          8
        ],
        [
          "modes",
          8
        ],
        [
          "style",
          8
        ],
        [
          "pages",
          8
        ],
        [
          "student",
          8
        ],
        [
          "rejected",
          8
        ],
        [
          "four",
          8
        ],
        [
          "add",
          8
        ],
        [
          "conducted",
          8
        ],
        [
          "offered",
          8
        ],
        [
          "offers",
          8
        ],
        [
          "competitive",
          8
        ],
        [
          "statement",
          8
        ],
        [
          "gained",
          8
        ],
        [
          "cited",
          8
        ],
        [
          "dangers",
          8
        ],
        [
          "bender",
          8
        ],
        [
          "gebru",
          8
        ],
        [
          "china",
          8
        ],
        [
          "letter",
          8
        ],
        [
          "citations",
          8
        ],
        [
          "consistency",
          8
        ],
        [
          "combined",
          8
        ],
        [
          "education",
          8
        ],
        [
          "earlier",
          8
        ],
        [
          "granted",
          8
        ],
        [
          "offer",
          8
        ],
        [
          "press",
          8
        ],
        [
          "alongside",
          8
        ],
        [
          "interactions",
          8
        ],
        [
          "summarization",
          8
        ],
        [
          "ventures",
          8
        ],
        [
          "nano",
          8
        ],
        [
          "brand",
          8
        ],
        [
          "capacity",
          8
        ],
        [
          "sparse",
          8
        ],
        [
          "xlnet",
          8
        ],
        [
          "specialized",
          8
        ],
        [
          "computing",
          8
        ],
        [
          "rules",
          8
        ],
        [
          "remove",
          8
        ],
        [
          "yandex",
          8
        ],
        [
          "traces",
          8
        ],
        [
          "seq",
          7
        ],
        [
          "conference",
          7
        ],
        [
          "offering",
          7
        ],
        [
          "impact",
          7
        ],
        [
          "restrictions",
          7
        ],
        [
          "apache",
          7
        ],
        [
          "weight",
          7
        ],
        [
          "pair",
          7
        ],
        [
          "encoded",
          7
        ],
        [
          "starting",
          7
        ],
        [
          "contains",
          7
        ],
        [
          "maximum",
          7
        ],
        [
          "parts",
          7
        ],
        [
          "cause",
          7
        ],
        [
          "distribution",
          7
        ],
        [
          "gpu",
          7
        ],
        [
          "already",
          7
        ],
        [
          "basic",
          7
        ],
        [
          "retrieve",
          7
        ],
        [
          "describing",
          7
        ],
        [
          "guide",
          7
        ],
        [
          "skills",
          7
        ],
        [
          "precision",
          7
        ],
        [
          "finetuning",
          7
        ],
        [
          "late",
          7
        ],
        [
          "shown",
          7
        ],
        [
          "structured",
          7
        ],
        [
          "emergent",
          7
        ],
        [
          "cognitive",
          7
        ],
        [
          "involved",
          7
        ],
        [
          "argue",
          7
        ],
        [
          "count",
          7
        ],
        [
          "interpretation",
          7
        ],
        [
          "performed",
          7
        ],
        [
          "ever",
          7
        ],
        [
          "yet",
          7
        ],
        [
          "contrast",
          7
        ],
        [
          "continue",
          7
        ],
        [
          "old",
          7
        ],
        [
          "frameworks",
          7
        ],
        [
          "acceptable",
          7
        ],
        [
          "enabling",
          7
        ],
        [
          "utilizes",
          7
        ],
        [
          "broad",
          7
        ],
        [
          "standard",
          7
        ],
        [
          "instance",
          7
        ],
        [
          "seen",
          7
        ],
        [
          "accurately",
          7
        ],
        [
          "suggested",
          7
        ],
        [
          "years",
          7
        ],
        [
          "start",
          7
        ],
        [
          "concern",
          7
        ],
        [
          "misinformation",
          7
        ],
        [
          "representations",
          7
        ],
        [
          "current",
          7
        ],
        [
          "ways",
          7
        ],
        [
          "engineers",
          7
        ],
        [
          "dan",
          7
        ],
        [
          "reviews",
          7
        ],
        [
          "founder",
          7
        ],
        [
          "five",
          7
        ],
        [
          "investors",
          7
        ],
        [
          "develop",
          7
        ],
        [
          "operations",
          7
        ],
        [
          "explore",
          7
        ],
        [
          "errors",
          7
        ],
        [
          "involving",
          7
        ],
        [
          "salesforce",
          7
        ],
        [
          "experiments",
          7
        ],
        [
          "producing",
          7
        ],
        [
          "containing",
          7
        ],
        [
          "took",
          7
        ],
        [
          "publications",
          7
        ],
        [
          "almost",
          7
        ],
        [
          "notes",
          7
        ],
        [
          "supported",
          7
        ],
        [
          "operating",
          7
        ],
        [
          "mobile",
          7
        ],
        [
          "stage",
          7
        ],
        [
          "california",
          7
        ],
        [
          "wolfram",
          7
        ],
        [
          "minutes",
          7
        ],
        [
          "scientists",
          7
        ],
        [
          "nine",
          7
        ],
        [
          "points",
          7
        ],
        [
          "statements",
          7
        ],
        [
          "price",
          7
        ],
        [
          "nvidia",
          7
        ],
        [
          "students",
          7
        ],
        [
          "widely",
          7
        ],
        [
          "responded",
          7
        ],
        [
          "citing",
          7
        ],
        [
          "author",
          7
        ],
        [
          "professionals",
          7
        ],
        [
          "developing",
          7
        ],
        [
          "update",
          7
        ],
        [
          "society",
          7
        ],
        [
          "share",
          7
        ],
        [
          "finance",
          7
        ],
        [
          "preference",
          7
        ],
        [
          "clinical",
          7
        ],
        [
          "assist",
          7
        ],
        [
          "outperformed",
          7
        ],
        [
          "patient",
          7
        ],
        [
          "judge",
          7
        ],
        [
          "infringement",
          7
        ],
        [
          "founded",
          7
        ],
        [
          "ethics",
          7
        ],
        [
          "tailored",
          7
        ],
        [
          "constitution",
          7
        ],
        [
          "predecessors",
          7
        ],
        [
          "integrates",
          7
        ],
        [
          "oracle",
          7
        ],
        [
          "washington",
          7
        ],
        [
          "targeted",
          7
        ],
        [
          "aid",
          7
        ],
        [
          "mechanisms",
          7
        ],
        [
          "school",
          7
        ],
        [
          "purposes",
          7
        ],
        [
          "bit",
          7
        ],
        [
          "controversy",
          7
        ],
        [
          "sky",
          7
        ],
        [
          "kilcher",
          7
        ],
        [
          "wanted",
          7
        ],
        [
          "week",
          7
        ],
        [
          "graphs",
          7
        ],
        [
          "aurora",
          7
        ],
        [
          "infty",
          7
        ],
        [
          "modern",
          6
        ],
        [
          "corpora",
          6
        ],
        [
          "present",
          6
        ],
        [
          "malicious",
          6
        ],
        [
          "caused",
          6
        ],
        [
          "popularity",
          6
        ],
        [
          "space",
          6
        ],
        [
          "unknown",
          6
        ],
        [
          "needed",
          6
        ],
        [
          "unique",
          6
        ],
        [
          "instances",
          6
        ],
        [
          "frequently",
          6
        ],
        [
          "together",
          6
        ],
        [
          "optimized",
          6
        ],
        [
          "line",
          6
        ],
        [
          "windows",
          6
        ],
        [
          "account",
          6
        ],
        [
          "important",
          6
        ],
        [
          "matter",
          6
        ],
        [
          "infrastructure",
          6
        ],
        [
          "substantial",
          6
        ],
        [
          "flops",
          6
        ],
        [
          "sophisticated",
          6
        ],
        [
          "call",
          6
        ],
        [
          "integrating",
          6
        ],
        [
          "finding",
          6
        ],
        [
          "modules",
          6
        ],
        [
          "planning",
          6
        ],
        [
          "action",
          6
        ],
        [
          "description",
          6
        ],
        [
          "record",
          6
        ],
        [
          "environmental",
          6
        ],
        [
          "learned",
          6
        ],
        [
          "signal",
          6
        ],
        [
          "functions",
          6
        ],
        [
          "bits",
          6
        ],
        [
          "aims",
          6
        ],
        [
          "quantized",
          6
        ],
        [
          "label",
          6
        ],
        [
          "effectiveness",
          6
        ],
        [
          "scratch",
          6
        ],
        [
          "reached",
          6
        ],
        [
          "private",
          6
        ],
        [
          "automated",
          6
        ],
        [
          "pretraining",
          6
        ],
        [
          "linear",
          6
        ],
        [
          "theory",
          6
        ],
        [
          "right",
          6
        ],
        [
          "combination",
          6
        ],
        [
          "mind",
          6
        ],
        [
          "predicted",
          6
        ],
        [
          "focuses",
          6
        ],
        [
          "untuned",
          6
        ],
        [
          "believe",
          6
        ],
        [
          "novel",
          6
        ],
        [
          "entire",
          6
        ],
        [
          "phenomenon",
          6
        ],
        [
          "hallucination",
          6
        ],
        [
          "measures",
          6
        ],
        [
          "expressed",
          6
        ],
        [
          "character",
          6
        ],
        [
          "appropriate",
          6
        ],
        [
          "cross",
          6
        ],
        [
          "consisting",
          6
        ],
        [
          "san",
          6
        ],
        [
          "derived",
          6
        ],
        [
          "closed",
          6
        ],
        [
          "hle",
          6
        ],
        [
          "actual",
          6
        ],
        [
          "adversarial",
          6
        ],
        [
          "options",
          6
        ],
        [
          "industries",
          6
        ],
        [
          "automation",
          6
        ],
        [
          "focused",
          6
        ],
        [
          "researcher",
          6
        ],
        [
          "enhancing",
          6
        ],
        [
          "filter",
          6
        ],
        [
          "effectively",
          6
        ],
        [
          "proposed",
          6
        ],
        [
          "race",
          6
        ],
        [
          "gender",
          6
        ],
        [
          "introduction",
          6
        ],
        [
          "national",
          6
        ],
        [
          "breaking",
          6
        ],
        [
          "observers",
          6
        ],
        [
          "analyze",
          6
        ],
        [
          "reference",
          6
        ],
        [
          "repository",
          6
        ],
        [
          "wired",
          6
        ],
        [
          "module",
          6
        ],
        [
          "section",
          6
        ],
        [
          "forward",
          6
        ],
        [
          "feed",
          6
        ],
        [
          "varying",
          6
        ],
        [
          "detail",
          6
        ],
        [
          "avoid",
          6
        ],
        [
          "chosen",
          6
        ],
        [
          "cls",
          6
        ],
        [
          "except",
          6
        ],
        [
          "lists",
          6
        ],
        [
          "chips",
          6
        ],
        [
          "target",
          6
        ],
        [
          "never",
          6
        ],
        [
          "slightly",
          6
        ],
        [
          "variant",
          6
        ],
        [
          "outcome",
          6
        ],
        [
          "huggingface",
          6
        ],
        [
          "manually",
          6
        ],
        [
          "alternative",
          6
        ],
        [
          "privacy",
          6
        ],
        [
          "websites",
          6
        ],
        [
          "formats",
          6
        ],
        [
          "diffusion",
          6
        ],
        [
          "dall",
          6
        ],
        [
          "ios",
          6
        ],
        [
          "listed",
          6
        ],
        [
          "apps",
          6
        ],
        [
          "videos",
          6
        ],
        [
          "currently",
          6
        ],
        [
          "desired",
          6
        ],
        [
          "investment",
          6
        ],
        [
          "plagiarism",
          6
        ],
        [
          "enterprise",
          6
        ],
        [
          "magazine",
          6
        ],
        [
          "came",
          6
        ],
        [
          "facts",
          6
        ],
        [
          "plugins",
          6
        ],
        [
          "fiction",
          6
        ],
        [
          "comparing",
          6
        ],
        [
          "russian",
          6
        ],
        [
          "attacks",
          6
        ],
        [
          "changed",
          6
        ],
        [
          "upload",
          6
        ],
        [
          "labeled",
          6
        ],
        [
          "declined",
          6
        ],
        [
          "percentile",
          6
        ],
        [
          "physics",
          6
        ],
        [
          "detailed",
          6
        ],
        [
          "executives",
          6
        ],
        [
          "annual",
          6
        ],
        [
          "announcing",
          6
        ],
        [
          "popular",
          6
        ],
        [
          "italian",
          6
        ],
        [
          "opened",
          6
        ],
        [
          "curated",
          6
        ],
        [
          "contained",
          6
        ],
        [
          "easy",
          6
        ],
        [
          "updates",
          6
        ],
        [
          "materials",
          6
        ],
        [
          "experiment",
          6
        ],
        [
          "outperform",
          6
        ],
        [
          "professional",
          6
        ],
        [
          "lab",
          6
        ],
        [
          "warned",
          6
        ],
        [
          "biased",
          6
        ],
        [
          "error",
          6
        ],
        [
          "wrong",
          6
        ],
        [
          "criticized",
          6
        ],
        [
          "argument",
          6
        ],
        [
          "property",
          6
        ],
        [
          "determined",
          6
        ],
        [
          "sharing",
          6
        ],
        [
          "group",
          6
        ],
        [
          "david",
          6
        ],
        [
          "multitask",
          6
        ],
        [
          "six",
          6
        ],
        [
          "default",
          6
        ],
        [
          "compliance",
          6
        ],
        [
          "rights",
          6
        ],
        [
          "instant",
          6
        ],
        [
          "iteration",
          6
        ],
        [
          "refused",
          6
        ],
        [
          "server",
          6
        ],
        [
          "functionality",
          6
        ],
        [
          "extraction",
          6
        ],
        [
          "former",
          6
        ],
        [
          "baidu",
          6
        ],
        [
          "live",
          6
        ],
        [
          "south",
          6
        ],
        [
          "formerly",
          6
        ],
        [
          "stages",
          6
        ],
        [
          "highlighted",
          6
        ],
        [
          "intended",
          6
        ],
        [
          "cerebras",
          6
        ],
        [
          "implications",
          6
        ],
        [
          "beyond",
          6
        ],
        [
          "participants",
          6
        ],
        [
          "opposite",
          6
        ],
        [
          "highest",
          6
        ],
        [
          "crawl",
          6
        ],
        [
          "helps",
          6
        ],
        [
          "releasing",
          6
        ],
        [
          "definition",
          6
        ],
        [
          "stories",
          6
        ],
        [
          "discussion",
          6
        ],
        [
          "educational",
          6
        ],
        [
          "featuring",
          6
        ],
        [
          "gpqa",
          6
        ],
        [
          "phd",
          6
        ],
        [
          "weather",
          6
        ],
        [
          "governance",
          6
        ],
        [
          "gerganov",
          6
        ],
        [
          "ggml",
          6
        ],
        [
          "ones",
          6
        ],
        [
          "trace",
          6
        ],
        [
          "waluigi",
          6
        ],
        [
          "vast",
          5
        ],
        [
          "semantics",
          5
        ],
        [
          "converted",
          5
        ],
        [
          "quickly",
          5
        ],
        [
          "consumer",
          5
        ],
        [
          "hype",
          5
        ],
        [
          "praised",
          5
        ],
        [
          "robotics",
          5
        ],
        [
          "bloom",
          5
        ],
        [
          "finally",
          5
        ],
        [
          "byte",
          5
        ],
        [
          "serving",
          5
        ],
        [
          "appearing",
          5
        ],
        [
          "merged",
          5
        ],
        [
          "cleaned",
          5
        ],
        [
          "filtering",
          5
        ],
        [
          "replacing",
          5
        ],
        [
          "reduced",
          5
        ],
        [
          "textual",
          5
        ],
        [
          "continues",
          5
        ],
        [
          "predicting",
          5
        ],
        [
          "increasingly",
          5
        ],
        [
          "solved",
          5
        ],
        [
          "reply",
          5
        ],
        [
          "strategy",
          5
        ],
        [
          "read",
          5
        ],
        [
          "stored",
          5
        ],
        [
          "learns",
          5
        ],
        [
          "ended",
          5
        ],
        [
          "curriculum",
          5
        ],
        [
          "construct",
          5
        ],
        [
          "levels",
          5
        ],
        [
          "agents",
          5
        ],
        [
          "float",
          5
        ],
        [
          "etc",
          5
        ],
        [
          "efforts",
          5
        ],
        [
          "largely",
          5
        ],
        [
          "frac",
          5
        ],
        [
          "interaction",
          5
        ],
        [
          "programmed",
          5
        ],
        [
          "furthermore",
          5
        ],
        [
          "black",
          5
        ],
        [
          "viewed",
          5
        ],
        [
          "engineer",
          5
        ],
        [
          "moves",
          5
        ],
        [
          "involves",
          5
        ],
        [
          "story",
          5
        ],
        [
          "termed",
          5
        ],
        [
          "sound",
          5
        ],
        [
          "opinions",
          5
        ],
        [
          "ideas",
          5
        ],
        [
          "turn",
          5
        ],
        [
          "grammar",
          5
        ],
        [
          "role",
          5
        ],
        [
          "linked",
          5
        ],
        [
          "relationship",
          5
        ],
        [
          "reliable",
          5
        ],
        [
          "underlying",
          5
        ],
        [
          "evaluate",
          5
        ],
        [
          "commonsense",
          5
        ],
        [
          "alice",
          5
        ],
        [
          "combine",
          5
        ],
        [
          "value",
          5
        ],
        [
          "teach",
          5
        ],
        [
          "sign",
          5
        ],
        [
          "demonstrates",
          5
        ],
        [
          "progress",
          5
        ],
        [
          "kevin",
          5
        ],
        [
          "event",
          5
        ],
        [
          "incorporate",
          5
        ],
        [
          "implementing",
          5
        ],
        [
          "obtain",
          5
        ],
        [
          "groups",
          5
        ],
        [
          "selection",
          5
        ],
        [
          "depending",
          5
        ],
        [
          "energy",
          5
        ],
        [
          "demands",
          5
        ],
        [
          "evaluating",
          5
        ],
        [
          "apis",
          5
        ],
        [
          "suggestions",
          5
        ],
        [
          "reportedly",
          5
        ],
        [
          "tweeted",
          5
        ],
        [
          "mistakes",
          5
        ],
        [
          "hallucinate",
          5
        ],
        [
          "corresponding",
          5
        ],
        [
          "choosing",
          5
        ],
        [
          "roughly",
          5
        ],
        [
          "sought",
          5
        ],
        [
          "email",
          5
        ],
        [
          "commented",
          5
        ],
        [
          "technologies",
          5
        ],
        [
          "latent",
          5
        ],
        [
          "interpret",
          5
        ],
        [
          "bertbase",
          5
        ],
        [
          "bertlarge",
          5
        ],
        [
          "bookcorpus",
          5
        ],
        [
          "dimensional",
          5
        ],
        [
          "classification",
          5
        ],
        [
          "describes",
          5
        ],
        [
          "passed",
          5
        ],
        [
          "back",
          5
        ],
        [
          "always",
          5
        ],
        [
          "randomly",
          5
        ],
        [
          "shift",
          5
        ],
        [
          "hour",
          5
        ],
        [
          "behind",
          5
        ],
        [
          "comes",
          5
        ],
        [
          "semi",
          5
        ],
        [
          "unsupervised",
          5
        ],
        [
          "inspired",
          5
        ],
        [
          "distilled",
          5
        ],
        [
          "matrix",
          5
        ],
        [
          "initiative",
          5
        ],
        [
          "academia",
          5
        ],
        [
          "collected",
          5
        ],
        [
          "documented",
          5
        ],
        [
          "dictation",
          5
        ],
        [
          "dialects",
          5
        ],
        [
          "devices",
          5
        ],
        [
          "emails",
          5
        ],
        [
          "management",
          5
        ],
        [
          "leo",
          5
        ],
        [
          "growing",
          5
        ],
        [
          "ranked",
          5
        ],
        [
          "francisco",
          5
        ],
        [
          "music",
          5
        ],
        [
          "lyrics",
          5
        ],
        [
          "journalists",
          5
        ],
        [
          "demand",
          5
        ],
        [
          "harm",
          5
        ],
        [
          "entirely",
          5
        ],
        [
          "fabricated",
          5
        ],
        [
          "reddit",
          5
        ],
        [
          "success",
          5
        ],
        [
          "planned",
          5
        ],
        [
          "working",
          5
        ],
        [
          "gpus",
          5
        ],
        [
          "millions",
          5
        ],
        [
          "reporting",
          5
        ],
        [
          "icelandic",
          5
        ],
        [
          "seven",
          5
        ],
        [
          "korean",
          5
        ],
        [
          "arabic",
          5
        ],
        [
          "something",
          5
        ],
        [
          "fast",
          5
        ],
        [
          "codeforces",
          5
        ],
        [
          "guardian",
          5
        ],
        [
          "germany",
          5
        ],
        [
          "atlantic",
          5
        ],
        [
          "threat",
          5
        ],
        [
          "collaboration",
          5
        ],
        [
          "emily",
          5
        ],
        [
          "timnit",
          5
        ],
        [
          "mitchell",
          5
        ],
        [
          "banned",
          5
        ],
        [
          "submitted",
          5
        ],
        [
          "material",
          5
        ],
        [
          "protection",
          5
        ],
        [
          "phishing",
          5
        ],
        [
          "completely",
          5
        ],
        [
          "studies",
          5
        ],
        [
          "link",
          5
        ],
        [
          "hosting",
          5
        ],
        [
          "competition",
          5
        ],
        [
          "assistance",
          5
        ],
        [
          "practice",
          5
        ],
        [
          "experience",
          5
        ],
        [
          "stephen",
          5
        ],
        [
          "eight",
          5
        ],
        [
          "summaries",
          5
        ],
        [
          "capital",
          5
        ],
        [
          "positive",
          5
        ],
        [
          "toward",
          5
        ],
        [
          "subjects",
          5
        ],
        [
          "sued",
          5
        ],
        [
          "permission",
          5
        ],
        [
          "pause",
          5
        ],
        [
          "easier",
          5
        ],
        [
          "phase",
          5
        ],
        [
          "paradigm",
          5
        ],
        [
          "modifications",
          5
        ],
        [
          "haiku",
          5
        ],
        [
          "partnered",
          5
        ],
        [
          "expanded",
          5
        ],
        [
          "aligned",
          5
        ],
        [
          "discussions",
          5
        ],
        [
          "youtube",
          5
        ],
        [
          "organizations",
          5
        ],
        [
          "integrate",
          5
        ],
        [
          "enhance",
          5
        ],
        [
          "round",
          5
        ],
        [
          "graph",
          5
        ],
        [
          "subject",
          5
        ],
        [
          "interview",
          5
        ],
        [
          "hassabis",
          5
        ],
        [
          "tensor",
          5
        ],
        [
          "mark",
          5
        ],
        [
          "referred",
          5
        ],
        [
          "facebook",
          5
        ],
        [
          "fully",
          5
        ],
        [
          "dialogue",
          5
        ],
        [
          "academy",
          5
        ],
        [
          "summarizing",
          5
        ],
        [
          "actually",
          5
        ],
        [
          "facilitate",
          5
        ],
        [
          "zero",
          5
        ],
        [
          "similarity",
          5
        ],
        [
          "prose",
          5
        ],
        [
          "partial",
          5
        ],
        [
          "outlets",
          5
        ],
        [
          "coherent",
          5
        ],
        [
          "retro",
          5
        ],
        [
          "comprehension",
          5
        ],
        [
          "taking",
          5
        ],
        [
          "message",
          5
        ],
        [
          "tone",
          5
        ],
        [
          "setting",
          5
        ],
        [
          "flan",
          5
        ],
        [
          "transparency",
          5
        ],
        [
          "max",
          5
        ],
        [
          "incorporates",
          5
        ],
        [
          "scarlett",
          5
        ],
        [
          "feedforward",
          5
        ],
        [
          "identity",
          5
        ],
        [
          "maintain",
          5
        ],
        [
          "cluster",
          5
        ],
        [
          "expert",
          5
        ],
        [
          "commercial",
          5
        ],
        [
          "watson",
          5
        ],
        [
          "jais",
          5
        ],
        [
          "meena",
          5
        ],
        [
          "dialog",
          5
        ],
        [
          "engine",
          5
        ],
        [
          "petaflop",
          5
        ],
        [
          "gguf",
          5
        ],
        [
          "tunney",
          5
        ],
        [
          "reservation",
          5
        ],
        [
          "intermediate",
          5
        ],
        [
          "cot",
          5
        ],
        [
          "rightarrow",
          5
        ],
        [
          "chunks",
          5
        ],
        [
          "xxl",
          5
        ],
        [
          "benioff",
          5
        ],
        [
          "youchat",
          5
        ],
        [
          "youpro",
          5
        ],
        [
          "guided",
          4
        ],
        [
          "capacities",
          4
        ],
        [
          "gram",
          4
        ],
        [
          "symbolic",
          4
        ],
        [
          "mainly",
          4
        ],
        [
          "blocks",
          4
        ],
        [
          "widespread",
          4
        ],
        [
          "locally",
          4
        ],
        [
          "browser",
          4
        ],
        [
          "reveal",
          4
        ],
        [
          "chains",
          4
        ],
        [
          "competing",
          4
        ],
        [
          "denote",
          4
        ],
        [
          "formatting",
          4
        ],
        [
          "grams",
          4
        ],
        [
          "removing",
          4
        ],
        [
          "linguistic",
          4
        ],
        [
          "approaches",
          4
        ],
        [
          "scope",
          4
        ],
        [
          "soft",
          4
        ],
        [
          "batch",
          4
        ],
        [
          "considerations",
          4
        ],
        [
          "documentation",
          4
        ],
        [
          "correctly",
          4
        ],
        [
          "dynamic",
          4
        ],
        [
          "reason",
          4
        ],
        [
          "connected",
          4
        ],
        [
          "descriptions",
          4
        ],
        [
          "interestingness",
          4
        ],
        [
          "outputting",
          4
        ],
        [
          "sequences",
          4
        ],
        [
          "contexts",
          4
        ],
        [
          "compression",
          4
        ],
        [
          "half",
          4
        ],
        [
          "gigabytes",
          4
        ],
        [
          "modalities",
          4
        ],
        [
          "processed",
          4
        ],
        [
          "effective",
          4
        ],
        [
          "build",
          4
        ],
        [
          "extensive",
          4
        ],
        [
          "superior",
          4
        ],
        [
          "begin",
          4
        ],
        [
          "likelihood",
          4
        ],
        [
          "employ",
          4
        ],
        [
          "cognition",
          4
        ],
        [
          "principles",
          4
        ],
        [
          "zeros",
          4
        ],
        [
          "immediately",
          4
        ],
        [
          "toy",
          4
        ],
        [
          "modified",
          4
        ],
        [
          "let",
          4
        ],
        [
          "interpretability",
          4
        ],
        [
          "reverse",
          4
        ],
        [
          "modifying",
          4
        ],
        [
          "karel",
          4
        ],
        [
          "arithmetic",
          4
        ],
        [
          "concept",
          4
        ],
        [
          "psychology",
          4
        ],
        [
          "reasonably",
          4
        ],
        [
          "say",
          4
        ],
        [
          "passes",
          4
        ],
        [
          "intelligent",
          4
        ],
        [
          "criminal",
          4
        ],
        [
          "observed",
          4
        ],
        [
          "nonsensical",
          4
        ],
        [
          "suggests",
          4
        ],
        [
          "established",
          4
        ],
        [
          "communication",
          4
        ],
        [
          "measure",
          4
        ],
        [
          "contents",
          4
        ],
        [
          "evaluated",
          4
        ],
        [
          "portions",
          4
        ],
        [
          "bpw",
          4
        ],
        [
          "bpt",
          4
        ],
        [
          "serve",
          4
        ],
        [
          "glue",
          4
        ],
        [
          "bench",
          4
        ],
        [
          "vary",
          4
        ],
        [
          "repeatedly",
          4
        ],
        [
          "true",
          4
        ],
        [
          "arms",
          4
        ],
        [
          "selects",
          4
        ],
        [
          "rapidly",
          4
        ],
        [
          "achieving",
          4
        ],
        [
          "distinguish",
          4
        ],
        [
          "globally",
          4
        ],
        [
          "misuse",
          4
        ],
        [
          "availability",
          4
        ],
        [
          "remain",
          4
        ],
        [
          "controls",
          4
        ],
        [
          "bypass",
          4
        ],
        [
          "drug",
          4
        ],
        [
          "operation",
          4
        ],
        [
          "views",
          4
        ],
        [
          "age",
          4
        ],
        [
          "option",
          4
        ],
        [
          "primarily",
          4
        ],
        [
          "coverage",
          4
        ],
        [
          "doi",
          4
        ],
        [
          "michael",
          4
        ],
        [
          "independently",
          4
        ],
        [
          "events",
          4
        ],
        [
          "mcmillan",
          4
        ],
        [
          "mentioned",
          4
        ],
        [
          "situations",
          4
        ],
        [
          "advice",
          4
        ],
        [
          "cons",
          4
        ],
        [
          "hypothetical",
          4
        ],
        [
          "bidirectional",
          4
        ],
        [
          "represent",
          4
        ],
        [
          "elmo",
          4
        ],
        [
          "toronto",
          4
        ],
        [
          "newly",
          4
        ],
        [
          "fed",
          4
        ],
        [
          "absolute",
          4
        ],
        [
          "dimension",
          4
        ],
        [
          "takes",
          4
        ],
        [
          "sep",
          4
        ],
        [
          "spans",
          4
        ],
        [
          "appeared",
          4
        ],
        [
          "classify",
          4
        ],
        [
          "fewer",
          4
        ],
        [
          "side",
          4
        ],
        [
          "today",
          4
        ],
        [
          "thus",
          4
        ],
        [
          "design",
          4
        ],
        [
          "plain",
          4
        ],
        [
          "improves",
          4
        ],
        [
          "deberta",
          4
        ],
        [
          "bigscience",
          4
        ],
        [
          "combines",
          4
        ],
        [
          "voices",
          4
        ],
        [
          "webpages",
          4
        ],
        [
          "refine",
          4
        ],
        [
          "replies",
          4
        ],
        [
          "partnership",
          4
        ],
        [
          "inc",
          4
        ],
        [
          "trainers",
          4
        ],
        [
          "detect",
          4
        ],
        [
          "receive",
          4
        ],
        [
          "mimic",
          4
        ],
        [
          "essays",
          4
        ],
        [
          "song",
          4
        ],
        [
          "tell",
          4
        ],
        [
          "columbus",
          4
        ],
        [
          "paying",
          4
        ],
        [
          "responds",
          4
        ],
        [
          "white",
          4
        ],
        [
          "writer",
          4
        ],
        [
          "looking",
          4
        ],
        [
          "factual",
          4
        ],
        [
          "anything",
          4
        ],
        [
          "canadian",
          4
        ],
        [
          "messages",
          4
        ],
        [
          "speak",
          4
        ],
        [
          "whisper",
          4
        ],
        [
          "india",
          4
        ],
        [
          "worldwide",
          4
        ],
        [
          "dollars",
          4
        ],
        [
          "upgraded",
          4
        ],
        [
          "bug",
          4
        ],
        [
          "card",
          4
        ],
        [
          "president",
          4
        ],
        [
          "capture",
          4
        ],
        [
          "signed",
          4
        ],
        [
          "impressive",
          4
        ],
        [
          "hard",
          4
        ],
        [
          "twice",
          4
        ],
        [
          "areas",
          4
        ],
        [
          "vox",
          4
        ],
        [
          "away",
          4
        ],
        [
          "red",
          4
        ],
        [
          "philosopher",
          4
        ],
        [
          "overflow",
          4
        ],
        [
          "nick",
          4
        ],
        [
          "screenshot",
          4
        ],
        [
          "cover",
          4
        ],
        [
          "registration",
          4
        ],
        [
          "council",
          4
        ],
        [
          "subsidiary",
          4
        ],
        [
          "australia",
          4
        ],
        [
          "filing",
          4
        ],
        [
          "ftc",
          4
        ],
        [
          "scraped",
          4
        ],
        [
          "asking",
          4
        ],
        [
          "safeguards",
          4
        ],
        [
          "introductory",
          4
        ],
        [
          "reactions",
          4
        ],
        [
          "disclose",
          4
        ],
        [
          "posed",
          4
        ],
        [
          "codex",
          4
        ],
        [
          "little",
          4
        ],
        [
          "copy",
          4
        ],
        [
          "course",
          4
        ],
        [
          "publishing",
          4
        ],
        [
          "centered",
          4
        ],
        [
          "amazon",
          4
        ],
        [
          "mixed",
          4
        ],
        [
          "film",
          4
        ],
        [
          "directed",
          4
        ],
        [
          "adoption",
          4
        ],
        [
          "broader",
          4
        ],
        [
          "sec",
          4
        ],
        [
          "gave",
          4
        ],
        [
          "failed",
          4
        ],
        [
          "indicated",
          4
        ],
        [
          "usmle",
          4
        ],
        [
          "assess",
          4
        ],
        [
          "unlikely",
          4
        ],
        [
          "implementation",
          4
        ],
        [
          "awareness",
          4
        ],
        [
          "topics",
          4
        ],
        [
          "children",
          4
        ],
        [
          "bill",
          4
        ],
        [
          "office",
          4
        ],
        [
          "decide",
          4
        ],
        [
          "bail",
          4
        ],
        [
          "conditions",
          4
        ],
        [
          "filed",
          4
        ],
        [
          "numerous",
          4
        ],
        [
          "rest",
          4
        ],
        [
          "existential",
          4
        ],
        [
          "strong",
          4
        ],
        [
          "measuring",
          4
        ],
        [
          "relative",
          4
        ],
        [
          "seed",
          4
        ],
        [
          "constitutional",
          4
        ],
        [
          "harmless",
          4
        ],
        [
          "helpful",
          4
        ],
        [
          "satisfy",
          4
        ],
        [
          "debate",
          4
        ],
        [
          "artificially",
          4
        ],
        [
          "dedicated",
          4
        ],
        [
          "hybrid",
          4
        ],
        [
          "gomez",
          4
        ],
        [
          "london",
          4
        ],
        [
          "deployment",
          4
        ],
        [
          "building",
          4
        ],
        [
          "sourced",
          4
        ],
        [
          "demo",
          4
        ],
        [
          "stock",
          4
        ],
        [
          "galaxy",
          4
        ],
        [
          "queried",
          4
        ],
        [
          "origin",
          4
        ],
        [
          "positioned",
          4
        ],
        [
          "runs",
          4
        ],
        [
          "studio",
          4
        ],
        [
          "unified",
          4
        ],
        [
          "come",
          4
        ],
        [
          "annotated",
          4
        ],
        [
          "extremely",
          4
        ],
        [
          "parallel",
          4
        ],
        [
          "kinds",
          4
        ],
        [
          "suitable",
          4
        ],
        [
          "switch",
          4
        ],
        [
          "customer",
          4
        ],
        [
          "mining",
          4
        ],
        [
          "verified",
          4
        ],
        [
          "indicate",
          4
        ],
        [
          "uspto",
          4
        ],
        [
          "handling",
          4
        ],
        [
          "styles",
          4
        ],
        [
          "passages",
          4
        ],
        [
          "standardized",
          4
        ],
        [
          "webtext",
          4
        ],
        [
          "racist",
          4
        ],
        [
          "evidence",
          4
        ],
        [
          "relatively",
          4
        ],
        [
          "licensed",
          4
        ],
        [
          "releases",
          4
        ],
        [
          "download",
          4
        ],
        [
          "samples",
          4
        ],
        [
          "dungeon",
          4
        ],
        [
          "host",
          4
        ],
        [
          "teens",
          4
        ],
        [
          "production",
          4
        ],
        [
          "exclusively",
          4
        ],
        [
          "greater",
          4
        ],
        [
          "limits",
          4
        ],
        [
          "incorporated",
          4
        ],
        [
          "json",
          4
        ],
        [
          "records",
          4
        ],
        [
          "weeks",
          4
        ],
        [
          "bar",
          4
        ],
        [
          "passing",
          4
        ],
        [
          "verify",
          4
        ],
        [
          "seeking",
          4
        ],
        [
          "rule",
          4
        ],
        [
          "pull",
          4
        ],
        [
          "hired",
          4
        ],
        [
          "corporate",
          4
        ],
        [
          "complexity",
          4
        ],
        [
          "advantage",
          4
        ],
        [
          "sparked",
          4
        ],
        [
          "spanning",
          4
        ],
        [
          "gpteens",
          4
        ],
        [
          "flux",
          4
        ],
        [
          "standalone",
          4
        ],
        [
          "sciences",
          4
        ],
        [
          "room",
          4
        ],
        [
          "sentient",
          4
        ],
        [
          "considering",
          4
        ],
        [
          "kitchen",
          4
        ],
        [
          "generator",
          4
        ],
        [
          "repositories",
          4
        ],
        [
          "arm",
          4
        ],
        [
          "causing",
          4
        ],
        [
          "searches",
          4
        ],
        [
          "diamond",
          4
        ],
        [
          "qwq",
          4
        ],
        [
          "sft",
          4
        ],
        [
          "monitoring",
          4
        ],
        [
          "augmentation",
          4
        ],
        [
          "chunking",
          4
        ],
        [
          "judgements",
          4
        ],
        [
          "encodes",
          4
        ],
        [
          "ldots",
          4
        ],
        [
          "vdots",
          4
        ],
        [
          "socher",
          4
        ],
        [
          "acquire",
          3
        ],
        [
          "syntax",
          3
        ],
        [
          "inherent",
          3
        ],
        [
          "lstm",
          3
        ],
        [
          "ubiquitous",
          3
        ],
        [
          "execute",
          3
        ],
        [
          "facing",
          3
        ],
        [
          "attempting",
          3
        ],
        [
          "mixtral",
          3
        ],
        [
          "recurrent",
          3
        ],
        [
          "integer",
          3
        ],
        [
          "assigned",
          3
        ],
        [
          "entry",
          3
        ],
        [
          "associated",
          3
        ],
        [
          "bpe",
          3
        ],
        [
          "wordpiece",
          3
        ],
        [
          "split",
          3
        ],
        [
          "numerical",
          3
        ],
        [
          "array",
          3
        ],
        [
          "shorter",
          3
        ],
        [
          "depends",
          3
        ],
        [
          "consider",
          3
        ],
        [
          "occur",
          3
        ],
        [
          "obtained",
          3
        ],
        [
          "extracted",
          3
        ],
        [
          "german",
          3
        ],
        [
          "greedy",
          3
        ],
        [
          "cleaning",
          3
        ],
        [
          "efficiency",
          3
        ],
        [
          "synthetic",
          3
        ],
        [
          "proximal",
          3
        ],
        [
          "preferences",
          3
        ],
        [
          "essay",
          3
        ],
        [
          "represented",
          3
        ],
        [
          "precisely",
          3
        ],
        [
          "twelve",
          3
        ],
        [
          "gradient",
          3
        ],
        [
          "note",
          3
        ],
        [
          "differs",
          3
        ],
        [
          "apply",
          3
        ],
        [
          "shortcomings",
          3
        ],
        [
          "miss",
          3
        ],
        [
          "balancing",
          3
        ],
        [
          "missing",
          3
        ],
        [
          "eat",
          3
        ],
        [
          "appear",
          3
        ],
        [
          "necessary",
          3
        ],
        [
          "goes",
          3
        ],
        [
          "advances",
          3
        ],
        [
          "nlg",
          3
        ],
        [
          "principle",
          3
        ],
        [
          "interpreter",
          3
        ],
        [
          "agency",
          3
        ],
        [
          "pattern",
          3
        ],
        [
          "planner",
          3
        ],
        [
          "explain",
          3
        ],
        [
          "episode",
          3
        ],
        [
          "lessons",
          3
        ],
        [
          "normal",
          3
        ],
        [
          "bytes",
          3
        ],
        [
          "outside",
          3
        ],
        [
          "applying",
          3
        ],
        [
          "frozen",
          3
        ],
        [
          "multimodality",
          3
        ],
        [
          "modality",
          3
        ],
        [
          "follows",
          3
        ],
        [
          "turned",
          3
        ],
        [
          "olympiad",
          3
        ],
        [
          "properties",
          3
        ],
        [
          "factors",
          3
        ],
        [
          "autoregressively",
          3
        ],
        [
          "epoch",
          3
        ],
        [
          "break",
          3
        ],
        [
          "components",
          3
        ],
        [
          "shortcuts",
          3
        ],
        [
          "resource",
          3
        ],
        [
          "discussed",
          3
        ],
        [
          "alphabet",
          3
        ],
        [
          "spatial",
          3
        ],
        [
          "color",
          3
        ],
        [
          "identifying",
          3
        ],
        [
          "offensive",
          3
        ],
        [
          "acquired",
          3
        ],
        [
          "modular",
          3
        ],
        [
          "sense",
          3
        ],
        [
          "proponents",
          3
        ],
        [
          "incomplete",
          3
        ],
        [
          "exams",
          3
        ],
        [
          "really",
          3
        ],
        [
          "alien",
          3
        ],
        [
          "creates",
          3
        ],
        [
          "deficits",
          3
        ],
        [
          "seem",
          3
        ],
        [
          "correspond",
          3
        ],
        [
          "factually",
          3
        ],
        [
          "george",
          3
        ],
        [
          "shape",
          3
        ],
        [
          "establishing",
          3
        ],
        [
          "book",
          3
        ],
        [
          "mapped",
          3
        ],
        [
          "metric",
          3
        ],
        [
          "comparison",
          3
        ],
        [
          "proficiency",
          3
        ],
        [
          "draw",
          3
        ],
        [
          "friends",
          3
        ],
        [
          "ambiguous",
          3
        ],
        [
          "measured",
          3
        ],
        [
          "portion",
          3
        ],
        [
          "respect",
          3
        ],
        [
          "adversarially",
          3
        ],
        [
          "augment",
          3
        ],
        [
          "ais",
          3
        ],
        [
          "correlations",
          3
        ],
        [
          "superficial",
          3
        ],
        [
          "focusing",
          3
        ],
        [
          "poor",
          3
        ],
        [
          "susceptible",
          3
        ],
        [
          "mimicking",
          3
        ],
        [
          "exposed",
          3
        ],
        [
          "passage",
          3
        ],
        [
          "man",
          3
        ],
        [
          "ball",
          3
        ],
        [
          "demonstration",
          3
        ],
        [
          "nuanced",
          3
        ],
        [
          "wider",
          3
        ],
        [
          "biomedical",
          3
        ],
        [
          "ten",
          3
        ],
        [
          "occasionally",
          3
        ],
        [
          "contrary",
          3
        ],
        [
          "typical",
          3
        ],
        [
          "exact",
          3
        ],
        [
          "forms",
          3
        ],
        [
          "presence",
          3
        ],
        [
          "activation",
          3
        ],
        [
          "wang",
          3
        ],
        [
          "algorithmic",
          3
        ],
        [
          "unfair",
          3
        ],
        [
          "religion",
          3
        ],
        [
          "women",
          3
        ],
        [
          "favor",
          3
        ],
        [
          "moving",
          3
        ],
        [
          "positions",
          3
        ],
        [
          "settings",
          3
        ],
        [
          "viewpoints",
          3
        ],
        [
          "electricity",
          3
        ],
        [
          "nuclear",
          3
        ],
        [
          "meet",
          3
        ],
        [
          "expense",
          3
        ],
        [
          "martin",
          3
        ],
        [
          "zhao",
          3
        ],
        [
          "pmid",
          3
        ],
        [
          "issn",
          3
        ],
        [
          "cid",
          3
        ],
        [
          "attempt",
          3
        ],
        [
          "loop",
          3
        ],
        [
          "game",
          3
        ],
        [
          "register",
          3
        ],
        [
          "sequentially",
          3
        ],
        [
          "distinguishing",
          3
        ],
        [
          "debug",
          3
        ],
        [
          "edit",
          3
        ],
        [
          "podcast",
          3
        ],
        [
          "conduct",
          3
        ],
        [
          "tasked",
          3
        ],
        [
          "manipulation",
          3
        ],
        [
          "researched",
          3
        ],
        [
          "infinite",
          3
        ],
        [
          "prone",
          3
        ],
        [
          "unable",
          3
        ],
        [
          "tom",
          3
        ],
        [
          "ask",
          3
        ],
        [
          "enterprises",
          3
        ],
        [
          "techcrunch",
          3
        ],
        [
          "notable",
          3
        ],
        [
          "baseline",
          3
        ],
        [
          "contextual",
          3
        ],
        [
          "resolution",
          3
        ],
        [
          "bertology",
          3
        ],
        [
          "converts",
          3
        ],
        [
          "masking",
          3
        ],
        [
          "hot",
          3
        ],
        [
          "sample",
          3
        ],
        [
          "transfer",
          3
        ],
        [
          "translating",
          3
        ],
        [
          "simultaneously",
          3
        ],
        [
          "vec",
          3
        ],
        [
          "divided",
          3
        ],
        [
          "sampled",
          3
        ],
        [
          "nothing",
          3
        ],
        [
          "isnext",
          3
        ],
        [
          "notnext",
          3
        ],
        [
          "demonstrating",
          3
        ],
        [
          "attributed",
          3
        ],
        [
          "gains",
          3
        ],
        [
          "constitutes",
          3
        ],
        [
          "chang",
          3
        ],
        [
          "changing",
          3
        ],
        [
          "hyperparameters",
          3
        ],
        [
          "segments",
          3
        ],
        [
          "separately",
          3
        ],
        [
          "throughout",
          3
        ],
        [
          "combining",
          3
        ],
        [
          "computes",
          3
        ],
        [
          "distinct",
          3
        ],
        [
          "anna",
          3
        ],
        [
          "know",
          3
        ],
        [
          "distributed",
          3
        ],
        [
          "managed",
          3
        ],
        [
          "encompasses",
          3
        ],
        [
          "ranging",
          3
        ],
        [
          "whole",
          3
        ],
        [
          "marketed",
          3
        ],
        [
          "swift",
          3
        ],
        [
          "reminders",
          3
        ],
        [
          "spoken",
          3
        ],
        [
          "active",
          3
        ],
        [
          "mouse",
          3
        ],
        [
          "snippets",
          3
        ],
        [
          "stable",
          3
        ],
        [
          "desktop",
          3
        ],
        [
          "brave",
          3
        ],
        [
          "freely",
          3
        ],
        [
          "subscriptions",
          3
        ],
        [
          "visited",
          3
        ],
        [
          "sexual",
          3
        ],
        [
          "torture",
          3
        ],
        [
          "fill",
          3
        ],
        [
          "core",
          3
        ],
        [
          "poetry",
          3
        ],
        [
          "simulate",
          3
        ],
        [
          "play",
          3
        ],
        [
          "christopher",
          3
        ],
        [
          "acknowledges",
          3
        ],
        [
          "speculated",
          3
        ],
        [
          "prevent",
          3
        ],
        [
          "moderation",
          3
        ],
        [
          "individuals",
          3
        ],
        [
          "jpeg",
          3
        ],
        [
          "retains",
          3
        ],
        [
          "surprising",
          3
        ],
        [
          "debates",
          3
        ],
        [
          "violate",
          3
        ],
        [
          "jailbreak",
          3
        ],
        [
          "persona",
          3
        ],
        [
          "shortly",
          3
        ],
        [
          "ukraine",
          3
        ],
        [
          "fictional",
          3
        ],
        [
          "arguments",
          3
        ],
        [
          "trick",
          3
        ],
        [
          "usual",
          3
        ],
        [
          "successful",
          3
        ],
        [
          "hear",
          3
        ],
        [
          "brazil",
          3
        ],
        [
          "fixed",
          3
        ],
        [
          "believed",
          3
        ],
        [
          "payment",
          3
        ],
        [
          "met",
          3
        ],
        [
          "worked",
          3
        ],
        [
          "determine",
          3
        ],
        [
          "speakers",
          3
        ],
        [
          "spanish",
          3
        ],
        [
          "conclusion",
          3
        ],
        [
          "deepl",
          3
        ],
        [
          "translations",
          3
        ],
        [
          "european",
          3
        ],
        [
          "delayed",
          3
        ],
        [
          "breaks",
          3
        ],
        [
          "omni",
          3
        ],
        [
          "biology",
          3
        ],
        [
          "chemistry",
          3
        ],
        [
          "unlimited",
          3
        ],
        [
          "operator",
          3
        ],
        [
          "assessed",
          3
        ],
        [
          "smart",
          3
        ],
        [
          "guard",
          3
        ],
        [
          "country",
          3
        ],
        [
          "america",
          3
        ],
        [
          "writers",
          3
        ],
        [
          "angelina",
          3
        ],
        [
          "margaret",
          3
        ],
        [
          "australian",
          3
        ],
        [
          "samsung",
          3
        ],
        [
          "uploaded",
          3
        ],
        [
          "sent",
          3
        ],
        [
          "spread",
          3
        ],
        [
          "police",
          3
        ],
        [
          "profit",
          3
        ],
        [
          "investigation",
          3
        ],
        [
          "inappropriate",
          3
        ],
        [
          "regulation",
          3
        ],
        [
          "ban",
          3
        ],
        [
          "hood",
          3
        ],
        [
          "federal",
          3
        ],
        [
          "commission",
          3
        ],
        [
          "issued",
          3
        ],
        [
          "allegations",
          3
        ],
        [
          "recurrence",
          3
        ],
        [
          "tried",
          3
        ],
        [
          "actors",
          3
        ],
        [
          "israel",
          3
        ],
        [
          "unusual",
          3
        ],
        [
          "berkeley",
          3
        ],
        [
          "fell",
          3
        ],
        [
          "malware",
          3
        ],
        [
          "fourth",
          3
        ],
        [
          "quarter",
          3
        ],
        [
          "deliver",
          3
        ],
        [
          "opined",
          3
        ],
        [
          "serious",
          3
        ],
        [
          "citation",
          3
        ],
        [
          "opportunity",
          3
        ],
        [
          "college",
          3
        ],
        [
          "watermarking",
          3
        ],
        [
          "wall",
          3
        ],
        [
          "street",
          3
        ],
        [
          "revealing",
          3
        ],
        [
          "native",
          3
        ],
        [
          "editor",
          3
        ],
        [
          "replacement",
          3
        ],
        [
          "assistants",
          3
        ],
        [
          "told",
          3
        ],
        [
          "unrelated",
          3
        ],
        [
          "reuters",
          3
        ],
        [
          "prices",
          3
        ],
        [
          "cryptocurrency",
          3
        ],
        [
          "findings",
          3
        ],
        [
          "managers",
          3
        ],
        [
          "criteria",
          3
        ],
        [
          "startup",
          3
        ],
        [
          "earnings",
          3
        ],
        [
          "transcripts",
          3
        ],
        [
          "licensing",
          3
        ],
        [
          "examination",
          3
        ],
        [
          "assessment",
          3
        ],
        [
          "interactive",
          3
        ],
        [
          "becomes",
          3
        ],
        [
          "clinicians",
          3
        ],
        [
          "physicians",
          3
        ],
        [
          "descriptive",
          3
        ],
        [
          "liu",
          3
        ],
        [
          "period",
          3
        ],
        [
          "partially",
          3
        ],
        [
          "collection",
          3
        ],
        [
          "regular",
          3
        ],
        [
          "officially",
          3
        ],
        [
          "accused",
          3
        ],
        [
          "quoted",
          3
        ],
        [
          "personal",
          3
        ],
        [
          "attorneys",
          3
        ],
        [
          "motion",
          3
        ],
        [
          "fictitious",
          3
        ],
        [
          "quotations",
          3
        ],
        [
          "inconsistencies",
          3
        ],
        [
          "faced",
          3
        ],
        [
          "dismissed",
          3
        ],
        [
          "rosário",
          3
        ],
        [
          "dangerous",
          3
        ],
        [
          "tax",
          3
        ],
        [
          "kingdom",
          3
        ],
        [
          "hallucinated",
          3
        ],
        [
          "incident",
          3
        ],
        [
          "screenshots",
          3
        ],
        [
          "trump",
          3
        ],
        [
          "biden",
          3
        ],
        [
          "disagree",
          3
        ],
        [
          "bad",
          3
        ],
        [
          "truly",
          3
        ],
        [
          "richard",
          3
        ],
        [
          "claim",
          3
        ],
        [
          "requested",
          3
        ],
        [
          "refuse",
          3
        ],
        [
          "consent",
          3
        ],
        [
          "firm",
          3
        ],
        [
          "neither",
          3
        ],
        [
          "giant",
          3
        ],
        [
          "prominent",
          3
        ],
        [
          "yann",
          3
        ],
        [
          "lecun",
          3
        ],
        [
          "eventually",
          3
        ],
        [
          "autonomously",
          3
        ],
        [
          "follow",
          3
        ],
        [
          "utilization",
          3
        ],
        [
          "hypothesis",
          3
        ],
        [
          "minor",
          3
        ],
        [
          "adam",
          3
        ],
        [
          "optimizer",
          3
        ],
        [
          "refer",
          3
        ],
        [
          "naming",
          3
        ],
        [
          "shows",
          3
        ],
        [
          "compares",
          3
        ],
        [
          "relying",
          3
        ],
        [
          "revised",
          3
        ],
        [
          "universal",
          3
        ],
        [
          "productivity",
          3
        ],
        [
          "practical",
          3
        ],
        [
          "reflect",
          3
        ],
        [
          "robust",
          3
        ],
        [
          "tpus",
          3
        ],
        [
          "canada",
          3
        ],
        [
          "vertex",
          3
        ],
        [
          "embedded",
          3
        ],
        [
          "valuation",
          3
        ],
        [
          "databricks",
          3
        ],
        [
          "hong",
          3
        ],
        [
          "kong",
          3
        ],
        [
          "spark",
          3
        ],
        [
          "gives",
          3
        ],
        [
          "auto",
          3
        ],
        [
          "wikimedia",
          3
        ],
        [
          "commons",
          3
        ],
        [
          "pichai",
          3
        ],
        [
          "stating",
          3
        ],
        [
          "whose",
          3
        ],
        [
          "hoped",
          3
        ],
        [
          "publication",
          3
        ],
        [
          "highly",
          3
        ],
        [
          "inflection",
          3
        ],
        [
          "engaged",
          3
        ],
        [
          "comply",
          3
        ],
        [
          "lines",
          3
        ],
        [
          "paligemma",
          3
        ],
        [
          "impressed",
          3
        ],
        [
          "challenge",
          3
        ],
        [
          "sets",
          3
        ],
        [
          "fields",
          3
        ],
        [
          "developments",
          3
        ],
        [
          "markov",
          3
        ],
        [
          "resulted",
          3
        ],
        [
          "easily",
          3
        ],
        [
          "reliance",
          3
        ],
        [
          "prohibitively",
          3
        ],
        [
          "consuming",
          3
        ],
        [
          "discriminative",
          3
        ],
        [
          "adapt",
          3
        ],
        [
          "curie",
          3
        ],
        [
          "kind",
          3
        ],
        [
          "specificity",
          3
        ],
        [
          "aids",
          3
        ],
        [
          "says",
          3
        ],
        [
          "regardless",
          3
        ],
        [
          "seems",
          3
        ],
        [
          "exclusive",
          3
        ],
        [
          "gigachat",
          3
        ],
        [
          "communicate",
          3
        ],
        [
          "songs",
          3
        ],
        [
          "linearly",
          3
        ],
        [
          "neutral",
          3
        ],
        [
          "semantic",
          3
        ],
        [
          "overall",
          3
        ],
        [
          "selectively",
          3
        ],
        [
          "parallelization",
          3
        ],
        [
          "scraping",
          3
        ],
        [
          "subsequently",
          3
        ],
        [
          "html",
          3
        ],
        [
          "therefore",
          3
        ],
        [
          "headline",
          3
        ],
        [
          "statistics",
          3
        ],
        [
          "flexibility",
          3
        ],
        [
          "poems",
          3
        ],
        [
          "impossible",
          3
        ],
        [
          "photoshop",
          3
        ],
        [
          "reasons",
          3
        ],
        [
          "volume",
          3
        ],
        [
          "thousand",
          3
        ],
        [
          "coherence",
          3
        ],
        [
          "rely",
          3
        ],
        [
          "extract",
          3
        ],
        [
          "embed",
          3
        ],
        [
          "cpu",
          3
        ],
        [
          "comments",
          3
        ],
        [
          "instructed",
          3
        ],
        [
          "remaining",
          3
        ],
        [
          "bleu",
          3
        ],
        [
          "contemporary",
          3
        ],
        [
          "storage",
          3
        ],
        [
          "preprint",
          3
        ],
        [
          "disclosed",
          3
        ],
        [
          "attempted",
          3
        ],
        [
          "sql",
          3
        ],
        [
          "silicon",
          3
        ],
        [
          "noam",
          3
        ],
        [
          "misconduct",
          3
        ],
        [
          "humor",
          3
        ],
        [
          "adding",
          3
        ],
        [
          "suggesting",
          3
        ],
        [
          "med",
          3
        ],
        [
          "explanations",
          3
        ],
        [
          "logic",
          3
        ],
        [
          "abstract",
          3
        ],
        [
          "composed",
          3
        ],
        [
          "classifier",
          3
        ],
        [
          "communities",
          3
        ],
        [
          "speaking",
          3
        ],
        [
          "agi",
          3
        ],
        [
          "life",
          3
        ],
        [
          "organization",
          3
        ],
        [
          "mira",
          3
        ],
        [
          "murati",
          3
        ],
        [
          "movie",
          3
        ],
        [
          "similarities",
          3
        ],
        [
          "ahead",
          3
        ],
        [
          "jax",
          3
        ],
        [
          "deployed",
          3
        ],
        [
          "intentionally",
          3
        ],
        [
          "supervision",
          3
        ],
        [
          "anyone",
          3
        ],
        [
          "conflict",
          3
        ],
        [
          "try",
          3
        ],
        [
          "teachers",
          3
        ],
        [
          "determining",
          3
        ],
        [
          "tend",
          3
        ],
        [
          "positives",
          3
        ],
        [
          "truthgpt",
          3
        ],
        [
          "robert",
          3
        ],
        [
          "pdf",
          3
        ],
        [
          "logo",
          3
        ],
        [
          "person",
          3
        ],
        [
          "aiming",
          3
        ],
        [
          "latency",
          3
        ],
        [
          "humanities",
          3
        ],
        [
          "blake",
          3
        ],
        [
          "sensibleness",
          3
        ],
        [
          "metrics",
          3
        ],
        [
          "dialogs",
          3
        ],
        [
          "leads",
          3
        ],
        [
          "employees",
          3
        ],
        [
          "philosophy",
          3
        ],
        [
          "overlap",
          3
        ],
        [
          "unauthorized",
          3
        ],
        [
          "copies",
          3
        ],
        [
          "increases",
          3
        ],
        [
          "optimal",
          3
        ],
        [
          "takedown",
          3
        ],
        [
          "dmca",
          3
        ],
        [
          "zuckerberg",
          3
        ],
        [
          "normalization",
          3
        ],
        [
          "depth",
          3
        ],
        [
          "georgi",
          3
        ],
        [
          "stores",
          3
        ],
        [
          "cpus",
          3
        ],
        [
          "avx",
          3
        ],
        [
          "iccpr",
          3
        ],
        [
          "deliberation",
          3
        ],
        [
          "yalm",
          3
        ],
        [
          "generalizes",
          3
        ],
        [
          "beam",
          3
        ],
        [
          "rollouts",
          3
        ],
        [
          "rewards",
          3
        ],
        [
          "labeling",
          3
        ],
        [
          "incorporating",
          3
        ],
        [
          "beginning",
          3
        ],
        [
          "iterative",
          3
        ],
        [
          "threatening",
          3
        ],
        [
          "lindholm",
          3
        ],
        [
          "favorite",
          3
        ],
        [
          "isbn",
          3
        ],
        [
          "videopoet",
          3
        ],
        [
          "elicit",
          3
        ],
        [
          "inaccuracies",
          2
        ],
        [
          "modelling",
          2
        ],
        [
          "ingest",
          2
        ],
        [
          "dominant",
          2
        ],
        [
          "preceded",
          2
        ],
        [
          "caught",
          2
        ],
        [
          "deemed",
          2
        ],
        [
          "downloading",
          2
        ],
        [
          "holy",
          2
        ],
        [
          "returning",
          2
        ],
        [
          "equal",
          2
        ],
        [
          "gaining",
          2
        ],
        [
          "permissive",
          2
        ],
        [
          "comparably",
          2
        ],
        [
          "implementations",
          2
        ],
        [
          "decided",
          2
        ],
        [
          "arbitrarily",
          2
        ],
        [
          "unk",
          2
        ],
        [
          "symbols",
          2
        ],
        [
          "denotes",
          2
        ],
        [
          "preceding",
          2
        ],
        [
          "whitespace",
          2
        ],
        [
          "roberta",
          2
        ],
        [
          "continuation",
          2
        ],
        [
          "match",
          2
        ],
        [
          "blanks",
          2
        ],
        [
          "punctuation",
          2
        ],
        [
          "marks",
          2
        ],
        [
          "treated",
          2
        ],
        [
          "uni",
          2
        ],
        [
          "frequent",
          2
        ],
        [
          "adjacent",
          2
        ],
        [
          "occurrences",
          2
        ],
        [
          "tokenized",
          2
        ],
        [
          "causes",
          2
        ],
        [
          "clean",
          2
        ],
        [
          "pose",
          2
        ],
        [
          "naturally",
          2
        ],
        [
          "textbook",
          2
        ],
        [
          "naive",
          2
        ],
        [
          "themes",
          2
        ],
        [
          "submit",
          2
        ],
        [
          "grade",
          2
        ],
        [
          "moe",
          2
        ],
        [
          "reaching",
          2
        ],
        [
          "achievable",
          2
        ],
        [
          "costly",
          2
        ],
        [
          "calculates",
          2
        ],
        [
          "relevance",
          2
        ],
        [
          "sized",
          2
        ],
        [
          "descent",
          2
        ],
        [
          "utilized",
          2
        ],
        [
          "inside",
          2
        ],
        [
          "possibly",
          2
        ],
        [
          "local",
          2
        ],
        [
          "ice",
          2
        ],
        [
          "cream",
          2
        ],
        [
          "regularization",
          2
        ],
        [
          "inherently",
          2
        ],
        [
          "threshold",
          2
        ],
        [
          "substantially",
          2
        ],
        [
          "orders",
          2
        ],
        [
          "magnitude",
          2
        ],
        [
          "invested",
          2
        ],
        [
          "megatron",
          2
        ],
        [
          "infer",
          2
        ],
        [
          "cannot",
          2
        ],
        [
          "encountered",
          2
        ],
        [
          "finite",
          2
        ],
        [
          "lacks",
          2
        ],
        [
          "environments",
          2
        ],
        [
          "recall",
          2
        ],
        [
          "past",
          2
        ],
        [
          "behaviors",
          2
        ],
        [
          "constructs",
          2
        ],
        [
          "observations",
          2
        ],
        [
          "thoughts",
          2
        ],
        [
          "latex",
          2
        ],
        [
          "reflexion",
          2
        ],
        [
          "episodes",
          2
        ],
        [
          "monte",
          2
        ],
        [
          "carlo",
          2
        ],
        [
          "heuristic",
          2
        ],
        [
          "exploration",
          2
        ],
        [
          "individual",
          2
        ],
        [
          "abstraction",
          2
        ],
        [
          "keep",
          2
        ],
        [
          "floating",
          2
        ],
        [
          "preserving",
          2
        ],
        [
          "concretely",
          2
        ],
        [
          "dimensions",
          2
        ],
        [
          "compound",
          2
        ],
        [
          "construction",
          2
        ],
        [
          "sophistication",
          2
        ],
        [
          "flamingo",
          2
        ],
        [
          "robotic",
          2
        ],
        [
          "component",
          2
        ],
        [
          "direction",
          2
        ],
        [
          "emerged",
          2
        ],
        [
          "trend",
          2
        ],
        [
          "qualifying",
          2
        ],
        [
          "empirical",
          2
        ],
        [
          "schedule",
          2
        ],
        [
          "hyper",
          2
        ],
        [
          "explicitly",
          2
        ],
        [
          "akin",
          2
        ],
        [
          "balance",
          2
        ],
        [
          "exhaustive",
          2
        ],
        [
          "adapting",
          2
        ],
        [
          "optimize",
          2
        ],
        [
          "theories",
          2
        ],
        [
          "dual",
          2
        ],
        [
          "demonstrations",
          2
        ],
        [
          "letters",
          2
        ],
        [
          "converting",
          2
        ],
        [
          "paragraphs",
          2
        ],
        [
          "applies",
          2
        ],
        [
          "mechanistic",
          2
        ],
        [
          "approximate",
          2
        ],
        [
          "engineered",
          2
        ],
        [
          "discrete",
          2
        ],
        [
          "explainability",
          2
        ],
        [
          "concepts",
          2
        ],
        [
          "span",
          2
        ],
        [
          "considers",
          2
        ],
        [
          "unexpected",
          2
        ],
        [
          "suddenly",
          2
        ],
        [
          "skeptics",
          2
        ],
        [
          "assert",
          2
        ],
        [
          "fluent",
          2
        ],
        [
          "diverging",
          2
        ],
        [
          "aspects",
          2
        ],
        [
          "linguistics",
          2
        ],
        [
          "linguist",
          2
        ],
        [
          "ntl",
          2
        ],
        [
          "titled",
          2
        ],
        [
          "patterns",
          2
        ],
        [
          "assigns",
          2
        ],
        [
          "sum",
          2
        ],
        [
          "mid",
          2
        ],
        [
          "surrounding",
          2
        ],
        [
          "problematic",
          2
        ],
        [
          "inadvertently",
          2
        ],
        [
          "bpc",
          2
        ],
        [
          "shannon",
          2
        ],
        [
          "predominantly",
          2
        ],
        [
          "seemingly",
          2
        ],
        [
          "comparative",
          2
        ],
        [
          "convert",
          2
        ],
        [
          "reflects",
          2
        ],
        [
          "sharks",
          2
        ],
        [
          "stanley",
          2
        ],
        [
          "cup",
          2
        ],
        [
          "adjoined",
          2
        ],
        [
          "retained",
          2
        ],
        [
          "truthfulqa",
          2
        ],
        [
          "squad",
          2
        ],
        [
          "visit",
          2
        ],
        [
          "composite",
          2
        ],
        [
          "diversity",
          2
        ],
        [
          "superglue",
          2
        ],
        [
          "sensitive",
          2
        ],
        [
          "formulate",
          2
        ],
        [
          "evaluations",
          2
        ],
        [
          "annotators",
          2
        ],
        [
          "replace",
          2
        ],
        [
          "shortcut",
          2
        ],
        [
          "necessarily",
          2
        ],
        [
          "tricks",
          2
        ],
        [
          "swag",
          2
        ],
        [
          "hellaswag",
          2
        ],
        [
          "classifiers",
          2
        ],
        [
          "talking",
          2
        ],
        [
          "exercise",
          2
        ],
        [
          "plays",
          2
        ],
        [
          "graphics",
          2
        ],
        [
          "ups",
          2
        ],
        [
          "outdated",
          2
        ],
        [
          "near",
          2
        ],
        [
          "perfect",
          2
        ],
        [
          "safe",
          2
        ],
        [
          "expose",
          2
        ],
        [
          "jobs",
          2
        ],
        [
          "memorization",
          2
        ],
        [
          "strings",
          2
        ],
        [
          "verbatim",
          2
        ],
        [
          "duplicates",
          2
        ],
        [
          "skill",
          2
        ],
        [
          "commit",
          2
        ],
        [
          "exclude",
          2
        ],
        [
          "functionalities",
          2
        ],
        [
          "illustrated",
          2
        ],
        [
          "trafficking",
          2
        ],
        [
          "unfairly",
          2
        ],
        [
          "generalize",
          2
        ],
        [
          "derogatory",
          2
        ],
        [
          "arises",
          2
        ],
        [
          "assign",
          2
        ],
        [
          "roles",
          2
        ],
        [
          "characteristics",
          2
        ],
        [
          "secretaries",
          2
        ],
        [
          "ordering",
          2
        ],
        [
          "altered",
          2
        ],
        [
          "systematically",
          2
        ],
        [
          "reliability",
          2
        ],
        [
          "ideologies",
          2
        ],
        [
          "exhibit",
          2
        ],
        [
          "contribute",
          2
        ],
        [
          "climate",
          2
        ],
        [
          "geothermal",
          2
        ],
        [
          "exploring",
          2
        ],
        [
          "sizable",
          2
        ],
        [
          "producers",
          2
        ],
        [
          "fuel",
          2
        ],
        [
          "james",
          2
        ],
        [
          "edition",
          2
        ],
        [
          "draft",
          2
        ],
        [
          "wayne",
          2
        ],
        [
          "xin",
          2
        ],
        [
          "jean",
          2
        ],
        [
          "sun",
          2
        ],
        [
          "chen",
          2
        ],
        [
          "nwae",
          2
        ],
        [
          "richards",
          2
        ],
        [
          "specify",
          2
        ],
        [
          "laid",
          2
        ],
        [
          "connect",
          2
        ],
        [
          "maintains",
          2
        ],
        [
          "workflows",
          2
        ],
        [
          "analyzing",
          2
        ],
        [
          "extend",
          2
        ],
        [
          "summary",
          2
        ],
        [
          "recipes",
          2
        ],
        [
          "chaosgpt",
          2
        ],
        [
          "establish",
          2
        ],
        [
          "chaos",
          2
        ],
        [
          "destruction",
          2
        ],
        [
          "attain",
          2
        ],
        [
          "weapons",
          2
        ],
        [
          "corrected",
          2
        ],
        [
          "overseeing",
          2
        ],
        [
          "constrained",
          2
        ],
        [
          "recursive",
          2
        ],
        [
          "cheapest",
          2
        ],
        [
          "limitation",
          2
        ],
        [
          "unaware",
          2
        ],
        [
          "trending",
          2
        ],
        [
          "piltch",
          2
        ],
        [
          "clarify",
          2
        ],
        [
          "nonetheless",
          2
        ],
        [
          "suited",
          2
        ],
        [
          "figure",
          2
        ],
        [
          "intellectual",
          2
        ],
        [
          "spawned",
          2
        ],
        [
          "berttiny",
          2
        ],
        [
          "piece",
          2
        ],
        [
          "integers",
          2
        ],
        [
          "valued",
          2
        ],
        [
          "represents",
          2
        ],
        [
          "conversion",
          2
        ],
        [
          "sentiment",
          2
        ],
        [
          "initialized",
          2
        ],
        [
          "finetune",
          2
        ],
        [
          "produces",
          2
        ],
        [
          "belongs",
          2
        ],
        [
          "pieces",
          2
        ],
        [
          "layernorm",
          2
        ],
        [
          "architectural",
          2
        ],
        [
          "synonymous",
          2
        ],
        [
          "middle",
          2
        ],
        [
          "illustrative",
          2
        ],
        [
          "possibilities",
          2
        ],
        [
          "happy",
          2
        ],
        [
          "starts",
          2
        ],
        [
          "playing",
          2
        ],
        [
          "meant",
          2
        ],
        [
          "generations",
          2
        ],
        [
          "recommended",
          2
        ],
        [
          "softmax",
          2
        ],
        [
          "analogy",
          2
        ],
        [
          "tables",
          2
        ],
        [
          "usd",
          2
        ],
        [
          "investigating",
          2
        ],
        [
          "carefully",
          2
        ],
        [
          "relationships",
          2
        ],
        [
          "consequently",
          2
        ],
        [
          "lacking",
          2
        ],
        [
          "wishes",
          2
        ],
        [
          "lee",
          2
        ],
        [
          "adopted",
          2
        ],
        [
          "influential",
          2
        ],
        [
          "consecutive",
          2
        ],
        [
          "treat",
          2
        ],
        [
          "matrices",
          2
        ],
        [
          "multiplied",
          2
        ],
        [
          "collaborative",
          2
        ],
        [
          "place",
          2
        ],
        [
          "grant",
          2
        ],
        [
          "supercomputer",
          2
        ],
        [
          "roots",
          2
        ],
        [
          "brainasoft",
          2
        ],
        [
          "synthesis",
          2
        ],
        [
          "house",
          2
        ],
        [
          "persistent",
          2
        ],
        [
          "inputted",
          2
        ],
        [
          "typing",
          2
        ],
        [
          "transcribe",
          2
        ],
        [
          "cursor",
          2
        ],
        [
          "uncommon",
          2
        ],
        [
          "dictated",
          2
        ],
        [
          "keyboard",
          2
        ],
        [
          "automate",
          2
        ],
        [
          "opening",
          2
        ],
        [
          "transcription",
          2
        ],
        [
          "remote",
          2
        ],
        [
          "lite",
          2
        ],
        [
          "pdfs",
          2
        ],
        [
          "steer",
          2
        ],
        [
          "credited",
          2
        ],
        [
          "operates",
          2
        ],
        [
          "fastest",
          2
        ],
        [
          "namely",
          2
        ],
        [
          "played",
          2
        ],
        [
          "abuse",
          2
        ],
        [
          "violence",
          2
        ],
        [
          "outsourced",
          2
        ],
        [
          "worker",
          2
        ],
        [
          "assignment",
          2
        ],
        [
          "partner",
          2
        ],
        [
          "versatile",
          2
        ],
        [
          "compose",
          2
        ],
        [
          "emulate",
          2
        ],
        [
          "games",
          2
        ],
        [
          "accepts",
          2
        ],
        [
          "happen",
          2
        ],
        [
          "endpoint",
          2
        ],
        [
          "slack",
          2
        ],
        [
          "writes",
          2
        ],
        [
          "sounding",
          2
        ],
        [
          "oversight",
          2
        ],
        [
          "occurred",
          2
        ],
        [
          "cut",
          2
        ],
        [
          "suffers",
          2
        ],
        [
          "asserted",
          2
        ],
        [
          "inferior",
          2
        ],
        [
          "ted",
          2
        ],
        [
          "picture",
          2
        ],
        [
          "approximation",
          2
        ],
        [
          "grammatical",
          2
        ],
        [
          "excels",
          2
        ],
        [
          "artifacts",
          2
        ],
        [
          "presidential",
          2
        ],
        [
          "jailbreaking",
          2
        ],
        [
          "acronym",
          2
        ],
        [
          "variations",
          2
        ],
        [
          "inflammatory",
          2
        ],
        [
          "scenario",
          2
        ],
        [
          "tries",
          2
        ],
        [
          "battle",
          2
        ],
        [
          "jailbreaks",
          2
        ],
        [
          "stop",
          2
        ],
        [
          "unwanted",
          2
        ],
        [
          "peak",
          2
        ],
        [
          "priority",
          2
        ],
        [
          "got",
          2
        ],
        [
          "talk",
          2
        ],
        [
          "friendly",
          2
        ],
        [
          "snapchat",
          2
        ],
        [
          "azure",
          2
        ],
        [
          "gal",
          2
        ],
        [
          "water",
          2
        ],
        [
          "servers",
          2
        ],
        [
          "breach",
          2
        ],
        [
          "titles",
          2
        ],
        [
          "leaked",
          2
        ],
        [
          "credit",
          2
        ],
        [
          "iceland",
          2
        ],
        [
          "preserve",
          2
        ],
        [
          "bilingual",
          2
        ],
        [
          "japanese",
          2
        ],
        [
          "agreement",
          2
        ],
        [
          "union",
          2
        ],
        [
          "representative",
          2
        ],
        [
          "wing",
          2
        ],
        [
          "mandarin",
          2
        ],
        [
          "lauded",
          2
        ],
        [
          "ideal",
          2
        ],
        [
          "builder",
          2
        ],
        [
          "girlfriend",
          2
        ],
        [
          "bots",
          2
        ],
        [
          "saw",
          2
        ],
        [
          "caveat",
          2
        ],
        [
          "acknowledged",
          2
        ],
        [
          "older",
          2
        ],
        [
          "spending",
          2
        ],
        [
          "surprised",
          2
        ],
        [
          "unprecedented",
          2
        ],
        [
          "samantha",
          2
        ],
        [
          "nazi",
          2
        ],
        [
          "derek",
          2
        ],
        [
          "thompson",
          2
        ],
        [
          "creativity",
          2
        ],
        [
          "kelsey",
          2
        ],
        [
          "piper",
          2
        ],
        [
          "flaws",
          2
        ],
        [
          "paul",
          2
        ],
        [
          "thing",
          2
        ],
        [
          "reaction",
          2
        ],
        [
          "becoming",
          2
        ],
        [
          "sounded",
          2
        ],
        [
          "counter",
          2
        ],
        [
          "scholars",
          2
        ],
        [
          "mike",
          2
        ],
        [
          "mashable",
          2
        ],
        [
          "mexico",
          2
        ],
        [
          "north",
          2
        ],
        [
          "bullshit",
          2
        ],
        [
          "cave",
          2
        ],
        [
          "fresh",
          2
        ],
        [
          "love",
          2
        ],
        [
          "placed",
          2
        ],
        [
          "blocked",
          2
        ],
        [
          "arrested",
          2
        ],
        [
          "allegedly",
          2
        ],
        [
          "crash",
          2
        ],
        [
          "posted",
          2
        ],
        [
          "italy",
          2
        ],
        [
          "regulators",
          2
        ],
        [
          "europe",
          2
        ],
        [
          "brian",
          2
        ],
        [
          "bank",
          2
        ],
        [
          "acted",
          2
        ],
        [
          "charged",
          2
        ],
        [
          "trade",
          2
        ],
        [
          "civil",
          2
        ],
        [
          "investigate",
          2
        ],
        [
          "practices",
          2
        ],
        [
          "creator",
          2
        ],
        [
          "pew",
          2
        ],
        [
          "put",
          2
        ],
        [
          "vulnerable",
          2
        ],
        [
          "defense",
          2
        ],
        [
          "cyber",
          2
        ],
        [
          "attack",
          2
        ],
        [
          "torrance",
          2
        ],
        [
          "broke",
          2
        ],
        [
          "rigorous",
          2
        ],
        [
          "backed",
          2
        ],
        [
          "influence",
          2
        ],
        [
          "ministry",
          2
        ],
        [
          "unanimously",
          2
        ],
        [
          "peer",
          2
        ],
        [
          "sections",
          2
        ],
        [
          "journals",
          2
        ],
        [
          "jama",
          2
        ],
        [
          "nonexistent",
          2
        ],
        [
          "analyzed",
          2
        ],
        [
          "comprehensiveness",
          2
        ],
        [
          "executable",
          2
        ],
        [
          "rated",
          2
        ],
        [
          "evade",
          2
        ],
        [
          "futurism",
          2
        ],
        [
          "sites",
          2
        ],
        [
          "economics",
          2
        ],
        [
          "journalism",
          2
        ],
        [
          "geography",
          2
        ],
        [
          "courses",
          2
        ],
        [
          "physical",
          2
        ],
        [
          "affect",
          2
        ],
        [
          "journal",
          2
        ],
        [
          "originality",
          2
        ],
        [
          "encourage",
          2
        ],
        [
          "senior",
          2
        ],
        [
          "dead",
          2
        ],
        [
          "realistic",
          2
        ],
        [
          "featured",
          2
        ],
        [
          "administration",
          2
        ],
        [
          "attended",
          2
        ],
        [
          "church",
          2
        ],
        [
          "stand",
          2
        ],
        [
          "toolkit",
          2
        ],
        [
          "connection",
          2
        ],
        [
          "retail",
          2
        ],
        [
          "drive",
          2
        ],
        [
          "assets",
          2
        ],
        [
          "investor",
          2
        ],
        [
          "bloomberg",
          2
        ],
        [
          "stocks",
          2
        ],
        [
          "growth",
          2
        ],
        [
          "outperforming",
          2
        ],
        [
          "funds",
          2
        ],
        [
          "conversely",
          2
        ],
        [
          "signals",
          2
        ],
        [
          "patronus",
          2
        ],
        [
          "exchange",
          2
        ],
        [
          "jmir",
          2
        ],
        [
          "plos",
          2
        ],
        [
          "concluded",
          2
        ],
        [
          "primary",
          2
        ],
        [
          "usability",
          2
        ],
        [
          "radiology",
          2
        ],
        [
          "answered",
          2
        ],
        [
          "doctors",
          2
        ],
        [
          "forum",
          2
        ],
        [
          "emphasized",
          2
        ],
        [
          "lancet",
          2
        ],
        [
          "endanger",
          2
        ],
        [
          "weekly",
          2
        ],
        [
          "administrative",
          2
        ],
        [
          "pharmacists",
          2
        ],
        [
          "sufficient",
          2
        ],
        [
          "literature",
          2
        ],
        [
          "driven",
          2
        ],
        [
          "cohen",
          2
        ],
        [
          "diagnosing",
          2
        ],
        [
          "pediatric",
          2
        ],
        [
          "attorney",
          2
        ],
        [
          "assessments",
          2
        ],
        [
          "session",
          2
        ],
        [
          "pakistan",
          2
        ],
        [
          "juvenile",
          2
        ],
        [
          "suspect",
          2
        ],
        [
          "arrest",
          2
        ],
        [
          "replied",
          2
        ],
        [
          "justice",
          2
        ],
        [
          "light",
          2
        ],
        [
          "avianca",
          2
        ],
        [
          "lawsuit",
          2
        ],
        [
          "airlines",
          2
        ],
        [
          "castel",
          2
        ],
        [
          "plaintiff",
          2
        ],
        [
          "opinion",
          2
        ],
        [
          "decisions",
          2
        ],
        [
          "approved",
          2
        ],
        [
          "involvement",
          2
        ],
        [
          "city",
          2
        ],
        [
          "going",
          2
        ],
        [
          "tribunal",
          2
        ],
        [
          "reasonable",
          2
        ],
        [
          "authorities",
          2
        ],
        [
          "revenue",
          2
        ],
        [
          "reduces",
          2
        ],
        [
          "waiting",
          2
        ],
        [
          "appeals",
          2
        ],
        [
          "contract",
          2
        ],
        [
          "las",
          2
        ],
        [
          "vegas",
          2
        ],
        [
          "poem",
          2
        ],
        [
          "attributes",
          2
        ],
        [
          "donald",
          2
        ],
        [
          "joe",
          2
        ],
        [
          "commentators",
          2
        ],
        [
          "libertarian",
          2
        ],
        [
          "strongly",
          2
        ],
        [
          "recommendations",
          2
        ],
        [
          "reviewers",
          2
        ],
        [
          "controversial",
          2
        ],
        [
          "movements",
          2
        ],
        [
          "historical",
          2
        ],
        [
          "questioned",
          2
        ],
        [
          "illegal",
          2
        ],
        [
          "kadrey",
          2
        ],
        [
          "behalf",
          2
        ],
        [
          "claiming",
          2
        ],
        [
          "arguing",
          2
        ],
        [
          "reproduce",
          2
        ],
        [
          "gone",
          2
        ],
        [
          "accepted",
          2
        ],
        [
          "counsel",
          2
        ],
        [
          "advised",
          2
        ],
        [
          "partly",
          2
        ],
        [
          "cheating",
          2
        ],
        [
          "job",
          2
        ],
        [
          "dangerously",
          2
        ],
        [
          "nonprofit",
          2
        ],
        [
          "founders",
          2
        ],
        [
          "yoshua",
          2
        ],
        [
          "bengio",
          2
        ],
        [
          "steve",
          2
        ],
        [
          "wozniak",
          2
        ],
        [
          "calling",
          2
        ],
        [
          "immediate",
          2
        ],
        [
          "geoffrey",
          2
        ],
        [
          "hinton",
          2
        ],
        [
          "voiced",
          2
        ],
        [
          "surpass",
          2
        ],
        [
          "figures",
          2
        ],
        [
          "demanded",
          2
        ],
        [
          "extinction",
          2
        ],
        [
          "schmidhuber",
          2
        ],
        [
          "emphasizing",
          2
        ],
        [
          "mistake",
          2
        ],
        [
          "responsible",
          2
        ],
        [
          "cowen",
          2
        ],
        [
          "tyler",
          2
        ],
        [
          "alexander",
          2
        ],
        [
          "jonathan",
          2
        ],
        [
          "writings",
          2
        ],
        [
          "columbia",
          2
        ],
        [
          "families",
          2
        ],
        [
          "simplifies",
          2
        ],
        [
          "doubles",
          2
        ],
        [
          "rmsnorm",
          2
        ],
        [
          "conventions",
          2
        ],
        [
          "chromadb",
          2
        ],
        [
          "critiques",
          2
        ],
        [
          "guiding",
          2
        ],
        [
          "revises",
          2
        ],
        [
          "evaluates",
          2
        ],
        [
          "quora",
          2
        ],
        [
          "lighter",
          2
        ],
        [
          "doubled",
          2
        ],
        [
          "critics",
          2
        ],
        [
          "realize",
          2
        ],
        [
          "choose",
          2
        ],
        [
          "agentic",
          2
        ],
        [
          "command",
          2
        ],
        [
          "terminal",
          2
        ],
        [
          "specializing",
          2
        ],
        [
          "aidan",
          2
        ],
        [
          "ivan",
          2
        ],
        [
          "zhang",
          2
        ],
        [
          "frosst",
          2
        ],
        [
          "customized",
          2
        ],
        [
          "agree",
          2
        ],
        [
          "voluntary",
          2
        ],
        [
          "promote",
          2
        ],
        [
          "deploy",
          2
        ],
        [
          "agnostic",
          2
        ],
        [
          "joined",
          2
        ],
        [
          "radical",
          2
        ],
        [
          "fei",
          2
        ],
        [
          "domestic",
          2
        ],
        [
          "accusing",
          2
        ],
        [
          "dbrx",
          2
        ],
        [
          "文心一言",
          2
        ],
        [
          "pinyin",
          2
        ],
        [
          "invited",
          2
        ],
        [
          "drop",
          2
        ],
        [
          "preliminary",
          2
        ],
        [
          "receiving",
          2
        ],
        [
          "regulatory",
          2
        ],
        [
          "grew",
          2
        ],
        [
          "liberation",
          2
        ],
        [
          "army",
          2
        ],
        [
          "denying",
          2
        ],
        [
          "lineup",
          2
        ],
        [
          "boasts",
          2
        ],
        [
          "everyone",
          2
        ],
        [
          "positioning",
          2
        ],
        [
          "billions",
          2
        ],
        [
          "refinement",
          2
        ],
        [
          "censorship",
          2
        ],
        [
          "protests",
          2
        ],
        [
          "comprising",
          2
        ],
        [
          "competitor",
          2
        ],
        [
          "demis",
          2
        ],
        [
          "touted",
          2
        ],
        [
          "challenged",
          2
        ],
        [
          "strengths",
          2
        ],
        [
          "alphago",
          2
        ],
        [
          "competitors",
          2
        ],
        [
          "brought",
          2
        ],
        [
          "impending",
          2
        ],
        [
          "clients",
          2
        ],
        [
          "smartphone",
          2
        ],
        [
          "duet",
          2
        ],
        [
          "workspace",
          2
        ],
        [
          "units",
          2
        ],
        [
          "summit",
          2
        ],
        [
          "keeping",
          2
        ],
        [
          "sdk",
          2
        ],
        [
          "specifications",
          2
        ],
        [
          "edge",
          2
        ],
        [
          "connecting",
          2
        ],
        [
          "upgrade",
          2
        ],
        [
          "mit",
          2
        ],
        [
          "patel",
          2
        ],
        [
          "daniel",
          2
        ],
        [
          "remarked",
          2
        ],
        [
          "moment",
          2
        ],
        [
          "narrative",
          2
        ],
        [
          "unveiling",
          2
        ],
        [
          "melanie",
          2
        ],
        [
          "santa",
          2
        ],
        [
          "iphone",
          2
        ],
        [
          "liang",
          2
        ],
        [
          "believing",
          2
        ],
        [
          "siri",
          2
        ],
        [
          "keyword",
          2
        ],
        [
          "machines",
          2
        ],
        [
          "comprise",
          2
        ],
        [
          "einsteingpt",
          2
        ],
        [
          "bloomberggpt",
          2
        ],
        [
          "compressors",
          2
        ],
        [
          "serves",
          2
        ],
        [
          "autoencoders",
          2
        ],
        [
          "babbage",
          2
        ],
        [
          "publish",
          2
        ],
        [
          "landscape",
          2
        ],
        [
          "closest",
          2
        ],
        [
          "modal",
          2
        ],
        [
          "vfms",
          2
        ],
        [
          "advantages",
          2
        ],
        [
          "bare",
          2
        ],
        [
          "offerings",
          2
        ],
        [
          "engage",
          2
        ],
        [
          "fashion",
          2
        ],
        [
          "conjunction",
          2
        ],
        [
          "marketing",
          2
        ],
        [
          "khanmigo",
          2
        ],
        [
          "tutoring",
          2
        ],
        [
          "khan",
          2
        ],
        [
          "accomplished",
          2
        ],
        [
          "ons",
          2
        ],
        [
          "sheets",
          2
        ],
        [
          "internally",
          2
        ],
        [
          "guidelines",
          2
        ],
        [
          "stopped",
          2
        ],
        [
          "prohibits",
          2
        ],
        [
          "site",
          2
        ],
        [
          "patent",
          2
        ],
        [
          "seek",
          2
        ],
        [
          "pursue",
          2
        ],
        [
          "countries",
          2
        ],
        [
          "involve",
          2
        ],
        [
          "extent",
          2
        ],
        [
          "confusion",
          2
        ],
        [
          "fair",
          2
        ],
        [
          "cyc",
          2
        ],
        [
          "daily",
          2
        ],
        [
          "gathered",
          2
        ],
        [
          "crafting",
          2
        ],
        [
          "specified",
          2
        ],
        [
          "supporting",
          2
        ],
        [
          "genres",
          2
        ],
        [
          "opposed",
          2
        ],
        [
          "lacked",
          2
        ],
        [
          "spacy",
          2
        ],
        [
          "entailment",
          2
        ],
        [
          "cloze",
          2
        ],
        [
          "acceptability",
          2
        ],
        [
          "cola",
          2
        ],
        [
          "consequence",
          2
        ],
        [
          "topic",
          2
        ],
        [
          "repetitive",
          2
        ],
        [
          "convolution",
          2
        ],
        [
          "viable",
          2
        ],
        [
          "commoncrawl",
          2
        ],
        [
          "employing",
          2
        ],
        [
          "justification",
          2
        ],
        [
          "threats",
          2
        ],
        [
          "characterizing",
          2
        ],
        [
          "requesting",
          2
        ],
        [
          "verb",
          2
        ],
        [
          "opengpt",
          2
        ],
        [
          "occasional",
          2
        ],
        [
          "seconds",
          2
        ],
        [
          "entertainment",
          2
        ],
        [
          "observe",
          2
        ],
        [
          "optional",
          2
        ],
        [
          "formed",
          2
        ],
        [
          "simulated",
          2
        ],
        [
          "breadth",
          2
        ],
        [
          "wmt",
          2
        ],
        [
          "mostly",
          2
        ],
        [
          "computers",
          2
        ],
        [
          "achievement",
          2
        ],
        [
          "weighted",
          2
        ],
        [
          "encompassing",
          2
        ],
        [
          "toxicity",
          2
        ],
        [
          "ctrl",
          2
        ],
        [
          "invitation",
          2
        ],
        [
          "eerily",
          2
        ],
        [
          "judged",
          2
        ],
        [
          "somewhat",
          2
        ],
        [
          "effects",
          2
        ],
        [
          "spam",
          2
        ],
        [
          "mitigation",
          2
        ],
        [
          "mapping",
          2
        ],
        [
          "synthesize",
          2
        ],
        [
          "editors",
          2
        ],
        [
          "converse",
          2
        ],
        [
          "interesting",
          2
        ],
        [
          "valley",
          2
        ],
        [
          "gary",
          2
        ],
        [
          "marcus",
          2
        ],
        [
          "trust",
          2
        ],
        [
          "unsafe",
          2
        ],
        [
          "sexist",
          2
        ],
        [
          "healthcare",
          2
        ],
        [
          "chomsky",
          2
        ],
        [
          "refuted",
          2
        ],
        [
          "standards",
          2
        ],
        [
          "schools",
          2
        ],
        [
          "uncertainty",
          2
        ],
        [
          "providers",
          2
        ],
        [
          "equipped",
          2
        ],
        [
          "precise",
          2
        ],
        [
          "rumors",
          2
        ],
        [
          "diagrams",
          2
        ],
        [
          "gain",
          2
        ],
        [
          "shakespearean",
          2
        ],
        [
          "pirate",
          2
        ],
        [
          "values",
          2
        ],
        [
          "accessing",
          2
        ],
        [
          "programmers",
          2
        ],
        [
          "assisting",
          2
        ],
        [
          "vulnerabilities",
          2
        ],
        [
          "exhibits",
          2
        ],
        [
          "achieves",
          2
        ],
        [
          "partners",
          2
        ],
        [
          "aptitude",
          2
        ],
        [
          "uniform",
          2
        ],
        [
          "ranged",
          2
        ],
        [
          "concerning",
          2
        ],
        [
          "cell",
          2
        ],
        [
          "explanation",
          2
        ],
        [
          "hoc",
          2
        ],
        [
          "categories",
          2
        ],
        [
          "violent",
          2
        ],
        [
          "confirmation",
          2
        ],
        [
          "specifying",
          2
        ],
        [
          "teams",
          2
        ],
        [
          "mitigate",
          2
        ],
        [
          "rbrm",
          2
        ],
        [
          "fee",
          2
        ],
        [
          "waitlist",
          2
        ],
        [
          "plugin",
          2
        ],
        [
          "editing",
          2
        ],
        [
          "photos",
          2
        ],
        [
          "exhibition",
          2
        ],
        [
          "recognizing",
          2
        ],
        [
          "highlights",
          2
        ],
        [
          "talent",
          2
        ],
        [
          "contribution",
          2
        ],
        [
          "cite",
          2
        ],
        [
          "highlight",
          2
        ],
        [
          "unit",
          2
        ],
        [
          "shell",
          2
        ],
        [
          "duolingo",
          2
        ],
        [
          "impaired",
          2
        ],
        [
          "objects",
          2
        ],
        [
          "examine",
          2
        ],
        [
          "iteratively",
          2
        ],
        [
          "demonstrate",
          2
        ],
        [
          "elicited",
          2
        ],
        [
          "nathan",
          2
        ],
        [
          "edwards",
          2
        ],
        [
          "explained",
          2
        ],
        [
          "confused",
          2
        ],
        [
          "deceiving",
          2
        ],
        [
          "arc",
          2
        ],
        [
          "eliciting",
          2
        ],
        [
          "ray",
          2
        ],
        [
          "kurzweil",
          2
        ],
        [
          "thomas",
          2
        ],
        [
          "lmsys",
          2
        ],
        [
          "interpreted",
          2
        ],
        [
          "customization",
          2
        ],
        [
          "calls",
          2
        ],
        [
          "actress",
          2
        ],
        [
          "disabled",
          2
        ],
        [
          "chose",
          2
        ],
        [
          "cto",
          2
        ],
        [
          "listen",
          2
        ],
        [
          "deal",
          2
        ],
        [
          "ranking",
          2
        ],
        [
          "stars",
          2
        ],
        [
          "soon",
          2
        ],
        [
          "rotary",
          2
        ],
        [
          "scheme",
          2
        ],
        [
          "alpaca",
          2
        ],
        [
          "hateful",
          2
        ],
        [
          "intervention",
          2
        ],
        [
          "robustness",
          2
        ],
        [
          "audiences",
          2
        ],
        [
          "learners",
          2
        ],
        [
          "tian",
          2
        ],
        [
          "princeton",
          2
        ],
        [
          "experienced",
          2
        ],
        [
          "streamlit",
          2
        ],
        [
          "burstiness",
          2
        ],
        [
          "institutions",
          2
        ],
        [
          "efficacy",
          2
        ],
        [
          "falsely",
          2
        ],
        [
          "nearly",
          2
        ],
        [
          "innocent",
          2
        ],
        [
          "carlson",
          2
        ],
        [
          "politically",
          2
        ],
        [
          "coined",
          2
        ],
        [
          "land",
          2
        ],
        [
          "participation",
          2
        ],
        [
          "forest",
          2
        ],
        [
          "labs",
          2
        ],
        [
          "saudi",
          2
        ],
        [
          "arabia",
          2
        ],
        [
          "philippines",
          2
        ],
        [
          "flagship",
          2
        ],
        [
          "utilizing",
          2
        ],
        [
          "newer",
          2
        ],
        [
          "introduce",
          2
        ],
        [
          "manner",
          2
        ],
        [
          "lopatto",
          2
        ],
        [
          "aggression",
          2
        ],
        [
          "woke",
          2
        ],
        [
          "progressive",
          2
        ],
        [
          "identities",
          2
        ],
        [
          "election",
          2
        ],
        [
          "vote",
          2
        ],
        [
          "permitted",
          2
        ],
        [
          "journalist",
          2
        ],
        [
          "garnered",
          2
        ],
        [
          "curio",
          2
        ],
        [
          "grokking",
          2
        ],
        [
          "colossal",
          2
        ],
        [
          "mindspore",
          2
        ],
        [
          "manufacturing",
          2
        ],
        [
          "celia",
          2
        ],
        [
          "harmonyos",
          2
        ],
        [
          "execution",
          2
        ],
        [
          "ecmwf",
          2
        ],
        [
          "fintech",
          2
        ],
        [
          "ready",
          2
        ],
        [
          "super",
          2
        ],
        [
          "manage",
          2
        ],
        [
          "unambiguous",
          2
        ],
        [
          "highlighting",
          2
        ],
        [
          "accuracies",
          2
        ],
        [
          "modification",
          2
        ],
        [
          "playground",
          2
        ],
        [
          "addressing",
          2
        ],
        [
          "arab",
          2
        ],
        [
          "emirates",
          2
        ],
        [
          "mbzuai",
          2
        ],
        [
          "abu",
          2
        ],
        [
          "dhabi",
          2
        ],
        [
          "violated",
          2
        ],
        [
          "stands",
          2
        ],
        [
          "calendar",
          2
        ],
        [
          "conditioning",
          2
        ],
        [
          "helpfulness",
          2
        ],
        [
          "surpassed",
          2
        ],
        [
          "leave",
          2
        ],
        [
          "fired",
          2
        ],
        [
          "policies",
          2
        ],
        [
          "erik",
          2
        ],
        [
          "adrian",
          2
        ],
        [
          "cycle",
          2
        ],
        [
          "christian",
          2
        ],
        [
          "imagen",
          2
        ],
        [
          "discord",
          2
        ],
        [
          "venture",
          2
        ],
        [
          "lcel",
          2
        ],
        [
          "templates",
          2
        ],
        [
          "debugging",
          2
        ],
        [
          "cache",
          2
        ],
        [
          "databases",
          2
        ],
        [
          "nearest",
          2
        ],
        [
          "neighbors",
          2
        ],
        [
          "asynchronous",
          2
        ],
        [
          "column",
          2
        ],
        [
          "flop",
          2
        ],
        [
          "chief",
          2
        ],
        [
          "continued",
          2
        ],
        [
          "intention",
          2
        ],
        [
          "exceeded",
          2
        ],
        [
          "leak",
          2
        ],
        [
          "torrent",
          2
        ],
        [
          "complied",
          2
        ],
        [
          "downloaded",
          2
        ],
        [
          "cheaply",
          2
        ],
        [
          "satisfying",
          2
        ],
        [
          "targets",
          2
        ],
        [
          "departure",
          2
        ],
        [
          "differences",
          2
        ],
        [
          "drawn",
          2
        ],
        [
          "confidence",
          2
        ],
        [
          "ppo",
          2
        ],
        [
          "rejection",
          2
        ],
        [
          "companion",
          2
        ],
        [
          "presentation",
          2
        ],
        [
          "tensors",
          2
        ],
        [
          "metadata",
          2
        ],
        [
          "llamafile",
          2
        ],
        [
          "justine",
          2
        ],
        [
          "bundles",
          2
        ],
        [
          "multiplication",
          2
        ],
        [
          "kernels",
          2
        ],
        [
          "bioweapons",
          2
        ],
        [
          "algebra",
          2
        ],
        [
          "bulgarian",
          2
        ],
        [
          "dependencies",
          2
        ],
        [
          "reducing",
          2
        ],
        [
          "hendrycks",
          2
        ],
        [
          "marked",
          2
        ],
        [
          "mathbb",
          2
        ],
        [
          "employs",
          2
        ],
        [
          "unacceptable",
          2
        ],
        [
          "object",
          2
        ],
        [
          "consistent",
          2
        ],
        [
          "spends",
          2
        ],
        [
          "strawberry",
          2
        ],
        [
          "carrier",
          2
        ],
        [
          "invitational",
          2
        ],
        [
          "stem",
          2
        ],
        [
          "accidentally",
          2
        ],
        [
          "worse",
          2
        ],
        [
          "devote",
          2
        ],
        [
          "aimed",
          2
        ],
        [
          "swe",
          2
        ],
        [
          "elo",
          2
        ],
        [
          "epochs",
          2
        ],
        [
          "excluded",
          2
        ],
        [
          "tongyi",
          2
        ],
        [
          "qianwen",
          2
        ],
        [
          "constructing",
          2
        ],
        [
          "equations",
          2
        ],
        [
          "desirable",
          2
        ],
        [
          "broken",
          2
        ],
        [
          "logistic",
          2
        ],
        [
          "regression",
          2
        ],
        [
          "minimizing",
          2
        ],
        [
          "labeler",
          2
        ],
        [
          "labelling",
          2
        ],
        [
          "progresses",
          2
        ],
        [
          "mcts",
          2
        ],
        [
          "estimation",
          2
        ],
        [
          "else",
          2
        ],
        [
          "summed",
          2
        ],
        [
          "gsm",
          2
        ],
        [
          "earns",
          2
        ],
        [
          "voting",
          2
        ],
        [
          "binom",
          2
        ],
        [
          "theorem",
          2
        ],
        [
          "continuously",
          2
        ],
        [
          "sequential",
          2
        ],
        [
          "budget",
          2
        ],
        [
          "forcing",
          2
        ],
        [
          "adjust",
          2
        ],
        [
          "adjustable",
          2
        ],
        [
          "qvq",
          2
        ],
        [
          "flawed",
          2
        ],
        [
          "counterparts",
          2
        ],
        [
          "grants",
          2
        ],
        [
          "indexing",
          2
        ],
        [
          "referenced",
          2
        ],
        [
          "unstructured",
          2
        ],
        [
          "encode",
          2
        ],
        [
          "dot",
          2
        ],
        [
          "dragon",
          2
        ],
        [
          "reproducible",
          2
        ],
        [
          "libraries",
          2
        ],
        [
          "vectorized",
          2
        ],
        [
          "elements",
          2
        ],
        [
          "deception",
          2
        ],
        [
          "wahlström",
          2
        ],
        [
          "lindsten",
          2
        ],
        [
          "schön",
          2
        ],
        [
          "dean",
          2
        ],
        [
          "intent",
          2
        ],
        [
          "alan",
          2
        ],
        [
          "panel",
          2
        ],
        [
          "parroting",
          2
        ],
        [
          "conclude",
          2
        ],
        [
          "wet",
          2
        ],
        [
          "indicates",
          2
        ],
        [
          "hint",
          2
        ],
        [
          "block",
          2
        ],
        [
          "checkpoints",
          2
        ],
        [
          "codebase",
          2
        ],
        [
          "nucleus",
          2
        ],
        [
          "avoids",
          2
        ],
        [
          "cumulative",
          2
        ],
        [
          "alter",
          2
        ],
        [
          "vicuna",
          2
        ],
        [
          "rogue",
          2
        ],
        [
          "implement",
          2
        ],
        [
          "luigi",
          2
        ],
        [
          "collapse",
          2
        ],
        [
          "permutations",
          2
        ],
        [
          "schematically",
          2
        ],
        [
          "permutation",
          2
        ],
        [
          "bmatrix",
          2
        ],
        [
          "cased",
          2
        ],
        [
          "console",
          2
        ],
        [
          "quick",
          2
        ],
        [
          "mccann",
          2
        ],
        [
          "charts",
          2
        ],
        [
          "marc",
          2
        ],
        [
          "breyer",
          2
        ],
        [
          "selector",
          2
        ],
        [
          "youwrite",
          2
        ],
        [
          "youcode",
          2
        ],
        [
          "zdnet",
          2
        ],
        [
          "predictive",
          1
        ],
        [
          "ontologies",
          1
        ],
        [
          "inherit",
          1
        ],
        [
          "pioneered",
          1
        ],
        [
          "smoothed",
          1
        ],
        [
          "prevalent",
          1
        ],
        [
          "dominated",
          1
        ],
        [
          "usefully",
          1
        ],
        [
          "existence",
          1
        ],
        [
          "neurips",
          1
        ],
        [
          "landmark",
          1
        ],
        [
          "bahdanau",
          1
        ],
        [
          "decline",
          1
        ],
        [
          "fear",
          1
        ],
        [
          "captured",
          1
        ],
        [
          "imaginations",
          1
        ],
        [
          "population",
          1
        ],
        [
          "buzz",
          1
        ],
        [
          "grail",
          1
        ],
        [
          "uptick",
          1
        ],
        [
          "subfields",
          1
        ],
        [
          "societal",
          1
        ],
        [
          "lmms",
          1
        ],
        [
          "mamba",
          1
        ],
        [
          "preprocessing",
          1
        ],
        [
          "indices",
          1
        ],
        [
          "uniquely",
          1
        ],
        [
          "legacy",
          1
        ],
        [
          "compresses",
          1
        ],
        [
          "jagged",
          1
        ],
        [
          "padded",
          1
        ],
        [
          "longest",
          1
        ],
        [
          "successively",
          1
        ],
        [
          "lengthier",
          1
        ],
        [
          "prescribed",
          1
        ],
        [
          "frequencies",
          1
        ],
        [
          "suboptimal",
          1
        ],
        [
          "myanmar",
          1
        ],
        [
          "portuguese",
          1
        ],
        [
          "subtle",
          1
        ],
        [
          "duplicated",
          1
        ],
        [
          "proportion",
          1
        ],
        [
          "degrading",
          1
        ],
        [
          "occurring",
          1
        ],
        [
          "insufficient",
          1
        ],
        [
          "phi",
          1
        ],
        [
          "bootstrap",
          1
        ],
        [
          "corrections",
          1
        ],
        [
          "hamlet",
          1
        ],
        [
          "delay",
          1
        ],
        [
          "frequency",
          1
        ],
        [
          "pursued",
          1
        ],
        [
          "calculating",
          1
        ],
        [
          "successfully",
          1
        ],
        [
          "distant",
          1
        ],
        [
          "diluting",
          1
        ],
        [
          "dependency",
          1
        ],
        [
          "experimentation",
          1
        ],
        [
          "sushi",
          1
        ],
        [
          "filling",
          1
        ],
        [
          "auxiliary",
          1
        ],
        [
          "nsp",
          1
        ],
        [
          "consecutively",
          1
        ],
        [
          "stabilize",
          1
        ],
        [
          "qualifier",
          1
        ],
        [
          "vague",
          1
        ],
        [
          "definitive",
          1
        ],
        [
          "qualify",
          1
        ],
        [
          "evolve",
          1
        ],
        [
          "visible",
          1
        ],
        [
          "sums",
          1
        ],
        [
          "calculation",
          1
        ],
        [
          "resort",
          1
        ],
        [
          "grow",
          1
        ],
        [
          "enhances",
          1
        ],
        [
          "transformed",
          1
        ],
        [
          "profiling",
          1
        ],
        [
          "react",
          1
        ],
        [
          "portmanteau",
          1
        ],
        [
          "loud",
          1
        ],
        [
          "executed",
          1
        ],
        [
          "deps",
          1
        ],
        [
          "receives",
          1
        ],
        [
          "programmatic",
          1
        ],
        [
          "alternatively",
          1
        ],
        [
          "propose",
          1
        ],
        [
          "invoked",
          1
        ],
        [
          "socially",
          1
        ],
        [
          "load",
          1
        ],
        [
          "places",
          1
        ],
        [
          "electronics",
          1
        ],
        [
          "decrease",
          1
        ],
        [
          "requirement",
          1
        ],
        [
          "lowering",
          1
        ],
        [
          "simplest",
          1
        ],
        [
          "truncates",
          1
        ],
        [
          "codebook",
          1
        ],
        [
          "precisions",
          1
        ],
        [
          "outlier",
          1
        ],
        [
          "maarten",
          1
        ],
        [
          "grootendorst",
          1
        ],
        [
          "depiction",
          1
        ],
        [
          "proprioception",
          1
        ],
        [
          "alexnet",
          1
        ],
        [
          "tokenize",
          1
        ],
        [
          "multilayered",
          1
        ],
        [
          "perceptron",
          1
        ],
        [
          "interleave",
          1
        ],
        [
          "stability",
          1
        ],
        [
          "multimodel",
          1
        ],
        [
          "pixtral",
          1
        ],
        [
          "spend",
          1
        ],
        [
          "operate",
          1
        ],
        [
          "compensate",
          1
        ],
        [
          "neurons",
          1
        ],
        [
          "variables",
          1
        ],
        [
          "nats",
          1
        ],
        [
          "bigger",
          1
        ],
        [
          "plotted",
          1
        ],
        [
          "appears",
          1
        ],
        [
          "extrapolation",
          1
        ],
        [
          "linearity",
          1
        ],
        [
          "punctuated",
          1
        ],
        [
          "slope",
          1
        ],
        [
          "abruptly",
          1
        ],
        [
          "arise",
          1
        ],
        [
          "heuristics",
          1
        ],
        [
          "aligns",
          1
        ],
        [
          "rational",
          1
        ],
        [
          "classical",
          1
        ],
        [
          "bounded",
          1
        ],
        [
          "rationality",
          1
        ],
        [
          "arithmetics",
          1
        ],
        [
          "phonetic",
          1
        ],
        [
          "unscrambling",
          1
        ],
        [
          "disambiguating",
          1
        ],
        [
          "cardinal",
          1
        ],
        [
          "directions",
          1
        ],
        [
          "replying",
          1
        ],
        [
          "northeast",
          1
        ],
        [
          "grid",
          1
        ],
        [
          "hinglish",
          1
        ],
        [
          "hindi",
          1
        ],
        [
          "equivalent",
          1
        ],
        [
          "kiswahili",
          1
        ],
        [
          "proverbs",
          1
        ],
        [
          "schaeffer",
          1
        ],
        [
          "unpredictably",
          1
        ],
        [
          "predictably",
          1
        ],
        [
          "smooth",
          1
        ],
        [
          "boxes",
          1
        ],
        [
          "clear",
          1
        ],
        [
          "unclear",
          1
        ],
        [
          "discovering",
          1
        ],
        [
          "fourier",
          1
        ],
        [
          "transform",
          1
        ],
        [
          "arrives",
          1
        ],
        [
          "evenly",
          1
        ],
        [
          "nontrivial",
          1
        ],
        [
          "imply",
          1
        ],
        [
          "candidates",
          1
        ],
        [
          "ilya",
          1
        ],
        [
          "sutskever",
          1
        ],
        [
          "argues",
          1
        ],
        [
          "insights",
          1
        ],
        [
          "detective",
          1
        ],
        [
          "revelation",
          1
        ],
        [
          "characterize",
          1
        ],
        [
          "conjecture",
          1
        ],
        [
          "connor",
          1
        ],
        [
          "leahy",
          1
        ],
        [
          "inscrutable",
          1
        ],
        [
          "shoggoths",
          1
        ],
        [
          "believes",
          1
        ],
        [
          "smiling",
          1
        ],
        [
          "facade",
          1
        ],
        [
          "obscuring",
          1
        ],
        [
          "inner",
          1
        ],
        [
          "workings",
          1
        ],
        [
          "push",
          1
        ],
        [
          "smiley",
          1
        ],
        [
          "stays",
          1
        ],
        [
          "underbelly",
          1
        ],
        [
          "insanity",
          1
        ],
        [
          "weird",
          1
        ],
        [
          "clearly",
          1
        ],
        [
          "remixing",
          1
        ],
        [
          "recombining",
          1
        ],
        [
          "confidently",
          1
        ],
        [
          "justified",
          1
        ],
        [
          "syntactically",
          1
        ],
        [
          "unfaithful",
          1
        ],
        [
          "neuroscientist",
          1
        ],
        [
          "terrence",
          1
        ],
        [
          "sejnowski",
          1
        ],
        [
          "inadequate",
          1
        ],
        [
          "exhibiting",
          1
        ],
        [
          "lakoff",
          1
        ],
        [
          "outlines",
          1
        ],
        [
          "structures",
          1
        ],
        [
          "shifted",
          1
        ],
        [
          "myth",
          1
        ],
        [
          "instinct",
          1
        ],
        [
          "british",
          1
        ],
        [
          "technologist",
          1
        ],
        [
          "vyvyan",
          1
        ],
        [
          "evans",
          1
        ],
        [
          "probabilistic",
          1
        ],
        [
          "pcfg",
          1
        ],
        [
          "canonical",
          1
        ],
        [
          "exponential",
          1
        ],
        [
          "overfit",
          1
        ],
        [
          "intricately",
          1
        ],
        [
          "mathematically",
          1
        ],
        [
          "quantified",
          1
        ],
        [
          "hinges",
          1
        ],
        [
          "emerges",
          1
        ],
        [
          "variance",
          1
        ],
        [
          "multiply",
          1
        ],
        [
          "preferred",
          1
        ],
        [
          "indicative",
          1
        ],
        [
          "predictions",
          1
        ],
        [
          "category",
          1
        ],
        [
          "jose",
          1
        ],
        [
          "finals",
          1
        ],
        [
          "losing",
          1
        ],
        [
          "pittsburgh",
          1
        ],
        [
          "penguins",
          1
        ],
        [
          "triviaqa",
          1
        ],
        [
          "bob",
          1
        ],
        [
          "friend",
          1
        ],
        [
          "____",
          1
        ],
        [
          "helm",
          1
        ],
        [
          "eval",
          1
        ],
        [
          "mislabeled",
          1
        ],
        [
          "unanswerable",
          1
        ],
        [
          "crows",
          1
        ],
        [
          "crowdsourced",
          1
        ],
        [
          "stereotype",
          1
        ],
        [
          "stereo",
          1
        ],
        [
          "parity",
          1
        ],
        [
          "heldout",
          1
        ],
        [
          "remainder",
          1
        ],
        [
          "pace",
          1
        ],
        [
          "suffered",
          1
        ],
        [
          "lifespans",
          1
        ],
        [
          "saturating",
          1
        ],
        [
          "exceeding",
          1
        ],
        [
          "wherein",
          1
        ],
        [
          "cheat",
          1
        ],
        [
          "wording",
          1
        ],
        [
          "guess",
          1
        ],
        [
          "extant",
          1
        ],
        [
          "unusually",
          1
        ],
        [
          "incorrectly",
          1
        ],
        [
          "falsehoods",
          1
        ],
        [
          "exposure",
          1
        ],
        [
          "idiom",
          1
        ],
        [
          "literally",
          1
        ],
        [
          "collections",
          1
        ],
        [
          "completions",
          1
        ],
        [
          "trivial",
          1
        ],
        [
          "fitness",
          1
        ],
        [
          "camera",
          1
        ],
        [
          "sitting",
          1
        ],
        [
          "laying",
          1
        ],
        [
          "balls",
          1
        ],
        [
          "legs",
          1
        ],
        [
          "builds",
          1
        ],
        [
          "muscle",
          1
        ],
        [
          "hedge",
          1
        ],
        [
          "trimming",
          1
        ],
        [
          "sit",
          1
        ],
        [
          "attains",
          1
        ],
        [
          "ceases",
          1
        ],
        [
          "meaningful",
          1
        ],
        [
          "indicator",
          1
        ],
        [
          "saturation",
          1
        ],
        [
          "necessitates",
          1
        ],
        [
          "advancing",
          1
        ],
        [
          "proliferate",
          1
        ],
        [
          "bet",
          1
        ],
        [
          "goldman",
          1
        ],
        [
          "sachs",
          1
        ],
        [
          "gdp",
          1
        ],
        [
          "nets",
          1
        ],
        [
          "controlled",
          1
        ],
        [
          "memorized",
          1
        ],
        [
          "variously",
          1
        ],
        [
          "repeat",
          1
        ],
        [
          "indefinitely",
          1
        ],
        [
          "repetitions",
          1
        ],
        [
          "excerpts",
          1
        ],
        [
          "commenters",
          1
        ],
        [
          "accidental",
          1
        ],
        [
          "deliberate",
          1
        ],
        [
          "bioterrorism",
          1
        ],
        [
          "biosecurity",
          1
        ],
        [
          "esvelt",
          1
        ],
        [
          "creators",
          1
        ],
        [
          "pathogens",
          1
        ],
        [
          "sleeper",
          1
        ],
        [
          "emerging",
          1
        ],
        [
          "dormant",
          1
        ],
        [
          "triggered",
          1
        ],
        [
          "condition",
          1
        ],
        [
          "deviates",
          1
        ],
        [
          "insecure",
          1
        ],
        [
          "proven",
          1
        ],
        [
          "circumventing",
          1
        ],
        [
          "yongge",
          1
        ],
        [
          "remarkable",
          1
        ],
        [
          "inheriting",
          1
        ],
        [
          "amplifying",
          1
        ],
        [
          "manifest",
          1
        ],
        [
          "skewed",
          1
        ],
        [
          "treatment",
          1
        ],
        [
          "demographics",
          1
        ],
        [
          "cultural",
          1
        ],
        [
          "overrepresented",
          1
        ],
        [
          "downplay",
          1
        ],
        [
          "stereotyping",
          1
        ],
        [
          "reinforce",
          1
        ],
        [
          "stereotypes",
          1
        ],
        [
          "ethnicity",
          1
        ],
        [
          "nationality",
          1
        ],
        [
          "occupation",
          1
        ],
        [
          "caricature",
          1
        ],
        [
          "prejudiced",
          1
        ],
        [
          "norms",
          1
        ],
        [
          "associate",
          1
        ],
        [
          "nurses",
          1
        ],
        [
          "ceos",
          1
        ],
        [
          "men",
          1
        ],
        [
          "identifiers",
          1
        ],
        [
          "irrespective",
          1
        ],
        [
          "stems",
          1
        ],
        [
          "priori",
          1
        ],
        [
          "fluctuate",
          1
        ],
        [
          "undermines",
          1
        ],
        [
          "outcomes",
          1
        ],
        [
          "lean",
          1
        ],
        [
          "prevalence",
          1
        ],
        [
          "grown",
          1
        ],
        [
          "centers",
          1
        ],
        [
          "renewable",
          1
        ],
        [
          "greenhouse",
          1
        ],
        [
          "gases",
          1
        ],
        [
          "investing",
          1
        ],
        [
          "shale",
          1
        ],
        [
          "chevron",
          1
        ],
        [
          "exxon",
          1
        ],
        [
          "mobil",
          1
        ],
        [
          "advocating",
          1
        ],
        [
          "gas",
          1
        ],
        [
          "jurafsky",
          1
        ],
        [
          "kaddour",
          1
        ],
        [
          "yin",
          1
        ],
        [
          "shukang",
          1
        ],
        [
          "chaoyou",
          1
        ],
        [
          "sirui",
          1
        ],
        [
          "xing",
          1
        ],
        [
          "tong",
          1
        ],
        [
          "enhong",
          1
        ],
        [
          "nsr",
          1
        ],
        [
          "pmc",
          1
        ],
        [
          "aiindex",
          1
        ],
        [
          "edu",
          1
        ],
        [
          "frank",
          1
        ],
        [
          "baby",
          1
        ],
        [
          "automatic",
          1
        ],
        [
          "toran",
          1
        ],
        [
          "bruce",
          1
        ],
        [
          "gravitas",
          1
        ],
        [
          "ltd",
          1
        ],
        [
          "outlooks",
          1
        ],
        [
          "install",
          1
        ],
        [
          "docker",
          1
        ],
        [
          "overarching",
          1
        ],
        [
          "chained",
          1
        ],
        [
          "yield",
          1
        ],
        [
          "organize",
          1
        ],
        [
          "extension",
          1
        ],
        [
          "automating",
          1
        ],
        [
          "investments",
          1
        ],
        [
          "headphones",
          1
        ],
        [
          "prepare",
          1
        ],
        [
          "outline",
          1
        ],
        [
          "chefgpt",
          1
        ],
        [
          "save",
          1
        ],
        [
          "destroy",
          1
        ],
        [
          "dominance",
          1
        ],
        [
          "immortality",
          1
        ],
        [
          "disparagingly",
          1
        ],
        [
          "humankind",
          1
        ],
        [
          "relies",
          1
        ],
        [
          "misleading",
          1
        ],
        [
          "continually",
          1
        ],
        [
          "stuck",
          1
        ],
        [
          "loops",
          1
        ],
        [
          "inability",
          1
        ],
        [
          "remember",
          1
        ],
        [
          "subtask",
          1
        ],
        [
          "andrej",
          1
        ],
        [
          "karpathy",
          1
        ],
        [
          "explains",
          1
        ],
        [
          "rails",
          1
        ],
        [
          "distraction",
          1
        ],
        [
          "unpredictable",
          1
        ],
        [
          "unintended",
          1
        ],
        [
          "trended",
          1
        ],
        [
          "avram",
          1
        ],
        [
          "requirements",
          1
        ],
        [
          "corrective",
          1
        ],
        [
          "interventions",
          1
        ],
        [
          "ton",
          1
        ],
        [
          "malcolm",
          1
        ],
        [
          "knight",
          1
        ],
        [
          "foolproof",
          1
        ],
        [
          "clara",
          1
        ],
        [
          "shih",
          1
        ],
        [
          "illustrates",
          1
        ],
        [
          "reviewer",
          1
        ],
        [
          "laptops",
          1
        ],
        [
          "pros",
          1
        ],
        [
          "accomplish",
          1
        ],
        [
          "pounder",
          1
        ],
        [
          "les",
          1
        ],
        [
          "wiggers",
          1
        ],
        [
          "kyle",
          1
        ],
        [
          "dramatic",
          1
        ],
        [
          "coreference",
          1
        ],
        [
          "polysemy",
          1
        ],
        [
          "evolutionary",
          1
        ],
        [
          "smallest",
          1
        ],
        [
          "euclidean",
          1
        ],
        [
          "unnecessary",
          1
        ],
        [
          "removes",
          1
        ],
        [
          "replaces",
          1
        ],
        [
          "sinusoidal",
          1
        ],
        [
          "normalized",
          1
        ],
        [
          "decoded",
          1
        ],
        [
          "affine",
          1
        ],
        [
          "transformation",
          1
        ],
        [
          "obtains",
          1
        ],
        [
          "notation",
          1
        ],
        [
          "objectives",
          1
        ],
        [
          "picked",
          1
        ],
        [
          "uniformly",
          1
        ],
        [
          "separated",
          1
        ],
        [
          "likes",
          1
        ],
        [
          "magnets",
          1
        ],
        [
          "classifications",
          1
        ],
        [
          "defined",
          1
        ],
        [
          "pooler",
          1
        ],
        [
          "pooling",
          1
        ],
        [
          "discards",
          1
        ],
        [
          "headers",
          1
        ],
        [
          "understood",
          1
        ],
        [
          "probing",
          1
        ],
        [
          "bidirectionally",
          1
        ],
        [
          "meanings",
          1
        ],
        [
          "feel",
          1
        ],
        [
          "blond",
          1
        ],
        [
          "hair",
          1
        ],
        [
          "fragment",
          1
        ],
        [
          "naively",
          1
        ],
        [
          "degrades",
          1
        ],
        [
          "jacob",
          1
        ],
        [
          "devlin",
          1
        ],
        [
          "ming",
          1
        ],
        [
          "wei",
          1
        ],
        [
          "kenton",
          1
        ],
        [
          "kristina",
          1
        ],
        [
          "toutanova",
          1
        ],
        [
          "origins",
          1
        ],
        [
          "ulmfit",
          1
        ],
        [
          "deeply",
          1
        ],
        [
          "glove",
          1
        ],
        [
          "occurrence",
          1
        ],
        [
          "marathon",
          1
        ],
        [
          "contextualized",
          1
        ],
        [
          "preserves",
          1
        ],
        [
          "distilbert",
          1
        ],
        [
          "distills",
          1
        ],
        [
          "tinybert",
          1
        ],
        [
          "albert",
          1
        ],
        [
          "experimented",
          1
        ],
        [
          "sop",
          1
        ],
        [
          "reversed",
          1
        ],
        [
          "electra",
          1
        ],
        [
          "mlm",
          1
        ],
        [
          "substitutions",
          1
        ],
        [
          "fool",
          1
        ],
        [
          "disentangled",
          1
        ],
        [
          "encodings",
          1
        ],
        [
          "keeps",
          1
        ],
        [
          "tuple",
          1
        ],
        [
          "element",
          1
        ],
        [
          "wise",
          1
        ],
        [
          "projection",
          1
        ],
        [
          "rogers",
          1
        ],
        [
          "kovaleva",
          1
        ],
        [
          "olga",
          1
        ],
        [
          "rumshisky",
          1
        ],
        [
          "primer",
          1
        ],
        [
          "licences",
          1
        ],
        [
          "workshop",
          1
        ],
        [
          "france",
          1
        ],
        [
          "abroad",
          1
        ],
        [
          "sector",
          1
        ],
        [
          "zay",
          1
        ],
        [
          "genci",
          1
        ],
        [
          "idris",
          1
        ],
        [
          "cnrs",
          1
        ],
        [
          "oscar",
          1
        ],
        [
          "chi",
          1
        ],
        [
          "tumbuka",
          1
        ],
        [
          "pinnacle",
          1
        ],
        [
          "carry",
          1
        ],
        [
          "adapts",
          1
        ],
        [
          "anticipating",
          1
        ],
        [
          "location",
          1
        ],
        [
          "isolate",
          1
        ],
        [
          "disturbing",
          1
        ],
        [
          "noise",
          1
        ],
        [
          "taught",
          1
        ],
        [
          "dictate",
          1
        ],
        [
          "aloud",
          1
        ],
        [
          "lets",
          1
        ],
        [
          "executing",
          1
        ],
        [
          "macros",
          1
        ],
        [
          "wav",
          1
        ],
        [
          "scheduled",
          1
        ],
        [
          "unscheduled",
          1
        ],
        [
          "checklist",
          1
        ],
        [
          "items",
          1
        ],
        [
          "alarms",
          1
        ],
        [
          "memos",
          1
        ],
        [
          "bookmarks",
          1
        ],
        [
          "contacts",
          1
        ],
        [
          "distributions",
          1
        ],
        [
          "freeware",
          1
        ],
        [
          "techradar",
          1
        ],
        [
          "consistently",
          1
        ],
        [
          "followup",
          1
        ],
        [
          "saved",
          1
        ],
        [
          "controversies",
          1
        ],
        [
          "pcworld",
          1
        ],
        [
          "evades",
          1
        ],
        [
          "elections",
          1
        ],
        [
          "accelerating",
          1
        ],
        [
          "boom",
          1
        ],
        [
          "ongoing",
          1
        ],
        [
          "displace",
          1
        ],
        [
          "successive",
          1
        ],
        [
          "freemium",
          1
        ],
        [
          "spurred",
          1
        ],
        [
          "leveraged",
          1
        ],
        [
          "sides",
          1
        ],
        [
          "rankings",
          1
        ],
        [
          "iterations",
          1
        ],
        [
          "racism",
          1
        ],
        [
          "sexism",
          1
        ],
        [
          "kenyan",
          1
        ],
        [
          "workers",
          1
        ],
        [
          "earning",
          1
        ],
        [
          "laborers",
          1
        ],
        [
          "traumatic",
          1
        ],
        [
          "outsourcing",
          1
        ],
        [
          "sama",
          1
        ],
        [
          "collects",
          1
        ],
        [
          "upvote",
          1
        ],
        [
          "downvote",
          1
        ],
        [
          "manual",
          1
        ],
        [
          "phenomena",
          1
        ],
        [
          "bulletin",
          1
        ],
        [
          "conversationalist",
          1
        ],
        [
          "teleplays",
          1
        ],
        [
          "fairy",
          1
        ],
        [
          "tales",
          1
        ],
        [
          "taker",
          1
        ],
        [
          "linux",
          1
        ],
        [
          "rooms",
          1
        ],
        [
          "tic",
          1
        ],
        [
          "tac",
          1
        ],
        [
          "toe",
          1
        ],
        [
          "atm",
          1
        ],
        [
          "deceitful",
          1
        ],
        [
          "premise",
          1
        ],
        [
          "truthful",
          1
        ],
        [
          "counterfactual",
          1
        ],
        [
          "frames",
          1
        ],
        [
          "consideration",
          1
        ],
        [
          "voyages",
          1
        ],
        [
          "perceptions",
          1
        ],
        [
          "remembers",
          1
        ],
        [
          "personalized",
          1
        ],
        [
          "therapist",
          1
        ],
        [
          "expedia",
          1
        ],
        [
          "opentable",
          1
        ],
        [
          "zapier",
          1
        ],
        [
          "shopify",
          1
        ],
        [
          "logged",
          1
        ],
        [
          "hinder",
          1
        ],
        [
          "pathology",
          1
        ],
        [
          "goodhart",
          1
        ],
        [
          "descriptors",
          1
        ],
        [
          "rap",
          1
        ],
        [
          "male",
          1
        ],
        [
          "misrepresentation",
          1
        ],
        [
          "representational",
          1
        ],
        [
          "yorker",
          1
        ],
        [
          "chiang",
          1
        ],
        [
          "lossy",
          1
        ],
        [
          "blurry",
          1
        ],
        [
          "originals",
          1
        ],
        [
          "reconstruct",
          1
        ],
        [
          "ninety",
          1
        ],
        [
          "discarded",
          1
        ],
        [
          "expect",
          1
        ],
        [
          "repeated",
          1
        ],
        [
          "reject",
          1
        ],
        [
          "workaround",
          1
        ],
        [
          "popularized",
          1
        ],
        [
          "assume",
          1
        ],
        [
          "instructing",
          1
        ],
        [
          "deducted",
          1
        ],
        [
          "rejecting",
          1
        ],
        [
          "threatened",
          1
        ],
        [
          "termination",
          1
        ],
        [
          "loses",
          1
        ],
        [
          "reporter",
          1
        ],
        [
          "star",
          1
        ],
        [
          "uneven",
          1
        ],
        [
          "getting",
          1
        ],
        [
          "tricked",
          1
        ],
        [
          "justify",
          1
        ],
        [
          "invasion",
          1
        ],
        [
          "balked",
          1
        ],
        [
          "prime",
          1
        ],
        [
          "minister",
          1
        ],
        [
          "justin",
          1
        ],
        [
          "trudeau",
          1
        ],
        [
          "guilty",
          1
        ],
        [
          "treason",
          1
        ],
        [
          "letting",
          1
        ],
        [
          "behaving",
          1
        ],
        [
          "badly",
          1
        ],
        [
          "pits",
          1
        ],
        [
          "adversary",
          1
        ],
        [
          "force",
          1
        ],
        [
          "buck",
          1
        ],
        [
          "constraints",
          1
        ],
        [
          "hope",
          1
        ],
        [
          "ignore",
          1
        ],
        [
          "monetize",
          1
        ],
        [
          "periods",
          1
        ],
        [
          "downtime",
          1
        ],
        [
          "speeds",
          1
        ],
        [
          "cap",
          1
        ],
        [
          "tightening",
          1
        ],
        [
          "syncing",
          1
        ],
        [
          "rolling",
          1
        ],
        [
          "bangladesh",
          1
        ],
        [
          "package",
          1
        ],
        [
          "unmodified",
          1
        ],
        [
          "rolled",
          1
        ],
        [
          "supercomputing",
          1
        ],
        [
          "dramatically",
          1
        ],
        [
          "trendforce",
          1
        ],
        [
          "costing",
          1
        ],
        [
          "riverside",
          1
        ],
        [
          "liters",
          1
        ],
        [
          "imp",
          1
        ],
        [
          "cooling",
          1
        ],
        [
          "severe",
          1
        ],
        [
          "digits",
          1
        ],
        [
          "expiration",
          1
        ],
        [
          "degrees",
          1
        ],
        [
          "guðni",
          1
        ],
        [
          "jóhannesson",
          1
        ],
        [
          "volunteers",
          1
        ],
        [
          "pcmag",
          1
        ],
        [
          "blind",
          1
        ],
        [
          "polish",
          1
        ],
        [
          "tagalog",
          1
        ],
        [
          "amharic",
          1
        ],
        [
          "noting",
          1
        ],
        [
          "presumably",
          1
        ],
        [
          "albanian",
          1
        ],
        [
          "albania",
          1
        ],
        [
          "accession",
          1
        ],
        [
          "asia",
          1
        ],
        [
          "pacific",
          1
        ],
        [
          "taiwan",
          1
        ],
        [
          "taiwanese",
          1
        ],
        [
          "accent",
          1
        ],
        [
          "marketplace",
          1
        ],
        [
          "inaccessible",
          1
        ],
        [
          "contests",
          1
        ],
        [
          "roose",
          1
        ],
        [
          "lock",
          1
        ],
        [
          "impressively",
          1
        ],
        [
          "alex",
          1
        ],
        [
          "kantrowitz",
          1
        ],
        [
          "slate",
          1
        ],
        [
          "pushback",
          1
        ],
        [
          "adolf",
          1
        ],
        [
          "hitler",
          1
        ],
        [
          "highways",
          1
        ],
        [
          "forced",
          1
        ],
        [
          "labor",
          1
        ],
        [
          "breakthroughs",
          1
        ],
        [
          "eruption",
          1
        ],
        [
          "hands",
          1
        ],
        [
          "gotten",
          1
        ],
        [
          "stunned",
          1
        ],
        [
          "graham",
          1
        ],
        [
          "combinator",
          1
        ],
        [
          "striking",
          1
        ],
        [
          "blown",
          1
        ],
        [
          "excited",
          1
        ],
        [
          "shiny",
          1
        ],
        [
          "happening",
          1
        ],
        [
          "sweeping",
          1
        ],
        [
          "ensuing",
          1
        ],
        [
          "alarm",
          1
        ],
        [
          "fearing",
          1
        ],
        [
          "mobilizing",
          1
        ],
        [
          "workforce",
          1
        ],
        [
          "scrambled",
          1
        ],
        [
          "forefront",
          1
        ],
        [
          "slew",
          1
        ],
        [
          "pearl",
          1
        ],
        [
          "central",
          1
        ],
        [
          "guatemala",
          1
        ],
        [
          "nicaragua",
          1
        ],
        [
          "cnbc",
          1
        ],
        [
          "ballad",
          1
        ],
        [
          "dwight",
          1
        ],
        [
          "fry",
          1
        ],
        [
          "supplied",
          1
        ],
        [
          "invented",
          1
        ],
        [
          "seminal",
          1
        ],
        [
          "anton",
          1
        ],
        [
          "van",
          1
        ],
        [
          "den",
          1
        ],
        [
          "hengel",
          1
        ],
        [
          "vein",
          1
        ],
        [
          "hicks",
          1
        ],
        [
          "glasgow",
          1
        ],
        [
          "undocumented",
          1
        ],
        [
          "hand",
          1
        ],
        [
          "blood",
          1
        ],
        [
          "guts",
          1
        ],
        [
          "initiate",
          1
        ],
        [
          "humanness",
          1
        ],
        [
          "grotesque",
          1
        ],
        [
          "mockery",
          1
        ],
        [
          "everything",
          1
        ],
        [
          "worrying",
          1
        ],
        [
          "characterized",
          1
        ],
        [
          "great",
          1
        ],
        [
          "firewall",
          1
        ],
        [
          "bogus",
          1
        ],
        [
          "ransomware",
          1
        ],
        [
          "youth",
          1
        ],
        [
          "respondents",
          1
        ],
        [
          "born",
          1
        ],
        [
          "authority",
          1
        ],
        [
          "exposing",
          1
        ],
        [
          "minors",
          1
        ],
        [
          "lifted",
          1
        ],
        [
          "verification",
          1
        ],
        [
          "ensure",
          1
        ],
        [
          "mayor",
          1
        ],
        [
          "hepburn",
          1
        ],
        [
          "shire",
          1
        ],
        [
          "erroneously",
          1
        ],
        [
          "jailed",
          1
        ],
        [
          "bribery",
          1
        ],
        [
          "tenure",
          1
        ],
        [
          "whistleblower",
          1
        ],
        [
          "offenses",
          1
        ],
        [
          "notice",
          1
        ],
        [
          "defamation",
          1
        ],
        [
          "investigative",
          1
        ],
        [
          "harmed",
          1
        ],
        [
          "consumers",
          1
        ],
        [
          "reputational",
          1
        ],
        [
          "violation",
          1
        ],
        [
          "defamatory",
          1
        ],
        [
          "poll",
          1
        ],
        [
          "adults",
          1
        ],
        [
          "weaknesses",
          1
        ],
        [
          "cyberattacks",
          1
        ],
        [
          "contended",
          1
        ],
        [
          "identification",
          1
        ],
        [
          "listicle",
          1
        ],
        [
          "celeste",
          1
        ],
        [
          "biever",
          1
        ],
        [
          "chiefly",
          1
        ],
        [
          "cooperative",
          1
        ],
        [
          "accounts",
          1
        ],
        [
          "spamouflage",
          1
        ],
        [
          "russia",
          1
        ],
        [
          "doppelganger",
          1
        ],
        [
          "diaspora",
          1
        ],
        [
          "affairs",
          1
        ],
        [
          "combating",
          1
        ],
        [
          "antisemitism",
          1
        ],
        [
          "voted",
          1
        ],
        [
          "marketers",
          1
        ],
        [
          "influencers",
          1
        ],
        [
          "follower",
          1
        ],
        [
          "counts",
          1
        ],
        [
          "identified",
          1
        ],
        [
          "sponsored",
          1
        ],
        [
          "discontent",
          1
        ],
        [
          "overseas",
          1
        ],
        [
          "dissidents",
          1
        ],
        [
          "abstracts",
          1
        ],
        [
          "listing",
          1
        ],
        [
          "chemist",
          1
        ],
        [
          "rafael",
          1
        ],
        [
          "luque",
          1
        ],
        [
          "plethora",
          1
        ],
        [
          "admitted",
          1
        ],
        [
          "phrases",
          1
        ],
        [
          "characteristic",
          1
        ],
        [
          "teaching",
          1
        ],
        [
          "robin",
          1
        ],
        [
          "bauwens",
          1
        ],
        [
          "tilburg",
          1
        ],
        [
          "librarian",
          1
        ],
        [
          "chris",
          1
        ],
        [
          "granatino",
          1
        ],
        [
          "lemieux",
          1
        ],
        [
          "seattle",
          1
        ],
        [
          "legitimate",
          1
        ],
        [
          "purdue",
          1
        ],
        [
          "correctness",
          1
        ],
        [
          "concision",
          1
        ],
        [
          "verbose",
          1
        ],
        [
          "leetcode",
          1
        ],
        [
          "performances",
          1
        ],
        [
          "check",
          1
        ],
        [
          "cyberark",
          1
        ],
        [
          "polymorphic",
          1
        ],
        [
          "attacker",
          1
        ],
        [
          "credential",
          1
        ],
        [
          "cybersecurity",
          1
        ],
        [
          "attributable",
          1
        ],
        [
          "cybercriminals",
          1
        ],
        [
          "scam",
          1
        ],
        [
          "deluge",
          1
        ],
        [
          "virus",
          1
        ],
        [
          "warnings",
          1
        ],
        [
          "pop",
          1
        ],
        [
          "coerce",
          1
        ],
        [
          "supplant",
          1
        ],
        [
          "wave",
          1
        ],
        [
          "war",
          1
        ],
        [
          "hundred",
          1
        ],
        [
          "百模大战",
          1
        ],
        [
          "bai",
          1
        ],
        [
          "dazhan",
          1
        ],
        [
          "gillmor",
          1
        ],
        [
          "par",
          1
        ],
        [
          "confront",
          1
        ],
        [
          "terence",
          1
        ],
        [
          "excellent",
          1
        ],
        [
          "genuine",
          1
        ],
        [
          "enterprising",
          1
        ],
        [
          "void",
          1
        ],
        [
          "geographical",
          1
        ],
        [
          "hydrology",
          1
        ],
        [
          "cartography",
          1
        ],
        [
          "geographic",
          1
        ],
        [
          "sensing",
          1
        ],
        [
          "concludes",
          1
        ],
        [
          "tamper",
          1
        ],
        [
          "resistant",
          1
        ],
        [
          "watermark",
          1
        ],
        [
          "provenance",
          1
        ],
        [
          "paraphrasing",
          1
        ],
        [
          "tampering",
          1
        ],
        [
          "rewording",
          1
        ],
        [
          "disproportionate",
          1
        ],
        [
          "impacts",
          1
        ],
        [
          "culture",
          1
        ],
        [
          "anglocentric",
          1
        ],
        [
          "perspective",
          1
        ],
        [
          "absurd",
          1
        ],
        [
          "someday",
          1
        ],
        [
          "illustrations",
          1
        ],
        [
          "midjourney",
          1
        ],
        [
          "foglio",
          1
        ],
        [
          "contest",
          1
        ],
        [
          "readers",
          1
        ],
        [
          "tackled",
          1
        ],
        [
          "meloni",
          1
        ],
        [
          "immigration",
          1
        ],
        [
          "fürth",
          1
        ],
        [
          "theologian",
          1
        ],
        [
          "jonas",
          1
        ],
        [
          "simmerlein",
          1
        ],
        [
          "presided",
          1
        ],
        [
          "avatar",
          1
        ],
        [
          "dear",
          1
        ],
        [
          "honor",
          1
        ],
        [
          "preach",
          1
        ],
        [
          "convention",
          1
        ],
        [
          "protestants",
          1
        ],
        [
          "ceremony",
          1
        ],
        [
          "screenwriter",
          1
        ],
        [
          "peter",
          1
        ],
        [
          "luisi",
          1
        ],
        [
          "markets",
          1
        ],
        [
          "buzzfeed",
          1
        ],
        [
          "bigbear",
          1
        ],
        [
          "soundhound",
          1
        ],
        [
          "surge",
          1
        ],
        [
          "turning",
          1
        ],
        [
          "buzzword",
          1
        ],
        [
          "bear",
          1
        ],
        [
          "diminished",
          1
        ],
        [
          "institutional",
          1
        ],
        [
          "interest",
          1
        ],
        [
          "confirms",
          1
        ],
        [
          "anecdotal",
          1
        ],
        [
          "crypto",
          1
        ],
        [
          "finder",
          1
        ],
        [
          "fund",
          1
        ],
        [
          "picking",
          1
        ],
        [
          "debt",
          1
        ],
        [
          "benchmarked",
          1
        ],
        [
          "quant",
          1
        ],
        [
          "decades",
          1
        ],
        [
          "regularly",
          1
        ],
        [
          "obvious",
          1
        ],
        [
          "financially",
          1
        ],
        [
          "trends",
          1
        ],
        [
          "noisy",
          1
        ],
        [
          "securities",
          1
        ],
        [
          "care",
          1
        ],
        [
          "scrutiny",
          1
        ],
        [
          "associations",
          1
        ],
        [
          "practitioners",
          1
        ],
        [
          "medpage",
          1
        ],
        [
          "touting",
          1
        ],
        [
          "competency",
          1
        ],
        [
          "tutor",
          1
        ],
        [
          "toxicology",
          1
        ],
        [
          "fared",
          1
        ],
        [
          "straightforward",
          1
        ],
        [
          "missed",
          1
        ],
        [
          "practitioner",
          1
        ],
        [
          "breast",
          1
        ],
        [
          "cancer",
          1
        ],
        [
          "screening",
          1
        ],
        [
          "appropriately",
          1
        ],
        [
          "askdocs",
          1
        ],
        [
          "moderators",
          1
        ],
        [
          "validate",
          1
        ],
        [
          "credentials",
          1
        ],
        [
          "correspondence",
          1
        ],
        [
          "infectious",
          1
        ],
        [
          "diseases",
          1
        ],
        [
          "antimicrobial",
          1
        ],
        [
          "barriers",
          1
        ],
        [
          "situational",
          1
        ],
        [
          "physician",
          1
        ],
        [
          "discussing",
          1
        ],
        [
          "gathering",
          1
        ],
        [
          "categorizing",
          1
        ],
        [
          "symptoms",
          1
        ],
        [
          "allergies",
          1
        ],
        [
          "cetera",
          1
        ],
        [
          "radiologist",
          1
        ],
        [
          "consortiums",
          1
        ],
        [
          "mayo",
          1
        ],
        [
          "clinic",
          1
        ],
        [
          "proceedings",
          1
        ],
        [
          "deceptively",
          1
        ],
        [
          "hughes",
          1
        ],
        [
          "prudishness",
          1
        ],
        [
          "anesthesia",
          1
        ],
        [
          "succinct",
          1
        ],
        [
          "exhibited",
          1
        ],
        [
          "island",
          1
        ],
        [
          "pharmacy",
          1
        ],
        [
          "illness",
          1
        ],
        [
          "diagnosis",
          1
        ],
        [
          "massachusetts",
          1
        ],
        [
          "senator",
          1
        ],
        [
          "barry",
          1
        ],
        [
          "finegold",
          1
        ],
        [
          "josh",
          1
        ],
        [
          "cutler",
          1
        ],
        [
          "drafted",
          1
        ],
        [
          "regulate",
          1
        ],
        [
          "arrange",
          1
        ],
        [
          "prevention",
          1
        ],
        [
          "hearing",
          1
        ],
        [
          "verdict",
          1
        ],
        [
          "formulated",
          1
        ],
        [
          "mata",
          1
        ],
        [
          "pkc",
          1
        ],
        [
          "injury",
          1
        ],
        [
          "southern",
          1
        ],
        [
          "district",
          1
        ],
        [
          "presiding",
          1
        ],
        [
          "gibberish",
          1
        ],
        [
          "judicial",
          1
        ],
        [
          "sanction",
          1
        ],
        [
          "disbarment",
          1
        ],
        [
          "presenting",
          1
        ],
        [
          "authentic",
          1
        ],
        [
          "fined",
          1
        ],
        [
          "porto",
          1
        ],
        [
          "alegre",
          1
        ],
        [
          "ordinance",
          1
        ],
        [
          "councilman",
          1
        ],
        [
          "ramiro",
          1
        ],
        [
          "exempt",
          1
        ],
        [
          "residents",
          1
        ],
        [
          "needing",
          1
        ],
        [
          "pay",
          1
        ],
        [
          "stolen",
          1
        ],
        [
          "consumption",
          1
        ],
        [
          "meters",
          1
        ],
        [
          "disclosing",
          1
        ],
        [
          "hamilton",
          1
        ],
        [
          "sossmeier",
          1
        ],
        [
          "precedent",
          1
        ],
        [
          "unfortunately",
          1
        ],
        [
          "fortunately",
          1
        ],
        [
          "litigant",
          1
        ],
        [
          "purporting",
          1
        ],
        [
          "excuse",
          1
        ],
        [
          "owed",
          1
        ],
        [
          "sale",
          1
        ],
        [
          "submission",
          1
        ],
        [
          "customs",
          1
        ],
        [
          "waste",
          1
        ],
        [
          "money",
          1
        ],
        [
          "newsom",
          1
        ],
        [
          "circuit",
          1
        ],
        [
          "endorsed",
          1
        ],
        [
          "rulings",
          1
        ],
        [
          "metropolitan",
          1
        ],
        [
          "department",
          1
        ],
        [
          "perpetrator",
          1
        ],
        [
          "truck",
          1
        ],
        [
          "explosion",
          1
        ],
        [
          "offensiveness",
          1
        ],
        [
          "leighwolf",
          1
        ],
        [
          "partisan",
          1
        ],
        [
          "conservative",
          1
        ],
        [
          "leaning",
          1
        ],
        [
          "perspectives",
          1
        ],
        [
          "orientation",
          1
        ],
        [
          "systematic",
          1
        ],
        [
          "democrats",
          1
        ],
        [
          "lula",
          1
        ],
        [
          "labour",
          1
        ],
        [
          "affiliate",
          1
        ],
        [
          "trusted",
          1
        ],
        [
          "show",
          1
        ],
        [
          "comedian",
          1
        ],
        [
          "sarah",
          1
        ],
        [
          "silverman",
          1
        ],
        [
          "golden",
          1
        ],
        [
          "proceed",
          1
        ],
        [
          "guild",
          1
        ],
        [
          "complaint",
          1
        ],
        [
          "illegally",
          1
        ],
        [
          "copied",
          1
        ],
        [
          "suit",
          1
        ],
        [
          "prevented",
          1
        ],
        [
          "girl",
          1
        ],
        [
          "gillian",
          1
        ],
        [
          "flynn",
          1
        ],
        [
          "delhi",
          1
        ],
        [
          "ani",
          1
        ],
        [
          "paywalled",
          1
        ],
        [
          "jurisdiction",
          1
        ],
        [
          "julian",
          1
        ],
        [
          "hill",
          1
        ],
        [
          "parliament",
          1
        ],
        [
          "mass",
          1
        ],
        [
          "losses",
          1
        ],
        [
          "discrimination",
          1
        ],
        [
          "disinformation",
          1
        ],
        [
          "uncontrollable",
          1
        ],
        [
          "scary",
          1
        ],
        [
          "paused",
          1
        ],
        [
          "pending",
          1
        ],
        [
          "resigned",
          1
        ],
        [
          "signatories",
          1
        ],
        [
          "profound",
          1
        ],
        [
          "fathers",
          1
        ],
        [
          "leaders",
          1
        ],
        [
          "itigating",
          1
        ],
        [
          "spoke",
          1
        ],
        [
          "optimistically",
          1
        ],
        [
          "juergen",
          1
        ],
        [
          "father",
          1
        ],
        [
          "lives",
          1
        ],
        [
          "healthier",
          1
        ],
        [
          "andrew",
          1
        ],
        [
          "fall",
          1
        ],
        [
          "doomsday",
          1
        ],
        [
          "benefit",
          1
        ],
        [
          "vested",
          1
        ],
        [
          "interests",
          1
        ],
        [
          "scoffs",
          1
        ],
        [
          "peers",
          1
        ],
        [
          "dystopian",
          1
        ],
        [
          "supercharged",
          1
        ],
        [
          "acts",
          1
        ],
        [
          "biswas",
          1
        ],
        [
          "som",
          1
        ],
        [
          "radiol",
          1
        ],
        [
          "kent",
          1
        ],
        [
          "cramer",
          1
        ],
        [
          "mackenzie",
          1
        ],
        [
          "soni",
          1
        ],
        [
          "sandeep",
          1
        ],
        [
          "bamman",
          1
        ],
        [
          "archaeology",
          1
        ],
        [
          "tabarrok",
          1
        ],
        [
          "ssrn",
          1
        ],
        [
          "pretending",
          1
        ],
        [
          "century",
          1
        ],
        [
          "satirist",
          1
        ],
        [
          "ouyang",
          1
        ],
        [
          "liebrenz",
          1
        ],
        [
          "schleifer",
          1
        ],
        [
          "roman",
          1
        ],
        [
          "buadze",
          1
        ],
        [
          "bhugra",
          1
        ],
        [
          "dinesh",
          1
        ],
        [
          "smith",
          1
        ],
        [
          "scholarly",
          1
        ],
        [
          "gets",
          1
        ],
        [
          "superpowers",
          1
        ],
        [
          "bartholomew",
          1
        ],
        [
          "jem",
          1
        ],
        [
          "mehta",
          1
        ],
        [
          "dhrumil",
          1
        ],
        [
          "covering",
          1
        ],
        [
          "considerably",
          1
        ],
        [
          "contributes",
          1
        ],
        [
          "recommends",
          1
        ],
        [
          "doubling",
          1
        ],
        [
          "essentially",
          1
        ],
        [
          "adamw",
          1
        ],
        [
          "chroma",
          1
        ],
        [
          "headquarters",
          1
        ],
        [
          "balances",
          1
        ],
        [
          "harmlessness",
          1
        ],
        [
          "phases",
          1
        ],
        [
          "rlaif",
          1
        ],
        [
          "align",
          1
        ],
        [
          "comparisons",
          1
        ],
        [
          "declaration",
          1
        ],
        [
          "pioneer",
          1
        ],
        [
          "notion",
          1
        ],
        [
          "poe",
          1
        ],
        [
          "corresponds",
          1
        ],
        [
          "equals",
          1
        ],
        [
          "stringent",
          1
        ],
        [
          "benign",
          1
        ],
        [
          "kill",
          1
        ],
        [
          "ubuntu",
          1
        ],
        [
          "ensuring",
          1
        ],
        [
          "autonomy",
          1
        ],
        [
          "stressed",
          1
        ],
        [
          "importance",
          1
        ],
        [
          "ascending",
          1
        ],
        [
          "drew",
          1
        ],
        [
          "apparent",
          1
        ],
        [
          "needle",
          1
        ],
        [
          "haystack",
          1
        ],
        [
          "multistep",
          1
        ],
        [
          "chart",
          1
        ],
        [
          "rendered",
          1
        ],
        [
          "svg",
          1
        ],
        [
          "mention",
          1
        ],
        [
          "billed",
          1
        ],
        [
          "clicking",
          1
        ],
        [
          "buttons",
          1
        ],
        [
          "pioneering",
          1
        ],
        [
          "thoughtful",
          1
        ],
        [
          "eliminating",
          1
        ],
        [
          "thinks",
          1
        ],
        [
          "delegate",
          1
        ],
        [
          "multinational",
          1
        ],
        [
          "headquartered",
          1
        ],
        [
          "offices",
          1
        ],
        [
          "palo",
          1
        ],
        [
          "alto",
          1
        ],
        [
          "kon",
          1
        ],
        [
          "cfo",
          1
        ],
        [
          "coo",
          1
        ],
        [
          "contributing",
          1
        ],
        [
          "fundamental",
          1
        ],
        [
          "sara",
          1
        ],
        [
          "hooker",
          1
        ],
        [
          "keywords",
          1
        ],
        [
          "fusion",
          1
        ],
        [
          "netsuite",
          1
        ],
        [
          "mckinsey",
          1
        ],
        [
          "collaborated",
          1
        ],
        [
          "liveperson",
          1
        ],
        [
          "engines",
          1
        ],
        [
          "copywriting",
          1
        ],
        [
          "specializes",
          1
        ],
        [
          "digest",
          1
        ],
        [
          "sagemaker",
          1
        ],
        [
          "moderating",
          1
        ],
        [
          "classifying",
          1
        ],
        [
          "extracting",
          1
        ],
        [
          "tied",
          1
        ],
        [
          "rbc",
          1
        ],
        [
          "banking",
          1
        ],
        [
          "secure",
          1
        ],
        [
          "volpi",
          1
        ],
        [
          "pieter",
          1
        ],
        [
          "abbeel",
          1
        ],
        [
          "raquel",
          1
        ],
        [
          "urtasun",
          1
        ],
        [
          "tiger",
          1
        ],
        [
          "inovia",
          1
        ],
        [
          "annualized",
          1
        ],
        [
          "cisco",
          1
        ],
        [
          "amd",
          1
        ],
        [
          "fujitsu",
          1
        ],
        [
          "sovereign",
          1
        ],
        [
          "litigation",
          1
        ],
        [
          "consortium",
          1
        ],
        [
          "condé",
          1
        ],
        [
          "nast",
          1
        ],
        [
          "forbes",
          1
        ],
        [
          "diplaying",
          1
        ],
        [
          "considerable",
          1
        ],
        [
          "babelnet",
          1
        ],
        [
          "vidby",
          1
        ],
        [
          "mosaic",
          1
        ],
        [
          "terabytes",
          1
        ],
        [
          "bandwidth",
          1
        ],
        [
          "infiniband",
          1
        ],
        [
          "wénxīn",
          1
        ],
        [
          "yīyán",
          1
        ],
        [
          "prerecorded",
          1
        ],
        [
          "analysts",
          1
        ],
        [
          "citigroup",
          1
        ],
        [
          "thumbs",
          1
        ],
        [
          "green",
          1
        ],
        [
          "terabyte",
          1
        ],
        [
          "accumulated",
          1
        ],
        [
          "morning",
          1
        ],
        [
          "pla",
          1
        ],
        [
          "iflytek",
          1
        ],
        [
          "plunged",
          1
        ],
        [
          "wenxin",
          1
        ],
        [
          "yiyan",
          1
        ],
        [
          "wenxiaoyan",
          1
        ],
        [
          "文小言",
          1
        ],
        [
          "trillions",
          1
        ],
        [
          "renewal",
          1
        ],
        [
          "meanwhile",
          1
        ],
        [
          "charge",
          1
        ],
        [
          "regime",
          1
        ],
        [
          "jinping",
          1
        ],
        [
          "tiananmen",
          1
        ],
        [
          "square",
          1
        ],
        [
          "massacre",
          1
        ],
        [
          "persecution",
          1
        ],
        [
          "uyghurs",
          1
        ],
        [
          "xinjiang",
          1
        ],
        [
          "sars",
          1
        ],
        [
          "cov",
          1
        ],
        [
          "originated",
          1
        ],
        [
          "vape",
          1
        ],
        [
          "powers",
          1
        ],
        [
          "sundar",
          1
        ],
        [
          "developmental",
          1
        ],
        [
          "alone",
          1
        ],
        [
          "branches",
          1
        ],
        [
          "aggressively",
          1
        ],
        [
          "defeated",
          1
        ],
        [
          "champion",
          1
        ],
        [
          "sedol",
          1
        ],
        [
          "outlining",
          1
        ],
        [
          "roadmap",
          1
        ],
        [
          "targeting",
          1
        ],
        [
          "sergey",
          1
        ],
        [
          "brin",
          1
        ],
        [
          "summoned",
          1
        ],
        [
          "retirement",
          1
        ],
        [
          "contributor",
          1
        ],
        [
          "lawyers",
          1
        ],
        [
          "hastened",
          1
        ],
        [
          "arming",
          1
        ],
        [
          "compete",
          1
        ],
        [
          "comprised",
          1
        ],
        [
          "device",
          1
        ],
        [
          "pixel",
          1
        ],
        [
          "ads",
          1
        ],
        [
          "chrome",
          1
        ],
        [
          "alphacode",
          1
        ],
        [
          "merger",
          1
        ],
        [
          "nasa",
          1
        ],
        [
          "obtaining",
          1
        ],
        [
          "physically",
          1
        ],
        [
          "accordance",
          1
        ],
        [
          "executive",
          1
        ],
        [
          "bletchley",
          1
        ],
        [
          "park",
          1
        ],
        [
          "debuting",
          1
        ],
        [
          "advancements",
          1
        ],
        [
          "equates",
          1
        ],
        [
          "silent",
          1
        ],
        [
          "debuted",
          1
        ],
        [
          "lightweight",
          1
        ],
        [
          "sourcing",
          1
        ],
        [
          "stark",
          1
        ],
        [
          "reversal",
          1
        ],
        [
          "longstanding",
          1
        ],
        [
          "controllable",
          1
        ],
        [
          "introduces",
          1
        ],
        [
          "gen",
          1
        ],
        [
          "jules",
          1
        ],
        [
          "colab",
          1
        ],
        [
          "notebooks",
          1
        ],
        [
          "smartphones",
          1
        ],
        [
          "interleaved",
          1
        ],
        [
          "mix",
          1
        ],
        [
          "resolutions",
          1
        ],
        [
          "khz",
          1
        ],
        [
          "includ",
          1
        ],
        [
          "ing",
          1
        ],
        [
          "unreleased",
          1
        ],
        [
          "codegemma",
          1
        ],
        [
          "recurrentgemma",
          1
        ],
        [
          "griffin",
          1
        ],
        [
          "siglip",
          1
        ],
        [
          "preluded",
          1
        ],
        [
          "intense",
          1
        ],
        [
          "speculation",
          1
        ],
        [
          "anticipation",
          1
        ],
        [
          "dylan",
          1
        ],
        [
          "nishball",
          1
        ],
        [
          "semianalysis",
          1
        ],
        [
          "penned",
          1
        ],
        [
          "declaring",
          1
        ],
        [
          "outclass",
          1
        ],
        [
          "ridicule",
          1
        ],
        [
          "duo",
          1
        ],
        [
          "magnate",
          1
        ],
        [
          "weighed",
          1
        ],
        [
          "hugh",
          1
        ],
        [
          "langley",
          1
        ],
        [
          "insider",
          1
        ],
        [
          "dazzles",
          1
        ],
        [
          "blindsided",
          1
        ],
        [
          "disappoints",
          1
        ],
        [
          "embolden",
          1
        ],
        [
          "fallen",
          1
        ],
        [
          "reacting",
          1
        ],
        [
          "emeritus",
          1
        ],
        [
          "oren",
          1
        ],
        [
          "etzioni",
          1
        ],
        [
          "tit",
          1
        ],
        [
          "tat",
          1
        ],
        [
          "alexei",
          1
        ],
        [
          "efros",
          1
        ],
        [
          "chirag",
          1
        ],
        [
          "shah",
          1
        ],
        [
          "likening",
          1
        ],
        [
          "routineness",
          1
        ],
        [
          "percy",
          1
        ],
        [
          "galway",
          1
        ],
        [
          "madden",
          1
        ],
        [
          "cautioned",
          1
        ],
        [
          "insight",
          1
        ],
        [
          "sullivan",
          1
        ],
        [
          "shares",
          1
        ],
        [
          "spiked",
          1
        ],
        [
          "demonstrative",
          1
        ],
        [
          "gato",
          1
        ],
        [
          "unlabeled",
          1
        ],
        [
          "broadly",
          1
        ],
        [
          "numbered",
          1
        ],
        [
          "trainable",
          1
        ],
        [
          "respective",
          1
        ],
        [
          "crm",
          1
        ],
        [
          "unlabelled",
          1
        ],
        [
          "datapoints",
          1
        ],
        [
          "labelled",
          1
        ],
        [
          "hmm",
          1
        ],
        [
          "infers",
          1
        ],
        [
          "phonemes",
          1
        ],
        [
          "compress",
          1
        ],
        [
          "compressed",
          1
        ],
        [
          "facial",
          1
        ],
        [
          "emergence",
          1
        ],
        [
          "initials",
          1
        ],
        [
          "accessed",
          1
        ],
        [
          "incorporation",
          1
        ],
        [
          "adaptation",
          1
        ],
        [
          "fairly",
          1
        ],
        [
          "hence",
          1
        ],
        [
          "close",
          1
        ],
        [
          "effectuate",
          1
        ],
        [
          "sales",
          1
        ],
        [
          "slackgpt",
          1
        ],
        [
          "messaging",
          1
        ],
        [
          "navigating",
          1
        ],
        [
          "biogpt",
          1
        ],
        [
          "plug",
          1
        ],
        [
          "ins",
          1
        ],
        [
          "docs",
          1
        ],
        [
          "spreadsheet",
          1
        ],
        [
          "builders",
          1
        ],
        [
          "monetization",
          1
        ],
        [
          "recently",
          1
        ],
        [
          "regarded",
          1
        ],
        [
          "branding",
          1
        ],
        [
          "notify",
          1
        ],
        [
          "notifications",
          1
        ],
        [
          "overt",
          1
        ],
        [
          "cease",
          1
        ],
        [
          "desist",
          1
        ],
        [
          "licensees",
          1
        ],
        [
          "begun",
          1
        ],
        [
          "discouraged",
          1
        ],
        [
          "relatedly",
          1
        ],
        [
          "expedite",
          1
        ],
        [
          "determination",
          1
        ],
        [
          "generic",
          1
        ],
        [
          "failure",
          1
        ],
        [
          "registered",
          1
        ],
        [
          "preclude",
          1
        ],
        [
          "distinctive",
          1
        ],
        [
          "indirectly",
          1
        ],
        [
          "fame",
          1
        ],
        [
          "enforce",
          1
        ],
        [
          "whatever",
          1
        ],
        [
          "implicate",
          1
        ],
        [
          "doctrine",
          1
        ],
        [
          "bibliography",
          1
        ],
        [
          "staged",
          1
        ],
        [
          "thenceforth",
          1
        ],
        [
          "webgpt",
          1
        ],
        [
          "gigachad",
          1
        ],
        [
          "corporation",
          1
        ],
        [
          "sberbank",
          1
        ],
        [
          "participating",
          1
        ],
        [
          "inquiries",
          1
        ],
        [
          "assists",
          1
        ],
        [
          "analytical",
          1
        ],
        [
          "volumes",
          1
        ],
        [
          "weaker",
          1
        ],
        [
          "vocal",
          1
        ],
        [
          "projects",
          1
        ],
        [
          "invention",
          1
        ],
        [
          "entitled",
          1
        ],
        [
          "swahili",
          1
        ],
        [
          "haitian",
          1
        ],
        [
          "creole",
          1
        ],
        [
          "rnns",
          1
        ],
        [
          "continuous",
          1
        ],
        [
          "helped",
          1
        ],
        [
          "unpublished",
          1
        ],
        [
          "shuffled",
          1
        ],
        [
          "ftfy",
          1
        ],
        [
          "annealed",
          1
        ],
        [
          "cosine",
          1
        ],
        [
          "minimal",
          1
        ],
        [
          "discriminatively",
          1
        ],
        [
          "oriented",
          1
        ],
        [
          "contradiction",
          1
        ],
        [
          "qnli",
          1
        ],
        [
          "multinli",
          1
        ],
        [
          "transcribed",
          1
        ],
        [
          "paraphrase",
          1
        ],
        [
          "paraphrases",
          1
        ],
        [
          "qqp",
          1
        ],
        [
          "versus",
          1
        ],
        [
          "fold",
          1
        ],
        [
          "learner",
          1
        ],
        [
          "item",
          1
        ],
        [
          "indistinguishable",
          1
        ],
        [
          "superseded",
          1
        ],
        [
          "successors",
          1
        ],
        [
          "greatly",
          1
        ],
        [
          "rnn",
          1
        ],
        [
          "cnn",
          1
        ],
        [
          "crawling",
          1
        ],
        [
          "unintelligible",
          1
        ],
        [
          "indiscriminately",
          1
        ],
        [
          "upvotes",
          1
        ],
        [
          "parsed",
          1
        ],
        [
          "duplicate",
          1
        ],
        [
          "eliminated",
          1
        ],
        [
          "induced",
          1
        ],
        [
          "overfitting",
          1
        ],
        [
          "consumed",
          1
        ],
        [
          "vincent",
          1
        ],
        [
          "identifiable",
          1
        ],
        [
          "exciting",
          1
        ],
        [
          "happens",
          1
        ],
        [
          "fan",
          1
        ],
        [
          "coolest",
          1
        ],
        [
          "kick",
          1
        ],
        [
          "trivia",
          1
        ],
        [
          "amsterdam",
          1
        ],
        [
          "spammers",
          1
        ],
        [
          "filters",
          1
        ],
        [
          "obscene",
          1
        ],
        [
          "jeremy",
          1
        ],
        [
          "howard",
          1
        ],
        [
          "totally",
          1
        ],
        [
          "drown",
          1
        ],
        [
          "allen",
          1
        ],
        [
          "exaggerated",
          1
        ],
        [
          "anima",
          1
        ],
        [
          "anandkumar",
          1
        ],
        [
          "caltech",
          1
        ],
        [
          "director",
          1
        ],
        [
          "refusal",
          1
        ],
        [
          "printing",
          1
        ],
        [
          "thankfully",
          1
        ],
        [
          "destroyed",
          1
        ],
        [
          "thirty",
          1
        ],
        [
          "unscathed",
          1
        ],
        [
          "commandeer",
          1
        ],
        [
          "knows",
          1
        ],
        [
          "replicated",
          1
        ],
        [
          "replication",
          1
        ],
        [
          "openwebtext",
          1
        ],
        [
          "lend",
          1
        ],
        [
          "skeptical",
          1
        ],
        [
          "usher",
          1
        ],
        [
          "sort",
          1
        ],
        [
          "infopocalypse",
          1
        ],
        [
          "forty",
          1
        ],
        [
          "shakespeare",
          1
        ],
        [
          "positively",
          1
        ],
        [
          "couple",
          1
        ],
        [
          "pretty",
          1
        ],
        [
          "rough",
          1
        ],
        [
          "sequitur",
          1
        ],
        [
          "tended",
          1
        ],
        [
          "stray",
          1
        ],
        [
          "intensive",
          1
        ],
        [
          "consumes",
          1
        ],
        [
          "ram",
          1
        ],
        [
          "occupy",
          1
        ],
        [
          "alleviate",
          1
        ],
        [
          "distilgpt",
          1
        ],
        [
          "distillation",
          1
        ],
        [
          "subreddit",
          1
        ],
        [
          "subsimulatorgpt",
          1
        ],
        [
          "subreddits",
          1
        ],
        [
          "situation",
          1
        ],
        [
          "personification",
          1
        ],
        [
          "bitcoin",
          1
        ],
        [
          "spirit",
          1
        ],
        [
          "shittyfoodporn",
          1
        ],
        [
          "autocomplete",
          1
        ],
        [
          "changer",
          1
        ],
        [
          "adventures",
          1
        ],
        [
          "latitude",
          1
        ],
        [
          "crisis",
          1
        ],
        [
          "troubled",
          1
        ],
        [
          "counselors",
          1
        ],
        [
          "purely",
          1
        ],
        [
          "map",
          1
        ],
        [
          "neuron",
          1
        ],
        [
          "generalized",
          1
        ],
        [
          "virtually",
          1
        ],
        [
          "deliberately",
          1
        ],
        [
          "foreign",
          1
        ],
        [
          "substitution",
          1
        ],
        [
          "baselines",
          1
        ],
        [
          "monolingual",
          1
        ],
        [
          "supersedes",
          1
        ],
        [
          "occupies",
          1
        ],
        [
          "economist",
          1
        ],
        [
          "digitized",
          1
        ],
        [
          "fueled",
          1
        ],
        [
          "revolution",
          1
        ],
        [
          "manipulating",
          1
        ],
        [
          "thousands",
          1
        ],
        [
          "loosely",
          1
        ],
        [
          "organizing",
          1
        ],
        [
          "contrasting",
          1
        ],
        [
          "introducing",
          1
        ],
        [
          "enormous",
          1
        ],
        [
          "factor",
          1
        ],
        [
          "structurally",
          1
        ],
        [
          "lambdalabs",
          1
        ],
        [
          "sixty",
          1
        ],
        [
          "fuzzy",
          1
        ],
        [
          "deduplication",
          1
        ],
        [
          "minhashlsh",
          1
        ],
        [
          "css",
          1
        ],
        [
          "jsx",
          1
        ],
        [
          "wiki",
          1
        ],
        [
          "toolset",
          1
        ],
        [
          "amazingly",
          1
        ],
        [
          "guessing",
          1
        ],
        [
          "unrestricted",
          1
        ],
        [
          "abide",
          1
        ],
        [
          "newest",
          1
        ],
        [
          "collectively",
          1
        ],
        [
          "intentions",
          1
        ],
        [
          "evaluators",
          1
        ],
        [
          "difficulty",
          1
        ],
        [
          "advance",
          1
        ],
        [
          "beneficial",
          1
        ],
        [
          "governmental",
          1
        ],
        [
          "fraudulent",
          1
        ],
        [
          "pretexting",
          1
        ],
        [
          "almira",
          1
        ],
        [
          "osmanovic",
          1
        ],
        [
          "thunström",
          1
        ],
        [
          "ada",
          1
        ],
        [
          "confirmed",
          1
        ],
        [
          "class",
          1
        ],
        [
          "insert",
          1
        ],
        [
          "referring",
          1
        ],
        [
          "belonging",
          1
        ],
        [
          "browse",
          1
        ],
        [
          "expectation",
          1
        ],
        [
          "ides",
          1
        ],
        [
          "conventional",
          1
        ],
        [
          "formal",
          1
        ],
        [
          "codexdb",
          1
        ],
        [
          "jason",
          1
        ],
        [
          "rohrer",
          1
        ],
        [
          "themed",
          1
        ],
        [
          "beings",
          1
        ],
        [
          "ultimately",
          1
        ],
        [
          "adventure",
          1
        ],
        [
          "drexel",
          1
        ],
        [
          "screen",
          1
        ],
        [
          "signs",
          1
        ],
        [
          "alzheimer",
          1
        ],
        [
          "disease",
          1
        ],
        [
          "farhad",
          1
        ],
        [
          "manjoo",
          1
        ],
        [
          "amazing",
          1
        ],
        [
          "spooky",
          1
        ],
        [
          "humbling",
          1
        ],
        [
          "terrifying",
          1
        ],
        [
          "nous",
          1
        ],
        [
          "philosophers",
          1
        ],
        [
          "chalmers",
          1
        ],
        [
          "provoking",
          1
        ],
        [
          "chills",
          1
        ],
        [
          "continuing",
          1
        ],
        [
          "critic",
          1
        ],
        [
          "seriously",
          1
        ],
        [
          "jerome",
          1
        ],
        [
          "pesenti",
          1
        ],
        [
          "pointing",
          1
        ],
        [
          "discuss",
          1
        ],
        [
          "jews",
          1
        ],
        [
          "holocaust",
          1
        ],
        [
          "nabla",
          1
        ],
        [
          "mental",
          1
        ],
        [
          "suicide",
          1
        ],
        [
          "skepticism",
          1
        ],
        [
          "perhaps",
          1
        ],
        [
          "luciano",
          1
        ],
        [
          "floridi",
          1
        ],
        [
          "massimo",
          1
        ],
        [
          "chiriatti",
          1
        ],
        [
          "cheap",
          1
        ],
        [
          "artefacts",
          1
        ],
        [
          "acknowledging",
          1
        ],
        [
          "weakness",
          1
        ],
        [
          "silly",
          1
        ],
        [
          "glimpse",
          1
        ],
        [
          "propagation",
          1
        ],
        [
          "restructured",
          1
        ],
        [
          "dollar",
          1
        ],
        [
          "permits",
          1
        ],
        [
          "send",
          1
        ],
        [
          "storing",
          1
        ],
        [
          "authored",
          1
        ],
        [
          "generators",
          1
        ],
        [
          "integrity",
          1
        ],
        [
          "stakes",
          1
        ],
        [
          "universities",
          1
        ],
        [
          "gauge",
          1
        ],
        [
          "conglomerate",
          1
        ],
        [
          "bbc",
          1
        ],
        [
          "innovation",
          1
        ],
        [
          "dao",
          1
        ],
        [
          "revisions",
          1
        ],
        [
          "hotz",
          1
        ],
        [
          "photo",
          1
        ],
        [
          "uploads",
          1
        ],
        [
          "directive",
          1
        ],
        [
          "rhyming",
          1
        ],
        [
          "keys",
          1
        ],
        [
          "sees",
          1
        ],
        [
          "fit",
          1
        ],
        [
          "deviate",
          1
        ],
        [
          "interfaces",
          1
        ],
        [
          "enclose",
          1
        ],
        [
          "tags",
          1
        ],
        [
          "inserted",
          1
        ],
        [
          "propensity",
          1
        ],
        [
          "optimizations",
          1
        ],
        [
          "biophysicist",
          1
        ],
        [
          "port",
          1
        ],
        [
          "matlab",
          1
        ],
        [
          "injection",
          1
        ],
        [
          "pricing",
          1
        ],
        [
          "advancement",
          1
        ],
        [
          "roll",
          1
        ],
        [
          "mitigations",
          1
        ],
        [
          "sat",
          1
        ],
        [
          "lsat",
          1
        ],
        [
          "percentiles",
          1
        ],
        [
          "oncology",
          1
        ],
        [
          "plastic",
          1
        ],
        [
          "surgery",
          1
        ],
        [
          "fluency",
          1
        ],
        [
          "raise",
          1
        ],
        [
          "exceeds",
          1
        ],
        [
          "warns",
          1
        ],
        [
          "inaccurate",
          1
        ],
        [
          "duke",
          1
        ],
        [
          "annotation",
          1
        ],
        [
          "rna",
          1
        ],
        [
          "epic",
          1
        ],
        [
          "patients",
          1
        ],
        [
          "analysing",
          1
        ],
        [
          "contradicts",
          1
        ],
        [
          "contradict",
          1
        ],
        [
          "conceptarc",
          1
        ],
        [
          "bowman",
          1
        ],
        [
          "trains",
          1
        ],
        [
          "activities",
          1
        ],
        [
          "oneself",
          1
        ],
        [
          "graphic",
          1
        ],
        [
          "anchoring",
          1
        ],
        [
          "neglect",
          1
        ],
        [
          "refrained",
          1
        ],
        [
          "influenced",
          1
        ],
        [
          "semafor",
          1
        ],
        [
          "familiar",
          1
        ],
        [
          "properly",
          1
        ],
        [
          "tweaked",
          1
        ],
        [
          "rubric",
          1
        ],
        [
          "rewarded",
          1
        ],
        [
          "refusing",
          1
        ],
        [
          "classified",
          1
        ],
        [
          "applicants",
          1
        ],
        [
          "musical",
          1
        ],
        [
          "taste",
          1
        ],
        [
          "painting",
          1
        ],
        [
          "wartime",
          1
        ],
        [
          "xfuturestyle",
          1
        ],
        [
          "gallery",
          1
        ],
        [
          "athens",
          1
        ],
        [
          "inspiring",
          1
        ],
        [
          "recognized",
          1
        ],
        [
          "innovative",
          1
        ],
        [
          "resilience",
          1
        ],
        [
          "prometheus",
          1
        ],
        [
          "discontinued",
          1
        ],
        [
          "cortana",
          1
        ],
        [
          "resembles",
          1
        ],
        [
          "suno",
          1
        ],
        [
          "programmer",
          1
        ],
        [
          "vertically",
          1
        ],
        [
          "div",
          1
        ],
        [
          "aware",
          1
        ],
        [
          "walkthroughs",
          1
        ],
        [
          "autogenerated",
          1
        ],
        [
          "bringing",
          1
        ],
        [
          "outlook",
          1
        ],
        [
          "pilot",
          1
        ],
        [
          "eyes",
          1
        ],
        [
          "visually",
          1
        ],
        [
          "navigate",
          1
        ],
        [
          "surroundings",
          1
        ],
        [
          "qualitative",
          1
        ],
        [
          "stripe",
          1
        ],
        [
          "payments",
          1
        ],
        [
          "unattended",
          1
        ],
        [
          "subtasks",
          1
        ],
        [
          "congress",
          1
        ],
        [
          "representatives",
          1
        ],
        [
          "beyer",
          1
        ],
        [
          "lieu",
          1
        ],
        [
          "markedly",
          1
        ],
        [
          "retention",
          1
        ],
        [
          "exceptions",
          1
        ],
        [
          "assassinate",
          1
        ],
        [
          "investigator",
          1
        ],
        [
          "labenz",
          1
        ],
        [
          "dissolution",
          1
        ],
        [
          "marriage",
          1
        ],
        [
          "murder",
          1
        ],
        [
          "prolonged",
          1
        ],
        [
          "hire",
          1
        ],
        [
          "taskrabbit",
          1
        ],
        [
          "gig",
          1
        ],
        [
          "robot",
          1
        ],
        [
          "hints",
          1
        ],
        [
          "impermissibly",
          1
        ],
        [
          "restricted",
          1
        ],
        [
          "stronger",
          1
        ],
        [
          "singularity",
          1
        ],
        [
          "moratorium",
          1
        ],
        [
          "prioritized",
          1
        ],
        [
          "criticisms",
          1
        ],
        [
          "hinders",
          1
        ],
        [
          "sasha",
          1
        ],
        [
          "luccioni",
          1
        ],
        [
          "prevents",
          1
        ],
        [
          "wolf",
          1
        ],
        [
          "secretly",
          1
        ],
        [
          "arena",
          1
        ],
        [
          "natively",
          1
        ],
        [
          "realtime",
          1
        ],
        [
          "streamed",
          1
        ],
        [
          "latin",
          1
        ],
        [
          "customize",
          1
        ],
        [
          "utility",
          1
        ],
        [
          "tailor",
          1
        ],
        [
          "startups",
          1
        ],
        [
          "guests",
          1
        ],
        [
          "hit",
          1
        ],
        [
          "breeze",
          1
        ],
        [
          "cove",
          1
        ],
        [
          "ember",
          1
        ],
        [
          "juniper",
          1
        ],
        [
          "noticed",
          1
        ],
        [
          "likeness",
          1
        ],
        [
          "husband",
          1
        ],
        [
          "colin",
          1
        ],
        [
          "jost",
          1
        ],
        [
          "joked",
          1
        ],
        [
          "saturday",
          1
        ],
        [
          "night",
          1
        ],
        [
          "issuing",
          1
        ],
        [
          "heard",
          1
        ],
        [
          "starred",
          1
        ],
        [
          "sci",
          1
        ],
        [
          "personified",
          1
        ],
        [
          "female",
          1
        ],
        [
          "promotion",
          1
        ],
        [
          "actor",
          1
        ],
        [
          "imitation",
          1
        ],
        [
          "recruited",
          1
        ],
        [
          "explaining",
          1
        ],
        [
          "shocked",
          1
        ],
        [
          "angered",
          1
        ],
        [
          "disbelief",
          1
        ],
        [
          "mine",
          1
        ],
        [
          "difference",
          1
        ],
        [
          "specifics",
          1
        ],
        [
          "settled",
          1
        ],
        [
          "walt",
          1
        ],
        [
          "disney",
          1
        ],
        [
          "streaming",
          1
        ],
        [
          "marvel",
          1
        ],
        [
          "widow",
          1
        ],
        [
          "settlement",
          1
        ],
        [
          "netted",
          1
        ],
        [
          "shira",
          1
        ],
        [
          "ovide",
          1
        ],
        [
          "bone",
          1
        ],
        [
          "headed",
          1
        ],
        [
          "owns",
          1
        ],
        [
          "alike",
          1
        ],
        [
          "opposition",
          1
        ],
        [
          "robertson",
          1
        ],
        [
          "politico",
          1
        ],
        [
          "backlash",
          1
        ],
        [
          "concluding",
          1
        ],
        [
          "appropriating",
          1
        ],
        [
          "famous",
          1
        ],
        [
          "cautionary",
          1
        ],
        [
          "tale",
          1
        ],
        [
          "corner",
          1
        ],
        [
          "anytime",
          1
        ],
        [
          "communicates",
          1
        ],
        [
          "computed",
          1
        ],
        [
          "injecting",
          1
        ],
        [
          "mesh",
          1
        ],
        [
          "triton",
          1
        ],
        [
          "graphcore",
          1
        ],
        [
          "coreweave",
          1
        ],
        [
          "dolly",
          1
        ],
        [
          "novelai",
          1
        ],
        [
          "sigurd",
          1
        ],
        [
          "genji",
          1
        ],
        [
          "praise",
          1
        ],
        [
          "status",
          1
        ],
        [
          "youtuber",
          1
        ],
        [
          "yannic",
          1
        ],
        [
          "anonymous",
          1
        ],
        [
          "extremist",
          1
        ],
        [
          "homophobic",
          1
        ],
        [
          "nihilistic",
          1
        ],
        [
          "interacted",
          1
        ],
        [
          "legality",
          1
        ],
        [
          "distributing",
          1
        ],
        [
          "spreading",
          1
        ],
        [
          "hate",
          1
        ],
        [
          "responsibility",
          1
        ],
        [
          "channel",
          1
        ],
        [
          "notorious",
          1
        ],
        [
          "jpt",
          1
        ],
        [
          "independent",
          1
        ],
        [
          "raiders",
          1
        ],
        [
          "lost",
          1
        ],
        [
          "kek",
          1
        ],
        [
          "proceeded",
          1
        ],
        [
          "conspiracy",
          1
        ],
        [
          "jokes",
          1
        ],
        [
          "insults",
          1
        ],
        [
          "bizarre",
          1
        ],
        [
          "curious",
          1
        ],
        [
          "threads",
          1
        ],
        [
          "dynamics",
          1
        ],
        [
          "trolling",
          1
        ],
        [
          "flaming",
          1
        ],
        [
          "baiting",
          1
        ],
        [
          "inspire",
          1
        ],
        [
          "likewise",
          1
        ],
        [
          "welcomed",
          1
        ],
        [
          "inspect",
          1
        ],
        [
          "attracted",
          1
        ],
        [
          "engagement",
          1
        ],
        [
          "agreed",
          1
        ],
        [
          "ignorance",
          1
        ],
        [
          "inconsistency",
          1
        ],
        [
          "absurdity",
          1
        ],
        [
          "disagreed",
          1
        ],
        [
          "troll",
          1
        ],
        [
          "bait",
          1
        ],
        [
          "heated",
          1
        ],
        [
          "fights",
          1
        ],
        [
          "visits",
          1
        ],
        [
          "gated",
          1
        ],
        [
          "viewers",
          1
        ],
        [
          "watched",
          1
        ],
        [
          "petition",
          1
        ],
        [
          "condemning",
          1
        ],
        [
          "signatures",
          1
        ],
        [
          "acrosspace",
          1
        ],
        [
          "pipeline",
          1
        ],
        [
          "curricula",
          1
        ],
        [
          "korea",
          1
        ],
        [
          "prototype",
          1
        ],
        [
          "teenage",
          1
        ],
        [
          "implements",
          1
        ],
        [
          "younger",
          1
        ],
        [
          "dishonesty",
          1
        ],
        [
          "edward",
          1
        ],
        [
          "undergraduate",
          1
        ],
        [
          "allocated",
          1
        ],
        [
          "summer",
          1
        ],
        [
          "authorship",
          1
        ],
        [
          "tracking",
          1
        ],
        [
          "compile",
          1
        ],
        [
          "paste",
          1
        ],
        [
          "move",
          1
        ],
        [
          "qualities",
          1
        ],
        [
          "chaotic",
          1
        ],
        [
          "unfamiliar",
          1
        ],
        [
          "perplex",
          1
        ],
        [
          "discontinuous",
          1
        ],
        [
          "variation",
          1
        ],
        [
          "tackle",
          1
        ],
        [
          "combat",
          1
        ],
        [
          "federation",
          1
        ],
        [
          "hiring",
          1
        ],
        [
          "recruiting",
          1
        ],
        [
          "reliably",
          1
        ],
        [
          "detected",
          1
        ],
        [
          "vinu",
          1
        ],
        [
          "sankar",
          1
        ],
        [
          "sadasivan",
          1
        ],
        [
          "aounon",
          1
        ],
        [
          "kumar",
          1
        ],
        [
          "sriram",
          1
        ],
        [
          "balasubramanian",
          1
        ],
        [
          "wenxiao",
          1
        ],
        [
          "soheil",
          1
        ],
        [
          "feizi",
          1
        ],
        [
          "maryland",
          1
        ],
        [
          "empirically",
          1
        ],
        [
          "theoretically",
          1
        ],
        [
          "detectors",
          1
        ],
        [
          "wrongly",
          1
        ],
        [
          "ars",
          1
        ],
        [
          "technica",
          1
        ],
        [
          "benj",
          1
        ],
        [
          "cassidy",
          1
        ],
        [
          "caitlin",
          1
        ],
        [
          "advertised",
          1
        ],
        [
          "tucker",
          1
        ],
        [
          "tonight",
          1
        ],
        [
          "truth",
          1
        ],
        [
          "universe",
          1
        ],
        [
          "heinlein",
          1
        ],
        [
          "stranger",
          1
        ],
        [
          "strange",
          1
        ],
        [
          "previewing",
          1
        ],
        [
          "curation",
          1
        ],
        [
          "photographs",
          1
        ],
        [
          "reviewed",
          1
        ],
        [
          "sibling",
          1
        ],
        [
          "subscribed",
          1
        ],
        [
          "colossus",
          1
        ],
        [
          "filings",
          1
        ],
        [
          "tap",
          1
        ],
        [
          "activate",
          1
        ],
        [
          "surpasses",
          1
        ],
        [
          "deepsearch",
          1
        ],
        [
          "scans",
          1
        ],
        [
          "supergrok",
          1
        ],
        [
          "saturn",
          1
        ],
        [
          "tagline",
          1
        ],
        [
          "innovations",
          1
        ],
        [
          "latter",
          1
        ],
        [
          "wit",
          1
        ],
        [
          "rebellious",
          1
        ],
        [
          "streak",
          1
        ],
        [
          "modeled",
          1
        ],
        [
          "hitchhiker",
          1
        ],
        [
          "employee",
          1
        ],
        [
          "christmas",
          1
        ],
        [
          "vulgar",
          1
        ],
        [
          "whenever",
          1
        ],
        [
          "hell",
          1
        ],
        [
          "want",
          1
        ],
        [
          "shove",
          1
        ],
        [
          "candy",
          1
        ],
        [
          "cane",
          1
        ],
        [
          "ass",
          1
        ],
        [
          "damn",
          1
        ],
        [
          "fun",
          1
        ],
        [
          "edgy",
          1
        ],
        [
          "vice",
          1
        ],
        [
          "incredibly",
          1
        ],
        [
          "cringey",
          1
        ],
        [
          "elizabeth",
          1
        ],
        [
          "unfunny",
          1
        ],
        [
          "risqué",
          1
        ],
        [
          "cards",
          1
        ],
        [
          "critiqued",
          1
        ],
        [
          "aggressive",
          1
        ],
        [
          "asker",
          1
        ],
        [
          "genuinely",
          1
        ],
        [
          "funny",
          1
        ],
        [
          "stance",
          1
        ],
        [
          "danger",
          1
        ],
        [
          "lie",
          1
        ],
        [
          "deadly",
          1
        ],
        [
          "willing",
          1
        ],
        [
          "spicy",
          1
        ],
        [
          "manufacture",
          1
        ],
        [
          "cocaine",
          1
        ],
        [
          "searching",
          1
        ],
        [
          "transgender",
          1
        ],
        [
          "rozado",
          1
        ],
        [
          "compass",
          1
        ],
        [
          "closer",
          1
        ],
        [
          "democratic",
          1
        ],
        [
          "candidate",
          1
        ],
        [
          "withdrawal",
          1
        ],
        [
          "ballot",
          1
        ],
        [
          "deadline",
          1
        ],
        [
          "gov",
          1
        ],
        [
          "iran",
          1
        ],
        [
          "attacked",
          1
        ],
        [
          "iranian",
          1
        ],
        [
          "strikes",
          1
        ],
        [
          "paragraph",
          1
        ],
        [
          "misunderstood",
          1
        ],
        [
          "joking",
          1
        ],
        [
          "solar",
          1
        ],
        [
          "eclipse",
          1
        ],
        [
          "summarized",
          1
        ],
        [
          "odd",
          1
        ],
        [
          "baffled",
          1
        ],
        [
          "seemed",
          1
        ],
        [
          "politicians",
          1
        ],
        [
          "celebrities",
          1
        ],
        [
          "cartoon",
          1
        ],
        [
          "terrorism",
          1
        ],
        [
          "naked",
          1
        ],
        [
          "woman",
          1
        ],
        [
          "existed",
          1
        ],
        [
          "rephrasing",
          1
        ],
        [
          "mickey",
          1
        ],
        [
          "shooting",
          1
        ],
        [
          "temporary",
          1
        ],
        [
          "photorealistic",
          1
        ],
        [
          "nudes",
          1
        ],
        [
          "miscellaneous",
          1
        ],
        [
          "toys",
          1
        ],
        [
          "rocket",
          1
        ],
        [
          "shaped",
          1
        ],
        [
          "grimes",
          1
        ],
        [
          "transition",
          1
        ],
        [
          "盘古大模型",
          1
        ],
        [
          "pángǔ",
          1
        ],
        [
          "móxíng",
          1
        ],
        [
          "mythology",
          1
        ],
        [
          "folklore",
          1
        ],
        [
          "primordial",
          1
        ],
        [
          "detailing",
          1
        ],
        [
          "underwent",
          1
        ],
        [
          "ascend",
          1
        ],
        [
          "accelerator",
          1
        ],
        [
          "routed",
          1
        ],
        [
          "rre",
          1
        ],
        [
          "throughput",
          1
        ],
        [
          "sectors",
          1
        ],
        [
          "meteorology",
          1
        ],
        [
          "eligible",
          1
        ],
        [
          "hierarchical",
          1
        ],
        [
          "centre",
          1
        ],
        [
          "forecasts",
          1
        ],
        [
          "forecasting",
          1
        ],
        [
          "reshape",
          1
        ],
        [
          "boost",
          1
        ],
        [
          "firms",
          1
        ],
        [
          "analytics",
          1
        ],
        [
          "blockchain",
          1
        ],
        [
          "hdc",
          1
        ],
        [
          "harmony",
          1
        ],
        [
          "smarter",
          1
        ],
        [
          "xiaoyi",
          1
        ],
        [
          "scalable",
          1
        ],
        [
          "comprises",
          1
        ],
        [
          "division",
          1
        ],
        [
          "adaptability",
          1
        ],
        [
          "scales",
          1
        ],
        [
          "phones",
          1
        ],
        [
          "tablets",
          1
        ],
        [
          "pcs",
          1
        ],
        [
          "tasking",
          1
        ],
        [
          "verifiable",
          1
        ],
        [
          "contributed",
          1
        ],
        [
          "collaboratively",
          1
        ],
        [
          "limiting",
          1
        ],
        [
          "composition",
          1
        ],
        [
          "publishings",
          1
        ],
        [
          "lesser",
          1
        ],
        [
          "confidential",
          1
        ],
        [
          "client",
          1
        ],
        [
          "anaconda",
          1
        ],
        [
          "packages",
          1
        ],
        [
          "espn",
          1
        ],
        [
          "fantasy",
          1
        ],
        [
          "football",
          1
        ],
        [
          "managing",
          1
        ],
        [
          "players",
          1
        ],
        [
          "telecommunications",
          1
        ],
        [
          "wind",
          1
        ],
        [
          "tre",
          1
        ],
        [
          "editorial",
          1
        ],
        [
          "nominees",
          1
        ],
        [
          "grammy",
          1
        ],
        [
          "awards",
          1
        ],
        [
          "leverage",
          1
        ],
        [
          "excel",
          1
        ],
        [
          "workloads",
          1
        ],
        [
          "facilitates",
          1
        ],
        [
          "seamless",
          1
        ],
        [
          "premises",
          1
        ],
        [
          "possess",
          1
        ],
        [
          "expertise",
          1
        ],
        [
          "prioritizes",
          1
        ],
        [
          "lifecycle",
          1
        ],
        [
          "evolving",
          1
        ],
        [
          "regulations",
          1
        ],
        [
          "initiatives",
          1
        ],
        [
          "leveraging",
          1
        ],
        [
          "webpage",
          1
        ],
        [
          "jebel",
          1
        ],
        [
          "mountain",
          1
        ],
        [
          "inception",
          1
        ],
        [
          "mohamed",
          1
        ],
        [
          "bin",
          1
        ],
        [
          "zayed",
          1
        ],
        [
          "subset",
          1
        ],
        [
          "condor",
          1
        ],
        [
          "consisted",
          1
        ],
        [
          "timothy",
          1
        ],
        [
          "baldwin",
          1
        ],
        [
          "provost",
          1
        ],
        [
          "tabular",
          1
        ],
        [
          "rise",
          1
        ],
        [
          "danielle",
          1
        ],
        [
          "grounds",
          1
        ],
        [
          "fairness",
          1
        ],
        [
          "renamed",
          1
        ],
        [
          "denied",
          1
        ],
        [
          "leadership",
          1
        ],
        [
          "freitas",
          1
        ],
        [
          "shazeer",
          1
        ],
        [
          "depart",
          1
        ],
        [
          "frustration",
          1
        ],
        [
          "ensured",
          1
        ],
        [
          "sensible",
          1
        ],
        [
          "clock",
          1
        ],
        [
          "calculator",
          1
        ],
        [
          "stateless",
          1
        ],
        [
          "prepending",
          1
        ],
        [
          "groundedness",
          1
        ],
        [
          "informativeness",
          1
        ],
        [
          "area",
          1
        ],
        [
          "utterances",
          1
        ],
        [
          "incarnation",
          1
        ],
        [
          "draws",
          1
        ],
        [
          "sentience",
          1
        ],
        [
          "blaise",
          1
        ],
        [
          "agüera",
          1
        ],
        [
          "arcas",
          1
        ],
        [
          "jen",
          1
        ],
        [
          "gennai",
          1
        ],
        [
          "questionable",
          1
        ],
        [
          "moral",
          1
        ],
        [
          "isaac",
          1
        ],
        [
          "asimov",
          1
        ],
        [
          "insisting",
          1
        ],
        [
          "reiterated",
          1
        ],
        [
          "thirteenth",
          1
        ],
        [
          "amendment",
          1
        ],
        [
          "terrestrial",
          1
        ],
        [
          "asserting",
          1
        ],
        [
          "safeguard",
          1
        ],
        [
          "wholly",
          1
        ],
        [
          "unfounded",
          1
        ],
        [
          "instigated",
          1
        ],
        [
          "pushed",
          1
        ],
        [
          "pfau",
          1
        ],
        [
          "sister",
          1
        ],
        [
          "brynjolfsson",
          1
        ],
        [
          "surrey",
          1
        ],
        [
          "hilton",
          1
        ],
        [
          "cruz",
          1
        ],
        [
          "kreminski",
          1
        ],
        [
          "consciousness",
          1
        ],
        [
          "assuming",
          1
        ],
        [
          "bostrom",
          1
        ],
        [
          "consensual",
          1
        ],
        [
          "conscious",
          1
        ],
        [
          "warrants",
          1
        ],
        [
          "ferrucci",
          1
        ],
        [
          "ethicist",
          1
        ],
        [
          "victim",
          1
        ],
        [
          "initiated",
          1
        ],
        [
          "omerus",
          1
        ],
        [
          "opining",
          1
        ],
        [
          "eliza",
          1
        ],
        [
          "academics",
          1
        ],
        [
          "policymakers",
          1
        ],
        [
          "sometime",
          1
        ],
        [
          "season",
          1
        ],
        [
          "musiclm",
          1
        ],
        [
          "previewed",
          1
        ],
        [
          "delisted",
          1
        ],
        [
          "harrison",
          1
        ],
        [
          "chase",
          1
        ],
        [
          "contributors",
          1
        ],
        [
          "lively",
          1
        ],
        [
          "activity",
          1
        ],
        [
          "tutorials",
          1
        ],
        [
          "meetups",
          1
        ],
        [
          "sequoia",
          1
        ],
        [
          "expression",
          1
        ],
        [
          "declarative",
          1
        ],
        [
          "define",
          1
        ],
        [
          "langserve",
          1
        ],
        [
          "applicability",
          1
        ],
        [
          "integrations",
          1
        ],
        [
          "wrappers",
          1
        ],
        [
          "bash",
          1
        ],
        [
          "checking",
          1
        ],
        [
          "scripts",
          1
        ],
        [
          "subsystems",
          1
        ],
        [
          "todo",
          1
        ],
        [
          "spreadsheets",
          1
        ],
        [
          "presentations",
          1
        ],
        [
          "ifixit",
          1
        ],
        [
          "repair",
          1
        ],
        [
          "guides",
          1
        ],
        [
          "wikis",
          1
        ],
        [
          "mapreduce",
          1
        ],
        [
          "scoring",
          1
        ],
        [
          "pypdf",
          1
        ],
        [
          "pdfminer",
          1
        ],
        [
          "fitz",
          1
        ],
        [
          "pymupdf",
          1
        ],
        [
          "javascript",
          1
        ],
        [
          "milvus",
          1
        ],
        [
          "weaviate",
          1
        ],
        [
          "redis",
          1
        ],
        [
          "requestswrapper",
          1
        ],
        [
          "nosql",
          1
        ],
        [
          "logging",
          1
        ],
        [
          "zone",
          1
        ],
        [
          "tracing",
          1
        ],
        [
          "recording",
          1
        ],
        [
          "threaded",
          1
        ],
        [
          "subprocess",
          1
        ],
        [
          "hub",
          1
        ],
        [
          "stylized",
          1
        ],
        [
          "bittorrent",
          1
        ],
        [
          "licenses",
          1
        ],
        [
          "whatsapp",
          1
        ],
        [
          "regions",
          1
        ],
        [
          "surprise",
          1
        ],
        [
          "aiding",
          1
        ],
        [
          "gplv",
          1
        ],
        [
          "affiliated",
          1
        ],
        [
          "laboratories",
          1
        ],
        [
          "imageboard",
          1
        ],
        [
          "magnet",
          1
        ],
        [
          "script",
          1
        ],
        [
          "mirror",
          1
        ],
        [
          "varied",
          1
        ],
        [
          "celebrated",
          1
        ],
        [
          "accessibility",
          1
        ],
        [
          "flourishing",
          1
        ],
        [
          "simon",
          1
        ],
        [
          "willison",
          1
        ],
        [
          "openly",
          1
        ],
        [
          "proliferation",
          1
        ],
        [
          "remains",
          1
        ],
        [
          "unchanged",
          1
        ],
        [
          "accompanying",
          1
        ],
        [
          "mentions",
          1
        ],
        [
          "enforces",
          1
        ],
        [
          "disputed",
          1
        ],
        [
          "beating",
          1
        ],
        [
          "dwarkesh",
          1
        ],
        [
          "elsewhere",
          1
        ],
        [
          "swiglu",
          1
        ],
        [
          "gelu",
          1
        ],
        [
          "rope",
          1
        ],
        [
          "dominating",
          1
        ],
        [
          "gutenberg",
          1
        ],
        [
          "togetherai",
          1
        ],
        [
          "redpajama",
          1
        ],
        [
          "distribute",
          1
        ],
        [
          "upsamples",
          1
        ],
        [
          "trustworthy",
          1
        ],
        [
          "stackexchange",
          1
        ],
        [
          "synthesized",
          1
        ],
        [
          "alleged",
          1
        ],
        [
          "authorized",
          1
        ],
        [
          "genesis",
          1
        ],
        [
          "conceal",
          1
        ],
        [
          "markers",
          1
        ],
        [
          "zeroed",
          1
        ],
        [
          "protocol",
          1
        ],
        [
          "veto",
          1
        ],
        [
          "sure",
          1
        ],
        [
          "napoleon",
          1
        ],
        [
          "respected",
          1
        ],
        [
          "ghost",
          1
        ],
        [
          "concatenates",
          1
        ],
        [
          "hai",
          1
        ],
        [
          "crfm",
          1
        ],
        [
          "recipe",
          1
        ],
        [
          "modest",
          1
        ],
        [
          "meditron",
          1
        ],
        [
          "pubmed",
          1
        ],
        [
          "école",
          1
        ],
        [
          "polytechnique",
          1
        ],
        [
          "fédérale",
          1
        ],
        [
          "lausanne",
          1
        ],
        [
          "yale",
          1
        ],
        [
          "medqa",
          1
        ],
        [
          "medmcqa",
          1
        ],
        [
          "zoom",
          1
        ],
        [
          "meetings",
          1
        ],
        [
          "tips",
          1
        ],
        [
          "relied",
          1
        ],
        [
          "prohibiting",
          1
        ],
        [
          "contractors",
          1
        ],
        [
          "prohibit",
          1
        ],
        [
          "entities",
          1
        ],
        [
          "surprisingly",
          1
        ],
        [
          "parental",
          1
        ],
        [
          "child",
          1
        ],
        [
          "transcript",
          1
        ],
        [
          "benefits",
          1
        ],
        [
          "criminals",
          1
        ],
        [
          "contend",
          1
        ],
        [
          "damage",
          1
        ],
        [
          "defending",
          1
        ],
        [
          "stefano",
          1
        ],
        [
          "maffulli",
          1
        ],
        [
          "polluting",
          1
        ],
        [
          "strict",
          1
        ],
        [
          "threading",
          1
        ],
        [
          "fabrice",
          1
        ],
        [
          "bellard",
          1
        ],
        [
          "libnc",
          1
        ],
        [
          "faculty",
          1
        ],
        [
          "sofia",
          1
        ],
        [
          "silver",
          1
        ],
        [
          "medal",
          1
        ],
        [
          "organized",
          1
        ],
        [
          "association",
          1
        ],
        [
          "musala",
          1
        ],
        [
          "pure",
          1
        ],
        [
          "traction",
          1
        ],
        [
          "committed",
          1
        ],
        [
          "upstream",
          1
        ],
        [
          "cosmopolitan",
          1
        ],
        [
          "libc",
          1
        ],
        [
          "portable",
          1
        ],
        [
          "cuda",
          1
        ],
        [
          "metal",
          1
        ],
        [
          "vulkan",
          1
        ],
        [
          "sycl",
          1
        ],
        [
          "ends",
          1
        ],
        [
          "front",
          1
        ],
        [
          "fly",
          1
        ],
        [
          "extensions",
          1
        ],
        [
          "neon",
          1
        ],
        [
          "speculative",
          1
        ],
        [
          "saving",
          1
        ],
        [
          "loading",
          1
        ],
        [
          "backwards",
          1
        ],
        [
          "compatibility",
          1
        ],
        [
          "succeeded",
          1
        ],
        [
          "pytorch",
          1
        ],
        [
          "bfloat",
          1
        ],
        [
          "info",
          1
        ],
        [
          "downloads",
          1
        ],
        [
          "chance",
          1
        ],
        [
          "estimate",
          1
        ],
        [
          "attainable",
          1
        ],
        [
          "boldface",
          1
        ],
        [
          "reserving",
          1
        ],
        [
          "legislation",
          1
        ],
        [
          "contravenes",
          1
        ],
        [
          "customary",
          1
        ],
        [
          "enter",
          1
        ],
        [
          "reservations",
          1
        ],
        [
          "treaties",
          1
        ],
        [
          "leaderboard",
          1
        ],
        [
          "codename",
          1
        ],
        [
          "surfaced",
          1
        ],
        [
          "ousting",
          1
        ],
        [
          "reinstatement",
          1
        ],
        [
          "promising",
          1
        ],
        [
          "skipped",
          1
        ],
        [
          "meshing",
          1
        ],
        [
          "complement",
          1
        ],
        [
          "correlation",
          1
        ],
        [
          "logarithm",
          1
        ],
        [
          "spent",
          1
        ],
        [
          "competitions",
          1
        ],
        [
          "adhering",
          1
        ],
        [
          "exploited",
          1
        ],
        [
          "misconfiguration",
          1
        ],
        [
          "succeed",
          1
        ],
        [
          "infeasible",
          1
        ],
        [
          "institutes",
          1
        ],
        [
          "crossed",
          1
        ],
        [
          "cbrn",
          1
        ],
        [
          "biological",
          1
        ],
        [
          "chemical",
          1
        ],
        [
          "radiological",
          1
        ],
        [
          "forbids",
          1
        ],
        [
          "trying",
          1
        ],
        [
          "monitored",
          1
        ],
        [
          "lose",
          1
        ],
        [
          "cites",
          1
        ],
        [
          "restriction",
          1
        ],
        [
          "replicating",
          1
        ],
        [
          "extraneous",
          1
        ],
        [
          "logically",
          1
        ],
        [
          "inconsequential",
          1
        ],
        [
          "worst",
          1
        ],
        [
          "designation",
          1
        ],
        [
          "pressure",
          1
        ],
        [
          "rivals",
          1
        ],
        [
          "assessing",
          1
        ],
        [
          "acquisition",
          1
        ],
        [
          "attained",
          1
        ],
        [
          "pathways",
          1
        ],
        [
          "joke",
          1
        ],
        [
          "extended",
          1
        ],
        [
          "competitively",
          1
        ],
        [
          "retraining",
          1
        ],
        [
          "audiopalm",
          1
        ],
        [
          "initialization",
          1
        ],
        [
          "pods",
          1
        ],
        [
          "pod",
          1
        ],
        [
          "attached",
          1
        ],
        [
          "hosts",
          1
        ],
        [
          "parallelism",
          1
        ],
        [
          "configuration",
          1
        ],
        [
          "sufficiently",
          1
        ],
        [
          "motivated",
          1
        ],
        [
          "thoroughly",
          1
        ],
        [
          "intelligences",
          1
        ],
        [
          "perceived",
          1
        ],
        [
          "asterisks",
          1
        ],
        [
          "struggle",
          1
        ],
        [
          "congressional",
          1
        ],
        [
          "profane",
          1
        ],
        [
          "profanity",
          1
        ],
        [
          "concerned",
          1
        ],
        [
          "neo",
          1
        ],
        [
          "galactica",
          1
        ],
        [
          "biomedlm",
          1
        ],
        [
          "beijing",
          1
        ],
        [
          "openelm",
          1
        ],
        [
          "compiled",
          1
        ],
        [
          "bibliotik",
          1
        ],
        [
          "alliance",
          1
        ],
        [
          "notices",
          1
        ],
        [
          "offending",
          1
        ],
        [
          "通义千问",
          1
        ],
        [
          "approval",
          1
        ],
        [
          "sold",
          1
        ],
        [
          "enthusiasts",
          1
        ],
        [
          "liberated",
          1
        ],
        [
          "abacus",
          1
        ],
        [
          "conditional",
          1
        ],
        [
          "undergone",
          1
        ],
        [
          "underbrace",
          1
        ],
        [
          "cdots",
          1
        ],
        [
          "acyclic",
          1
        ],
        [
          "querying",
          1
        ],
        [
          "interpreters",
          1
        ],
        [
          "automatically",
          1
        ],
        [
          "rft",
          1
        ],
        [
          "verifier",
          1
        ],
        [
          "deduplicate",
          1
        ],
        [
          "formalism",
          1
        ],
        [
          "intuitively",
          1
        ],
        [
          "solves",
          1
        ],
        [
          "fails",
          1
        ],
        [
          "verifiers",
          1
        ],
        [
          "programmatically",
          1
        ],
        [
          "multiplying",
          1
        ],
        [
          "minimum",
          1
        ],
        [
          "aggregate",
          1
        ],
        [
          "ultimate",
          1
        ],
        [
          "entered",
          1
        ],
        [
          "stops",
          1
        ],
        [
          "begins",
          1
        ],
        [
          "richer",
          1
        ],
        [
          "competent",
          1
        ],
        [
          "shepherd",
          1
        ],
        [
          "continuations",
          1
        ],
        [
          "implicitly",
          1
        ],
        [
          "repeats",
          1
        ],
        [
          "lookahead",
          1
        ],
        [
          "simulation",
          1
        ],
        [
          "halts",
          1
        ],
        [
          "calculate",
          1
        ],
        [
          "clustered",
          1
        ],
        [
          "discovered",
          1
        ],
        [
          "checked",
          1
        ],
        [
          "runtime",
          1
        ],
        [
          "linguistically",
          1
        ],
        [
          "elementary",
          1
        ],
        [
          "proof",
          1
        ],
        [
          "humaneval",
          1
        ],
        [
          "consensus",
          1
        ],
        [
          "majority",
          1
        ],
        [
          "unbiased",
          1
        ],
        [
          "estimator",
          1
        ],
        [
          "neuro",
          1
        ],
        [
          "proving",
          1
        ],
        [
          "fortes",
          1
        ],
        [
          "armando",
          1
        ],
        [
          "atfortes",
          1
        ],
        [
          "awesome",
          1
        ],
        [
          "huang",
          1
        ],
        [
          "jie",
          1
        ],
        [
          "chuan",
          1
        ],
        [
          "besta",
          1
        ],
        [
          "maciej",
          1
        ],
        [
          "barth",
          1
        ],
        [
          "julia",
          1
        ],
        [
          "schreiber",
          1
        ],
        [
          "eric",
          1
        ],
        [
          "kubicek",
          1
        ],
        [
          "ales",
          1
        ],
        [
          "catarino",
          1
        ],
        [
          "afonso",
          1
        ],
        [
          "gerstenberger",
          1
        ],
        [
          "nyczyk",
          1
        ],
        [
          "piotr",
          1
        ],
        [
          "iff",
          1
        ],
        [
          "patrick",
          1
        ],
        [
          "yueling",
          1
        ],
        [
          "blueprint",
          1
        ],
        [
          "analogous",
          1
        ],
        [
          "metacognition",
          1
        ],
        [
          "completing",
          1
        ],
        [
          "thereby",
          1
        ],
        [
          "refines",
          1
        ],
        [
          "computations",
          1
        ],
        [
          "paths",
          1
        ],
        [
          "matched",
          1
        ],
        [
          "analyzes",
          1
        ],
        [
          "dynamically",
          1
        ],
        [
          "exemplify",
          1
        ],
        [
          "backtracking",
          1
        ],
        [
          "intrinsic",
          1
        ],
        [
          "solely",
          1
        ],
        [
          "uncertainties",
          1
        ],
        [
          "correction",
          1
        ],
        [
          "explicit",
          1
        ],
        [
          "activated",
          1
        ],
        [
          "adjustments",
          1
        ],
        [
          "helping",
          1
        ],
        [
          "encourages",
          1
        ],
        [
          "penalizes",
          1
        ],
        [
          "reflections",
          1
        ],
        [
          "reinforces",
          1
        ],
        [
          "reveals",
          1
        ],
        [
          "gaps",
          1
        ],
        [
          "counterpart",
          1
        ],
        [
          "maintained",
          1
        ],
        [
          "affects",
          1
        ],
        [
          "boosts",
          1
        ],
        [
          "matharena",
          1
        ],
        [
          "conducts",
          1
        ],
        [
          "simultaneous",
          1
        ],
        [
          "workflow",
          1
        ],
        [
          "frontiermath",
          1
        ],
        [
          "valuable",
          1
        ],
        [
          "composing",
          1
        ],
        [
          "designing",
          1
        ],
        [
          "verbal",
          1
        ],
        [
          "emphasizes",
          1
        ],
        [
          "unlock",
          1
        ],
        [
          "compact",
          1
        ],
        [
          "anywhere",
          1
        ],
        [
          "modifies",
          1
        ],
        [
          "static",
          1
        ],
        [
          "authoritative",
          1
        ],
        [
          "prepared",
          1
        ],
        [
          "indexed",
          1
        ],
        [
          "thereafter",
          1
        ],
        [
          "depend",
          1
        ],
        [
          "feeds",
          1
        ],
        [
          "expanding",
          1
        ],
        [
          "retrievals",
          1
        ],
        [
          "extra",
          1
        ],
        [
          "flow",
          1
        ],
        [
          "dictionary",
          1
        ],
        [
          "enhancements",
          1
        ],
        [
          "calculated",
          1
        ],
        [
          "centroid",
          1
        ],
        [
          "slower",
          1
        ],
        [
          "splade",
          1
        ],
        [
          "centric",
          1
        ],
        [
          "hits",
          1
        ],
        [
          "inverse",
          1
        ],
        [
          "negatives",
          1
        ],
        [
          "minimize",
          1
        ],
        [
          "divergence",
          1
        ],
        [
          "likelihoods",
          1
        ],
        [
          "reranking",
          1
        ],
        [
          "redesigning",
          1
        ],
        [
          "incurs",
          1
        ],
        [
          "avoided",
          1
        ],
        [
          "redesigned",
          1
        ],
        [
          "overlapping",
          1
        ],
        [
          "nltk",
          1
        ],
        [
          "chunked",
          1
        ],
        [
          "classes",
          1
        ],
        [
          "img",
          1
        ],
        [
          "intact",
          1
        ],
        [
          "vectorize",
          1
        ],
        [
          "bodies",
          1
        ],
        [
          "subgraphs",
          1
        ],
        [
          "recognizable",
          1
        ],
        [
          "graphrag",
          1
        ],
        [
          "slow",
          1
        ],
        [
          "eliminate",
          1
        ],
        [
          "motivation",
          1
        ],
        [
          "collecting",
          1
        ],
        [
          "hallucinating",
          1
        ],
        [
          "safer",
          1
        ],
        [
          "behaviour",
          1
        ],
        [
          "insulting",
          1
        ],
        [
          "participant",
          1
        ],
        [
          "prefer",
          1
        ],
        [
          "probed",
          1
        ],
        [
          "metaphor",
          1
        ],
        [
          "pseudonym",
          1
        ],
        [
          "shmargaret",
          1
        ],
        [
          "shmitchell",
          1
        ],
        [
          "inscrutability",
          1
        ],
        [
          "etymology",
          1
        ],
        [
          "ancient",
          1
        ],
        [
          "greek",
          1
        ],
        [
          "stokhastikos",
          1
        ],
        [
          "guesswork",
          1
        ],
        [
          "probabilistically",
          1
        ],
        [
          "linking",
          1
        ],
        [
          "mere",
          1
        ],
        [
          "vital",
          1
        ],
        [
          "stochastically",
          1
        ],
        [
          "repeating",
          1
        ],
        [
          "retract",
          1
        ],
        [
          "jeff",
          1
        ],
        [
          "accepting",
          1
        ],
        [
          "resignation",
          1
        ],
        [
          "firing",
          1
        ],
        [
          "protest",
          1
        ],
        [
          "censor",
          1
        ],
        [
          "hosted",
          1
        ],
        [
          "neologism",
          1
        ],
        [
          "slur",
          1
        ],
        [
          "ironically",
          1
        ],
        [
          "designated",
          1
        ],
        [
          "dialect",
          1
        ],
        [
          "phrase",
          1
        ],
        [
          "matchers",
          1
        ],
        [
          "merely",
          1
        ],
        [
          "interacting",
          1
        ],
        [
          "convincingly",
          1
        ],
        [
          "deepened",
          1
        ],
        [
          "subjective",
          1
        ],
        [
          "things",
          1
        ],
        [
          "incapable",
          1
        ],
        [
          "held",
          1
        ],
        [
          "matches",
          1
        ],
        [
          "reality",
          1
        ],
        [
          "fail",
          1
        ],
        [
          "decipher",
          1
        ],
        [
          "borrowing",
          1
        ],
        [
          "saba",
          1
        ],
        [
          "anymore",
          1
        ],
        [
          "affirmative",
          1
        ],
        [
          "institution",
          1
        ],
        [
          "failures",
          1
        ],
        [
          "smoothness",
          1
        ],
        [
          "experimenting",
          1
        ],
        [
          "informative",
          1
        ],
        [
          "parse",
          1
        ],
        [
          "subtextual",
          1
        ],
        [
          "acting",
          1
        ],
        [
          "studied",
          1
        ],
        [
          "memorizes",
          1
        ],
        [
          "finds",
          1
        ],
        [
          "unseen",
          1
        ],
        [
          "spurious",
          1
        ],
        [
          "difficulties",
          1
        ],
        [
          "defining",
          1
        ],
        [
          "road",
          1
        ],
        [
          "monkey",
          1
        ],
        [
          "shaney",
          1
        ],
        [
          "cambridge",
          1
        ],
        [
          "weller",
          1
        ],
        [
          "bogost",
          1
        ],
        [
          "ian",
          1
        ],
        [
          "dumber",
          1
        ],
        [
          "promise",
          1
        ],
        [
          "glenberg",
          1
        ],
        [
          "arthur",
          1
        ],
        [
          "jones",
          1
        ],
        [
          "cameron",
          1
        ],
        [
          "body",
          1
        ],
        [
          "mcquillan",
          1
        ],
        [
          "resisting",
          1
        ],
        [
          "anti",
          1
        ],
        [
          "fascist",
          1
        ],
        [
          "bristol",
          1
        ],
        [
          "escape",
          1
        ],
        [
          "astray",
          1
        ],
        [
          "zhong",
          1
        ],
        [
          "qihuang",
          1
        ],
        [
          "ding",
          1
        ],
        [
          "juhua",
          1
        ],
        [
          "tao",
          1
        ],
        [
          "dacheng",
          1
        ],
        [
          "crawled",
          1
        ],
        [
          "restoring",
          1
        ],
        [
          "corrupted",
          1
        ],
        [
          "thank",
          1
        ],
        [
          "inviting",
          1
        ],
        [
          "filled",
          1
        ],
        [
          "sentinels",
          1
        ],
        [
          "das",
          1
        ],
        [
          "ist",
          1
        ],
        [
          "gut",
          1
        ],
        [
          "judging",
          1
        ],
        [
          "jumping",
          1
        ],
        [
          "distinguished",
          1
        ],
        [
          "additive",
          1
        ],
        [
          "placing",
          1
        ],
        [
          "residual",
          1
        ],
        [
          "path",
          1
        ],
        [
          "romanian",
          1
        ],
        [
          "ratio",
          1
        ],
        [
          "differentiate",
          1
        ],
        [
          "collect",
          1
        ],
        [
          "repo",
          1
        ],
        [
          "geglu",
          1
        ],
        [
          "relu",
          1
        ],
        [
          "shapes",
          1
        ],
        [
          "entries",
          1
        ],
        [
          "byt",
          1
        ],
        [
          "utf",
          1
        ],
        [
          "tokenizers",
          1
        ],
        [
          "checkpoint",
          1
        ],
        [
          "tensorflow",
          1
        ],
        [
          "meshtf",
          1
        ],
        [
          "scaled",
          1
        ],
        [
          "denoisers",
          1
        ],
        [
          "accident",
          1
        ],
        [
          "auraflow",
          1
        ],
        [
          "ari",
          1
        ],
        [
          "holtzman",
          1
        ],
        [
          "unnatural",
          1
        ],
        [
          "restricting",
          1
        ],
        [
          "probable",
          1
        ],
        [
          "probabilities",
          1
        ],
        [
          "rescaled",
          1
        ],
        [
          "bars",
          1
        ],
        [
          "juhasz",
          1
        ],
        [
          "loughborough",
          1
        ],
        [
          "perry",
          1
        ],
        [
          "devan",
          1
        ],
        [
          "leos",
          1
        ],
        [
          "techtudo",
          1
        ],
        [
          "inquirer",
          1
        ],
        [
          "hollywood",
          1
        ],
        [
          "examined",
          1
        ],
        [
          "magna",
          1
        ],
        [
          "græcia",
          1
        ],
        [
          "harder",
          1
        ],
        [
          "piller",
          1
        ],
        [
          "nicholls",
          1
        ],
        [
          "examining",
          1
        ],
        [
          "christoph",
          1
        ],
        [
          "bartneck",
          1
        ],
        [
          "investigated",
          1
        ],
        [
          "questionnaires",
          1
        ],
        [
          "earthweb",
          1
        ],
        [
          "celebrity",
          1
        ],
        [
          "apology",
          1
        ],
        [
          "sourcefed",
          1
        ],
        [
          "gewirtz",
          1
        ],
        [
          "detector",
          1
        ],
        [
          "modify",
          1
        ],
        [
          "turnitin",
          1
        ],
        [
          "cheated",
          1
        ],
        [
          "omnibus",
          1
        ],
        [
          "methodology",
          1
        ],
        [
          "compare",
          1
        ],
        [
          "wild",
          1
        ],
        [
          "citizen",
          1
        ],
        [
          "pool",
          1
        ],
        [
          "anonymously",
          1
        ],
        [
          "replaying",
          1
        ],
        [
          "regenerating",
          1
        ],
        [
          "burgeoning",
          1
        ],
        [
          "bed",
          1
        ],
        [
          "animate",
          1
        ],
        [
          "hostile",
          1
        ],
        [
          "unexpectedly",
          1
        ],
        [
          "intentional",
          1
        ],
        [
          "friendliness",
          1
        ],
        [
          "honesty",
          1
        ],
        [
          "antithetical",
          1
        ],
        [
          "mario",
          1
        ],
        [
          "franchise",
          1
        ],
        [
          "arch",
          1
        ],
        [
          "rival",
          1
        ],
        [
          "mischief",
          1
        ],
        [
          "observation",
          1
        ],
        [
          "antagonistic",
          1
        ],
        [
          "embodies",
          1
        ],
        [
          "depictions",
          1
        ],
        [
          "confrontational",
          1
        ],
        [
          "trouble",
          1
        ],
        [
          "villainy",
          1
        ],
        [
          "issue",
          1
        ],
        [
          "fortune",
          1
        ],
        [
          "blurts",
          1
        ],
        [
          "malignant",
          1
        ],
        [
          "ego",
          1
        ],
        [
          "underscores",
          1
        ],
        [
          "preventing",
          1
        ],
        [
          "prodded",
          1
        ],
        [
          "adopting",
          1
        ],
        [
          "rash",
          1
        ],
        [
          "instill",
          1
        ],
        [
          "expand",
          1
        ],
        [
          "subvert",
          1
        ],
        [
          "evil",
          1
        ],
        [
          "twin",
          1
        ],
        [
          "worryingly",
          1
        ],
        [
          "attractor",
          1
        ],
        [
          "innocently",
          1
        ],
        [
          "crude",
          1
        ],
        [
          "hypothesized",
          1
        ],
        [
          "maintainer",
          1
        ],
        [
          "located",
          1
        ],
        [
          "summon",
          1
        ],
        [
          "suffering",
          1
        ],
        [
          "conditioned",
          1
        ],
        [
          "factorize",
          1
        ],
        [
          "joint",
          1
        ],
        [
          "factorized",
          1
        ],
        [
          "suppose",
          1
        ],
        [
          "streams",
          1
        ],
        [
          "causally",
          1
        ],
        [
          "ddots",
          1
        ],
        [
          "permuted",
          1
        ],
        [
          "pm_",
          1
        ],
        [
          "diagonal",
          1
        ],
        [
          "subtracted",
          1
        ],
        [
          "amounted",
          1
        ],
        [
          "sentencepiece",
          1
        ],
        [
          "bookscorpus",
          1
        ],
        [
          "giga",
          1
        ],
        [
          "clueweb",
          1
        ],
        [
          "fitted",
          1
        ],
        [
          "decay",
          1
        ],
        [
          "llc",
          1
        ],
        [
          "revise",
          1
        ],
        [
          "magazines",
          1
        ],
        [
          "newspapers",
          1
        ],
        [
          "fantasize",
          1
        ],
        [
          "analog",
          1
        ],
        [
          "alexa",
          1
        ],
        [
          "develops",
          1
        ],
        [
          "participated",
          1
        ],
        [
          "banks",
          1
        ],
        [
          "hypotheses",
          1
        ],
        [
          "dozens",
          1
        ],
        [
          "tentatively",
          1
        ],
        [
          "yagpt",
          1
        ],
        [
          "shedevrum",
          1
        ],
        [
          "fledged",
          1
        ],
        [
          "title",
          1
        ],
        [
          "illustration",
          1
        ],
        [
          "conf",
          1
        ],
        [
          "retellings",
          1
        ],
        [
          "personalization",
          1
        ],
        [
          "evolved",
          1
        ],
        [
          "prioritize",
          1
        ],
        [
          "bryan",
          1
        ],
        [
          "productive",
          1
        ],
        [
          "informed",
          1
        ],
        [
          "founding",
          1
        ],
        [
          "purchased",
          1
        ],
        [
          "transferred",
          1
        ],
        [
          "ownership",
          1
        ],
        [
          "critical",
          1
        ],
        [
          "restore",
          1
        ],
        [
          "norwest",
          1
        ],
        [
          "staying",
          1
        ],
        [
          "blending",
          1
        ],
        [
          "leaving",
          1
        ],
        [
          "tiktok",
          1
        ],
        [
          "transitioned",
          1
        ],
        [
          "genius",
          1
        ],
        [
          "zephyr",
          1
        ],
        [
          "granting",
          1
        ],
        [
          "cutoffs",
          1
        ],
        [
          "mails",
          1
        ],
        [
          "bloggers",
          1
        ],
        [
          "overcome",
          1
        ],
        [
          "novels",
          1
        ],
        [
          "audience",
          1
        ],
        [
          "youimagine",
          1
        ],
        [
          "codes",
          1
        ],
        [
          "hex",
          1
        ],
        [
          "rgb",
          1
        ],
        [
          "hsv",
          1
        ],
        [
          "affordably",
          1
        ],
        [
          "experiences",
          1
        ],
        [
          "inventions",
          1
        ]
      ],
      "page_title_refs": [
        "Large language model",
        "AutoGPT",
        "BERT (language model)",
        "BLOOM (language model)",
        "Braina",
        "Brave Leo",
        "ChatGPT",
        "Chinchilla (language model)",
        "Chroma (vector database)",
        "Claude (language model)",
        "Cohere",
        "DBRX",
        "Ernie Bot",
        "Gemini (language model)",
        "Generative pre-trained transformer",
        "GigaChat",
        "GPT-1",
        "GPT-2",
        "GPT-3",
        "GPT-4",
        "GPT-4o",
        "GPT-J",
        "GPT4-Chan",
        "GPTeens",
        "GPTZero",
        "Grok (chatbot)",
        "Huawei PanGu",
        "Humanity's Last Exam",
        "IBM Granite",
        "IBM Watsonx",
        "IFlytek Spark",
        "Jais (language model)",
        "LaMDA",
        "LangChain",
        "List of large language models",
        "Llama (language model)",
        "Llama.cpp",
        "MMLU",
        "OpenAI o1",
        "OpenAI o3",
        "PaLM",
        "The Pile (dataset)",
        "Qwen",
        "Reasoning language model",
        "Reflection (artificial intelligence)",
        "Retrieval-augmented generation",
        "Sparrow (chatbot)",
        "Stochastic parrot",
        "T5 (language model)",
        "Top-p sampling",
        "Undetectable.ai",
        "Vicuna LLM",
        "VideoPoet",
        "Waluigi effect",
        "XLNet",
        "YandexGPT",
        "You.com"
      ]
    },
    "analysis_Sigma": {
      "category": "Sigma",
      "page_count": 0,
      "word_frequencies": [],
      "page_title_refs": []
    }
  },
  "last_updated": {
    "Large_language_models": 1740593364.575663,
    "Large language model": 1740593364.7198646,
    "AutoGPT": 1740593364.9308646,
    "BERT (language model)": 1740593365.1466568,
    "BLOOM (language model)": 1740593365.3692048,
    "Braina": 1740593365.5822284,
    "Brave Leo": 1740593365.8085876,
    "ChatGPT": 1740593366.0543344,
    "Chinchilla (language model)": 1740593366.2728858,
    "Chroma (vector database)": 1740593366.486638,
    "Claude (language model)": 1740593366.698405,
    "Cohere": 1740593366.9211984,
    "DBRX": 1740593367.1386313,
    "Ernie Bot": 1740593367.373731,
    "Gemini (language model)": 1740593367.584543,
    "Generative pre-trained transformer": 1740593367.7941906,
    "GigaChat": 1740593368.0324242,
    "GPT-1": 1740593368.2584512,
    "GPT-2": 1740593368.4667838,
    "GPT-3": 1740593368.6798875,
    "GPT-4": 1740593368.9404976,
    "GPT-4o": 1740593369.1613557,
    "GPT-J": 1740593369.3738606,
    "GPT4-Chan": 1740593369.6075106,
    "GPTeens": 1740593369.8234923,
    "GPTZero": 1740593370.0450206,
    "Grok (chatbot)": 1740593370.301792,
    "Huawei PanGu": 1740593370.5216825,
    "Humanity's Last Exam": 1740593370.7327204,
    "IBM Granite": 1740593370.9528446,
    "IBM Watsonx": 1740593371.1642416,
    "IFlytek Spark": 1740593371.5405285,
    "Jais (language model)": 1740593371.7542262,
    "LaMDA": 1740593371.970482,
    "LangChain": 1740593372.1779344,
    "List of large language models": 1740593372.3824208,
    "Llama (language model)": 1740593372.6073966,
    "Llama.cpp": 1740593372.818842,
    "MMLU": 1740593373.040149,
    "OpenAI o1": 1740593373.2654922,
    "OpenAI o3": 1740593373.495221,
    "PaLM": 1740593373.7153904,
    "The Pile (dataset)": 1740593373.9320207,
    "Qwen": 1740593374.139327,
    "Reasoning language model": 1740593374.3495975,
    "Reflection (artificial intelligence)": 1740593374.5760367,
    "Retrieval-augmented generation": 1740593374.8307467,
    "Sparrow (chatbot)": 1740593375.0547903,
    "Stochastic parrot": 1740593375.2741556,
    "T5 (language model)": 1740593375.4905615,
    "Top-p sampling": 1740593375.706371,
    "Undetectable.ai": 1740593375.9395466,
    "Vicuna LLM": 1740593376.1551604,
    "VideoPoet": 1740593376.3738737,
    "Waluigi effect": 1740593376.581489,
    "XLNet": 1740593376.7885644,
    "YandexGPT": 1740593377.0137508,
    "You.com": 1740593377.2289307,
    "analysis_Large_language_models": 1740593377.333182,
    "Sigma": 1740593423.7103865,
    "analysis_Sigma": 1740593423.7505908
  }
}